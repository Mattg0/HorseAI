Index: config.yaml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Root directory - uncomment the appropriate line for your environment\nbase:\n  #rootdir: '/Users/mattg0/Docs/HorseAIv2/'\n  rootdir: '/root/HorseAI'\n  # Active database configuration\n  active_db: \"5years\"  # Can be \"full\", \"2years\", or \"5years\"\n\n# Database configurations\ndatabases:\n  - name: \"full\"\n    type: \"sqlite\"\n    path: \"data/hippique.db\"\n    description: \"Full historical database\"\n  - name: \"2years\"\n    type: \"sqlite\"\n    path: \"data/hippique2.db\"\n    description: \"Last 2 years of data\"\n  - name: \"5years\"\n    type: \"sqlite\"\n    path: \"data/hippique5.db\"\n    description: \"Last 5 years of data\"\n  - name: \"dev\"\n    type: \"sqlite\"\n    path: \"data/test_lite.db\"\n    description: \"Dev DB\"\n  - name: \"mysql\"\n    type: \"mysql\"\n    host: \"localhost\"\n    user: \"turfai\"\n    password: \"welcome123\"\n    dbname: \"pturf2024\"\n    description: \"MySQL datasource\"\n\ncache:\n  base_path: 'cache'\n  types:\n    historical_data: \"historical_data.parquet\"\n    processed_data: \"processed_data.parquet\"\n    features: \"features.parquet\"\n    models: \"models.parquet\"\n    predictions: \"predictions.parquet\"\n\nfeatures:\n  features_dir: './data/feature_store'\n  embedding_dim: 8\n  default_task_type: 'regression'\n\nmodels:\n  model_dir: './models'\n  types:\n    - hybrid_model\n    - incremental_models\n    - hybrid_LGBM\n\n  default_type: 'hybrid_model'\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/config.yaml b/config.yaml
--- a/config.yaml	(revision 59920ec236ed6d1f3998260516c48f185e450296)
+++ b/config.yaml	(date 1744018413169)
@@ -1,9 +1,9 @@
 # Root directory - uncomment the appropriate line for your environment
 base:
-  #rootdir: '/Users/mattg0/Docs/HorseAIv2/'
-  rootdir: '/root/HorseAI'
+  rootdir: '/Users/mattg0/Docs/HorseAIv2/'
+  #rootdir: '/root/HorseAI'
   # Active database configuration
-  active_db: "5years"  # Can be "full", "2years", or "5years"
+  active_db: "2years"  # Can be "full", "2years", or "5years"
 
 # Database configurations
 databases:
Index: model_training/historical/train_race_model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport argparse\nimport time\nimport json\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, Dict, Any, Optional\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n# Now - simply import from the package\nfrom model_training.regressions.isotonic_calibration import CalibratedRegressor, regression_metrics_report, plot_calibration_effect\n# Import consolidated orchestrator\nfrom core.orchestrators.embedding_feature import FeatureEmbeddingOrchestrator\nfrom utils.env_setup import AppConfig, get_sqlite_dbpath\n\n\nclass HorseRaceModel:\n    \"\"\"Horse race prediction model that combines random forest and LSTM for predictions.\"\"\"\n\n    def __init__(self, config_path: str = 'config.yaml', model_name: str = 'hybrid_model',\n                 model_type: str = None, sequence_length: int = 5, embedding_dim: int = None,\n                 verbose: bool = False):\n        \"\"\"Initialize the model with configuration.\"\"\"\n        self.config = AppConfig(config_path)\n        self.model_name = model_name\n        self.model_type = model_type or ('hybrid_model' if model_name == 'hybrid_model' else 'incremental_models')\n        self.model_paths = self.config.get_model_paths(model_name=self.model_name, model_type=self.model_type)\n        self.verbose = verbose\n\n        # Initialize model components\n        self.models = {}\n        self.rf_model = None\n        self.lstm_model = None\n        self.history = None\n\n        # Get active database configuration\n        self.db_type = self.config._config.base.active_db\n        db_path = get_sqlite_dbpath(self.db_type)\n\n        # Initialize data orchestrator for all data preparation\n        self.data_orchestrator = FeatureEmbeddingOrchestrator(\n            sqlite_path=db_path,\n            embedding_dim=embedding_dim or self.config._config.features.embedding_dim,\n            sequence_length=sequence_length,\n            verbose=verbose\n        )\n\n        # Load training configuration\n        self.training_config = self._load_training_config()\n\n        self.log_info(f\"Initialized HorseRaceModel with database: {self.db_type}\")\n        self.log_info(f\"Model paths: {self.model_paths}\")\n\n    def log_info(self, message):\n        \"\"\"Simple logging method.\"\"\"\n        if self.verbose:\n            print(message)\n\n    def _load_training_config(self) -> Dict:\n        \"\"\"Load training configuration parameters.\"\"\"\n        config = self.config._config\n\n        # Default configuration\n        training_config = {\n            # RF parameters\n            'rf_params': {\n                'n_estimators': 100,\n                'max_depth': None,\n                'min_samples_split': 2,\n                'min_samples_leaf': 1,\n                'max_features': 'sqrt',\n                'n_jobs': -1,\n                'random_state': 42\n            },\n            # LSTM parameters\n            'lstm_params': {\n                'lstm_units': 64,\n                'dropout_rate': 0.2,\n                'epochs': 100,\n                'batch_size': 32,\n                'validation_split': 0.2,\n                'early_stopping_patience': 10,\n                'lr_reduction_factor': 0.5,\n                'lr_patience': 5,\n                'min_lr': 0.00001\n            },\n            # Data parameters\n            'data_params': {\n                'target_field': 'final_position',\n                'test_size': 0.2,\n                'val_size': 0.1,\n                'random_state': 42,\n                'normalize_features': True\n            }\n        }\n\n        # Override with config values if they exist\n        if hasattr(config, 'training'):\n            if hasattr(config.training, 'rf_params'):\n                training_config['rf_params'].update(config.training.rf_params)\n            if hasattr(config.training, 'lstm_params'):\n                training_config['lstm_params'].update(config.training.lstm_params)\n            if hasattr(config.training, 'data_params'):\n                training_config['data_params'].update(config.training.data_params)\n\n        return training_config\n\n    def train_rf_model(self, X_train: pd.DataFrame, y_train: pd.Series,\n                       X_val: pd.DataFrame = None, y_val: pd.Series = None) -> None:\n        \"\"\"\n        Train the Random Forest model.\n\n        Args:\n            X_train: Training features\n            y_train: Training targets\n            X_val: Validation features (optional)\n            y_val: Validation targets (optional)\n        \"\"\"\n        self.log_info(\"\\n===== TRAINING RANDOM FOREST MODEL =====\")\n\n        # Get RF parameters from configuration\n        rf_params = self.training_config['rf_params']\n\n        # Fix the max_features parameter\n        if rf_params.get('max_features') == 'auto':\n            self.log_info(\"Warning: 'max_features=auto' is deprecated, using 'sqrt' instead\")\n            max_features = 'sqrt'\n        else:\n            max_features = rf_params.get('max_features', 'sqrt')  # Default to 'sqrt'\n\n        # Create Random Forest model\n        from sklearn.ensemble import RandomForestRegressor\n        base_rf = RandomForestRegressor(\n            n_estimators=rf_params.get('n_estimators', 100),\n            max_depth=rf_params.get('max_depth', None),\n            min_samples_split=rf_params.get('min_samples_split', 2),\n            min_samples_leaf=rf_params.get('min_samples_leaf', 1),\n            max_features=max_features,\n            n_jobs=rf_params.get('n_jobs', -1),\n            random_state=rf_params.get('random_state', 42)\n        )\n\n        # Create calibrated regressor (will automatically handle calibration)\n        self.rf_model = CalibratedRegressor(\n            base_regressor=base_rf,\n            # Clip predictions between 1 and highest position\n            clip_min=1.0,\n            clip_max=None  # Will be determined during fitting\n        )\n\n        # Train the model with calibration\n        self.log_info(f\"Training RF model on {len(X_train)} samples with {X_train.shape[1]} features...\")\n        start_time = time.time()\n\n        # If no validation set, use a portion of training data for calibration\n        if X_val is None or y_val is None:\n            self.rf_model.fit(X_train, y_train)\n        else:\n            # Use validation set for calibration\n            self.rf_model.fit(X_train, y_train, X_calib=X_val, y_calib=y_val)\n\n        training_time = time.time() - start_time\n        self.log_info(f\"RF model training completed in {training_time:.2f} seconds\")\n\n        # Evaluate performance\n        train_metrics = self.rf_model.evaluate(X_train, y_train)\n        self.log_info(\"\\nTraining set performance:\")\n        self.log_info(f\"  Uncalibrated - RMSE: {train_metrics['raw_rmse']:.4f}, MAE: {train_metrics['raw_mae']:.4f}\")\n        self.log_info(\n            f\"  Calibrated - RMSE: {train_metrics['calibrated_rmse']:.4f}, MAE: {train_metrics['calibrated_mae']:.4f}\")\n\n        if X_val is not None and y_val is not None:\n            val_metrics = self.rf_model.evaluate(X_val, y_val)\n            self.log_info(\"\\nValidation set performance:\")\n            self.log_info(f\"  Uncalibrated - RMSE: {val_metrics['raw_rmse']:.4f}, MAE: {val_metrics['raw_mae']:.4f}\")\n            self.log_info(\n                f\"  Calibrated - RMSE: {val_metrics['calibrated_rmse']:.4f}, MAE: {val_metrics['calibrated_mae']:.4f}\")\n\n            # Generate calibration effect plot\n            logs_path = Path(self.model_paths['logs'])\n            logs_path.mkdir(parents=True, exist_ok=True)\n            plot_path = logs_path / f'calibration_effect_{self.db_type}.png'\n\n            # Get raw and calibrated predictions\n            raw_preds = self.rf_model.predict_raw(X_val)\n            cal_preds = self.rf_model.predict(X_val)\n\n            # Create plot\n            plot_calibration_effect(\n                raw_preds, cal_preds, y_val.values,\n                save_path=str(plot_path)\n            )\n            self.log_info(f\"Calibration effect plot saved to {plot_path}\")\n\n        # Calculate and display feature importance\n        base_regressor = self.rf_model.base_regressor\n        if hasattr(base_regressor, 'feature_importances_'):\n            feature_importance = pd.DataFrame({\n                'feature': X_train.columns,\n                'importance': base_regressor.feature_importances_\n            }).sort_values('importance', ascending=False)\n\n            self.log_info(\"\\nTop feature importance for RF model:\")\n            for i, (feature, importance) in enumerate(\n                    zip(feature_importance['feature'][:10], feature_importance['importance'][:10])\n            ):\n                self.log_info(f\"{i + 1}. {feature}: {importance:.4f}\")\n\n            # Save feature importance\n            importance_path = Path(self.model_paths['logs']) / f'rf_feature_importance_{self.db_type}.csv'\n            importance_path.parent.mkdir(parents=True, exist_ok=True)\n            feature_importance.to_csv(importance_path, index=False)\n            self.log_info(f\"Feature importance saved to {importance_path}\")\n\n        # Store model in models dictionary\n        self.models['rf'] = self.rf_model\n\n        # Store performance metrics\n        self.models['rf_metrics'] = {\n            'training_time': training_time,\n            'train_metrics': train_metrics,\n            'val_metrics': val_metrics if X_val is not None else None,\n        }\n\n    def train_lgbm_model(self, X_train: pd.DataFrame, y_train: pd.Series,\n                         X_val: pd.DataFrame = None, y_val: pd.Series = None) -> None:\n        \"\"\"\n        Train the LightGBM model using scikit-learn API as a direct replacement for RF.\n        No calibration is applied.\n        \"\"\"\n        self.log_info(\"\\n===== TRAINING LIGHTGBM MODEL =====\")\n\n        # Get LightGBM parameters\n        lgbm_params = self.training_config.get('lgbm_params', {\n            'objective': 'regression',\n            'metric': 'mae',\n            'boosting_type': 'gbdt',\n            'num_leaves': 31,\n            'learning_rate': 0.05,\n            'feature_fraction': 0.9,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'verbose': -1,\n            'n_estimators': 1000\n        })\n\n        import lightgbm as lgb\n\n        # Use LGBMRegressor (scikit-learn API) for direct RF replacement\n        self.lgbm_model = lgb.LGBMRegressor(**lgbm_params)\n\n        # Time the training\n        start_time = time.time()\n\n        # Train the model\n        if X_val is not None and y_val is not None:\n            # Use validation data for early stopping\n            self.lgbm_model.fit(\n                X_train, y_train,\n                eval_set=[(X_val, y_val)],\n                eval_metric='mae'            )\n        else:\n            self.lgbm_model.fit(X_train, y_train)\n\n        training_time = time.time() - start_time\n        self.log_info(f\"LightGBM model training completed in {training_time:.2f} seconds\")\n\n        # Store model in models dictionary\n        self.models['lgbm'] = self.lgbm_model\n\n        # Calculate and store metrics\n        train_pred = self.lgbm_model.predict(X_train)\n        train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n        train_mae = mean_absolute_error(y_train, train_pred)\n\n        self.log_info(f\"Training set performance:\")\n        self.log_info(f\"  RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}\")\n\n        # Evaluate on validation data if available\n        val_rmse = None\n        val_mae = None\n        if X_val is not None and y_val is not None:\n            val_pred = self.lgbm_model.predict(X_val)\n            val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n            val_mae = mean_absolute_error(y_val, val_pred)\n\n            self.log_info(f\"Validation set performance:\")\n            self.log_info(f\"  RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}\")\n\n        # Store metrics\n        self.models['lgbm_metrics'] = {\n            'training_time': training_time,\n            'train_rmse': train_rmse,\n            'train_mae': train_mae,\n            'val_rmse': val_rmse,\n            'val_mae': val_mae\n        }\n\n        # Display feature importance if available\n        if hasattr(self.lgbm_model, 'feature_importances_'):\n            feature_importance = pd.DataFrame({\n                'feature': X_train.columns,\n                'importance': self.lgbm_model.feature_importances_\n            }).sort_values('importance', ascending=False)\n\n            self.log_info(\"\\nTop feature importance for LightGBM model:\")\n            for i, (feature, importance) in enumerate(\n                    zip(feature_importance['feature'][:10], feature_importance['importance'][:10])\n            ):\n                self.log_info(f\"{i + 1}. {feature}: {importance:.4f}\")\n\n\n    def train_lstm_model(self, X_sequences, X_static, y_targets) -> None:\n        \"\"\"\n        Train the LSTM model for sequence prediction.\n\n        Args:\n            X_sequences: Sequential input features\n            X_static: Static input features\n            y_targets: Target values\n        \"\"\"\n        self.log_info(\"\\n===== TRAINING LSTM MODEL =====\")\n\n        # Get LSTM parameters from configuration\n        lstm_params = self.training_config['lstm_params']\n\n        # Split into train/validation sets\n        from sklearn.model_selection import train_test_split\n\n        # Use validation_split from parameters\n        val_split = lstm_params.get('validation_split', 0.2)\n        random_state = self.training_config['data_params'].get('random_state', 42)\n\n        X_seq_train, X_seq_val, X_static_train, X_static_val, y_train, y_val = train_test_split(\n            X_sequences, X_static, y_targets,\n            test_size=val_split,\n            random_state=random_state\n        )\n\n        self.log_info(f\"Training set: {len(X_seq_train)} sequences\")\n        self.log_info(f\"Validation set: {len(X_seq_val)} sequences\")\n\n        # Create LSTM model if not already created\n        if self.lstm_model is None:\n            # Get model from orchestrator\n            hybrid_model = self.data_orchestrator.create_hybrid_model(\n                sequence_shape=X_sequences.shape,\n                static_shape=X_static.shape,\n                lstm_units=lstm_params.get('lstm_units', 64),\n                dropout_rate=lstm_params.get('dropout_rate', 0.2)\n            )\n\n            if hybrid_model is None:\n                self.log_info(\"Failed to create LSTM model. TensorFlow may not be available.\")\n                return\n\n            self.lstm_model = hybrid_model['lstm']\n            self.models['lstm'] = self.lstm_model\n\n        # Configure callbacks\n        callbacks = [\n            EarlyStopping(\n                monitor='val_loss',\n                patience=lstm_params.get('early_stopping_patience', 10),\n                restore_best_weights=True\n            ),\n            ReduceLROnPlateau(\n                monitor='val_loss',\n                factor=lstm_params.get('lr_reduction_factor', 0.5),\n                patience=lstm_params.get('lr_patience', 5),\n                min_lr=lstm_params.get('min_lr', 0.00001)\n            )\n        ]\n\n        # Add model checkpoint\n        if self.model_paths['logs']:\n            checkpoint_path = Path(self.model_paths['logs']) / f'best_lstm_model_{self.db_type}.keras'\n            checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n            callbacks.append(\n                ModelCheckpoint(\n                    str(checkpoint_path),\n                    monitor='val_loss',\n                    save_best_only=True,\n                    mode='min'\n                )\n            )\n\n        # Train LSTM model\n        self.log_info(f\"Training LSTM model...\")\n        start_time = time.time()\n\n        history = self.lstm_model.fit(\n            [X_seq_train, X_static_train],\n            y_train,\n            epochs=lstm_params.get('epochs', 100),\n            batch_size=lstm_params.get('batch_size', 32),\n            validation_data=([X_seq_val, X_static_val], y_val),\n            callbacks=callbacks,\n            verbose=1 if self.verbose else 0\n        )\n\n        training_time = time.time() - start_time\n        self.log_info(f\"LSTM model training completed in {training_time:.2f} seconds\")\n\n        # Store training history\n        self.history = history.history\n\n        # Evaluate on training and validation sets\n        train_loss, train_mae = self.lstm_model.evaluate(\n            [X_seq_train, X_static_train], y_train, verbose=0\n        )\n        val_loss, val_mae = self.lstm_model.evaluate(\n            [X_seq_val, X_static_val], y_val, verbose=0\n        )\n\n        self.log_info(f\"LSTM Training metrics - Loss: {train_loss:.4f}, MAE: {train_mae:.4f}\")\n        self.log_info(f\"LSTM Validation metrics - Loss: {val_loss:.4f}, MAE: {val_mae:.4f}\")\n\n        # Store performance metrics\n        self.models['lstm_metrics'] = {\n            'train_loss': train_loss,\n            'train_mae': train_mae,\n            'val_loss': val_loss,\n            'val_mae': val_mae,\n            'history': {k: [float(v) for v in vals] for k, vals in history.history.items()},\n            'training_time': training_time\n        }\n\n        # Plot training history\n        self._plot_lstm_history(history)\n\n    def _plot_lstm_history(self, history):\n        \"\"\"\n        Plot LSTM training history.\n\n        Args:\n            history: History object from model training\n        \"\"\"\n        try:\n            # Create logs directory if it doesn't exist\n            logs_path = Path(self.model_paths['logs'])\n            logs_path.mkdir(parents=True, exist_ok=True)\n\n            # Plot loss\n            plt.figure(figsize=(12, 5))\n\n            plt.subplot(1, 2, 1)\n            plt.plot(history.history['loss'])\n            plt.plot(history.history['val_loss'])\n            plt.title('Model Loss')\n            plt.ylabel('Loss')\n            plt.xlabel('Epoch')\n            plt.legend(['Train', 'Validation'], loc='upper right')\n\n            # Plot MAE\n            plt.subplot(1, 2, 2)\n            plt.plot(history.history['mae'])\n            plt.plot(history.history['val_mae'])\n            plt.title('Model MAE')\n            plt.ylabel('MAE')\n            plt.xlabel('Epoch')\n            plt.legend(['Train', 'Validation'], loc='upper right')\n\n            # Save figure\n            plt.tight_layout()\n            plt.savefig(logs_path / f'lstm_history_{self.db_type}.png')\n            plt.close()\n\n            self.log_info(f\"Training history plot saved to {logs_path}/lstm_history_{self.db_type}.png\")\n        except Exception as e:\n            self.log_info(f\"Error plotting LSTM history: {str(e)}\")\n\n    def evaluate_models(self, X_test, y_test, X_seq_test=None, X_static_test=None):\n        \"\"\"\n        Evaluate trained models on test data.\n\n        Args:\n            X_test: Test features for RF model\n            y_test: Test targets\n            X_seq_test: Test sequential features for LSTM\n            X_static_test: Test static features for LSTM\n\n        Returns:\n            Dictionary with evaluation metrics\n        \"\"\"\n        results = {}\n\n        # Evaluate RF model if available\n        if self.rf_model is not None:\n            self.log_info(\"\\n===== EVALUATING RANDOM FOREST MODEL =====\")\n            rf_pred = self.rf_model.predict(X_test)\n\n            rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n            rf_mae = mean_absolute_error(y_test, rf_pred)\n            rf_r2 = r2_score(y_test, rf_pred)\n\n            self.log_info(f\"RF Test metrics - RMSE: {rf_rmse:.4f}, MAE: {rf_mae:.4f}, R²: {rf_r2:.4f}\")\n\n            results['rf'] = {\n                'rmse': rf_rmse,\n                'mae': rf_mae,\n                'r2': rf_r2\n            }\n\n        # Evaluate LSTM model if available\n        if self.lstm_model is not None and X_seq_test is not None and X_static_test is not None:\n            self.log_info(\"\\n===== EVALUATING LSTM MODEL =====\")\n            lstm_loss, lstm_mae = self.lstm_model.evaluate(\n                [X_seq_test, X_static_test], y_test, verbose=0\n            )\n\n            # Get predictions for R² calculation\n            lstm_pred = self.lstm_model.predict([X_seq_test, X_static_test], verbose=0)\n            lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_pred))\n            lstm_r2 = r2_score(y_test, lstm_pred)\n\n            self.log_info(\n                f\"LSTM Test metrics - Loss: {lstm_loss:.4f}, MAE: {lstm_mae:.4f}, RMSE: {lstm_rmse:.4f}, R²: {lstm_r2:.4f}\")\n\n            results['lstm'] = {\n                'loss': lstm_loss,\n                'mae': lstm_mae,\n                'rmse': lstm_rmse,\n                'r2': lstm_r2\n            }\n\n        # Store evaluation results\n        self.models['test_evaluation'] = results\n        return results\n\n    def train(self, limit=None, race_filter=None, date_filter=None,\n              use_cache=True, train_lgbm=False, train_rf=True, train_lstm=True):\n        \"\"\"\n        Train either or both models based on parameters.\n\n        Args:\n            limit: Optional limit for number of races to load\n            race_filter: Optional filter for specific race types\n            date_filter: Optional date filter\n            use_cache: Whether to use cached results when available\n            train_rf: Whether to train the Random Forest model\n            train_lstm: Whether to train the LSTM model\n        \"\"\"\n        start_time = time.time()\n        print(f\" value for cache is {use_cache}\")\n        self.log_info(f\"\\nStarting training process for {self.db_type} database...\")\n\n        # Train RF model if requested\n        if train_rf:\n            # Get prepared data through the orchestrator\n            X_train, X_val, X_test, y_train, y_val, y_test = self.data_orchestrator.run_pipeline(\n                limit=limit,\n                race_filter=race_filter,\n                date_filter=date_filter,\n                use_cache=use_cache,\n                clean_embeddings=True\n            )\n\n            # Train the RF model with integrated calibration (CalibratedRegressor handles calibration now)\n            self.train_rf_model(X_train, y_train, X_val, y_val)\n\n            # No need for separate calibration since it's now integrated into the CalibratedRegressor\n            # REMOVE THIS LINE: self.calibrate_predictions(X_val, y_val, X_test, y_test)\n\n            # Evaluate on test set\n            self.evaluate_models(X_test, y_test)\n\n        # Continue with LSTM training as before...\n        # Train LGBM model if requested\n        if train_lgbm:\n            X_train, X_val, X_test, y_train, y_val, y_test = self.data_orchestrator.run_pipeline(\n                limit=limit,\n                race_filter=race_filter,\n                date_filter=date_filter,\n                use_cache=use_cache,\n                clean_embeddings=True\n            )\n            self.train_lgbm_model(X_train, y_train, X_val, y_val)\n\n        if train_lstm:\n            try:\n                # Get data for sequential model\n                df_features, _ = self.data_orchestrator.load_or_prepare_data(\n                    use_cache=use_cache,\n                    limit=limit,\n                    race_filter=race_filter,\n                    date_filter=date_filter\n                )\n\n                # Prepare sequence data\n                X_sequences, X_static, y = self.data_orchestrator.prepare_sequence_data(\n                    df_features,\n                    sequence_length=self.sequence_length\n                )\n\n                # Split into train/test sets\n                from sklearn.model_selection import train_test_split\n\n                # Split with test_size from parameters\n                test_size = self.training_config['data_params'].get('test_size', 0.2)\n                random_state = self.training_config['data_params'].get('random_state', 42)\n\n                X_seq_train, X_seq_test, X_static_train, X_static_test, y_train, y_test = train_test_split(\n                    X_sequences, X_static, y,\n                    test_size=test_size,\n                    random_state=random_state\n                )\n\n                # Train the LSTM model (includes further train/val split)\n                self.train_lstm_model(X_seq_train, X_static_train, y_train)\n\n                # Evaluate on test set\n                self.evaluate_models(None, y_test, X_seq_test, X_static_test)\n\n            except Exception as e:\n                self.log_info(f\"Error during LSTM model training: {str(e)}\")\n                import traceback\n                self.log_info(traceback.format_exc())\n\n        # Calculate total training time\n        total_time = time.time() - start_time\n        self.log_info(f\"\\nTotal training time: {total_time:.2f} seconds\")\n\n        # Save models if any were trained\n        if train_rf or train_lstm or train_lgbm:\n            self.save_models(save_rf=train_rf, save_lstm=train_lstm, save_lgbm=train_lgbm)\n\n        self.log_info(\"\\nTraining process completed!\")\n\n    def save_models(self, save_rf: bool = True, save_lstm: bool = True, save_lgbm: bool = True) -> None:\n        \"\"\"\n        Save the trained models and metadata.\n\n        Args:\n            save_rf: Whether to save RF model\n            save_lstm: Whether to save LSTM model\n        \"\"\"\n        self.log_info(\"\\n===== SAVING MODELS =====\")\n\n        # Create version string\n        version = f\"v{time.strftime('%Y%m%d')}\"\n\n        # Create version directory in appropriate model type folder\n        save_dir = Path(self.model_paths['model_path']) / version\n        save_dir.mkdir(parents=True, exist_ok=True)\n\n        self.log_info(f\"Saving models to: {save_dir}\")\n\n        # Save RF model (now using CalibratedRegressor's save method)\n        if save_rf and self.rf_model is not None:\n            rf_path = save_dir / self.model_paths['artifacts']['rf_model']\n\n            if hasattr(self.rf_model, 'save'):\n                # Use CalibratedRegressor's save method\n                self.rf_model.save(rf_path)\n                self.log_info(f\"Saved calibrated RF model to: {rf_path}\")\n            else:\n                # Fallback to traditional joblib dump with metadata\n                rf_metadata = {\n                    'model': self.rf_model,\n                    'version': version,\n                    'features': self.data_orchestrator.preprocessing_params.get('feature_columns', []),\n                    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n                    'metrics': self.models.get('rf_metrics', {})\n                }\n                joblib.dump(rf_metadata, rf_path)\n                self.log_info(f\"Saved RF model with metadata to: {rf_path}\")\n\n        # Save LSTM model\n        if save_lstm and self.lstm_model is not None:\n            lstm_path = save_dir / self.model_paths['artifacts']['lstm_model']\n            self.lstm_model.save(lstm_path)\n            self.log_info(f\"Saved LSTM model to: {lstm_path}\")\n\n            # Save LSTM training history separately\n            if self.history:\n                history_path = save_dir / 'lstm_history.joblib'\n                joblib.dump(self.history, history_path)\n                self.log_info(f\"Saved LSTM training history to: {history_path}\")\n        # Save LGBM model\n        if save_lgbm and hasattr(self, 'lgbm_model') and self.lgbm_model is not None:\n            lgbm_path = save_dir / \"hybrid_lgbm_model.joblib\"\n\n            if hasattr(self.lgbm_model, 'save'):\n                self.lgbm_model.save(lgbm_path)\n            else:\n                lgbm_metadata = {\n                    'model': self.lgbm_model,\n                    'version': version,\n                    'features': self.data_orchestrator.preprocessing_params.get('feature_columns', []),\n                    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n                    'metrics': self.models.get('lgbm_metrics', {})\n                }\n                joblib.dump(lgbm_metadata, lgbm_path)\n            self.log_info(f\"Saved LightGBM model to: {lgbm_path}\")\n        # Save orchestrator state for reproducibility\n        feature_path = save_dir / self.model_paths['artifacts']['feature_engineer']\n        orchestrator_state = {\n            'preprocessing_params': self.data_orchestrator.preprocessing_params,\n            'embedding_dim': self.data_orchestrator.embedding_dim,\n            'sequence_length': self.data_orchestrator.sequence_length,\n            'target_info': self.data_orchestrator.target_info\n        }\n        joblib.dump(orchestrator_state, feature_path)\n        self.log_info(f\"Saved feature engineering state to: {feature_path}\")\n\n        # Save model configuration\n        config_path = save_dir / 'model_config.json'\n        model_config = {\n            'version': version,\n            'model_name': self.model_name,\n            'model_type': self.model_type,\n            'db_type': self.db_type,\n            'sequence_length': self.sequence_length,\n            'embedding_dim': self.data_orchestrator.embedding_dim,\n            'training_config': self.training_config,\n            'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'models_trained': {\n                'rf': save_rf and self.rf_model is not None,\n                'lstm': save_lstm and self.lstm_model is not None,\n                'lgbm': save_lgbm and self.lgbm_model is not None\n\n            },\n            'evaluation_results': self.models.get('test_evaluation', {})\n        }\n\n        with open(config_path, 'w') as f:\n            json.dump(model_config, f, indent=2, default=str)\n        self.log_info(f\"Saved model configuration to: {config_path}\")\n\n        self.log_info(f\"All models and components saved successfully to {save_dir}\")\n\n    def load_models(self, model_dir=None, version=None):\n        \"\"\"\n        Load saved models.\n\n        Args:\n            model_dir: Path to model directory, if None uses default from config\n            version: Specific version to load, if None uses latest\n\n        Returns:\n            Whether loading was successful\n        \"\"\"\n        # Determine model directory\n        if model_dir is None:\n            model_dir = self.model_paths['model_path']\n\n        model_path = Path(model_dir)\n\n        # Find available versions\n        versions = [d for d in model_path.iterdir() if d.is_dir() and d.name.startswith('v')]\n\n        if not versions:\n            self.log_info(f\"No model versions found in {model_path}\")\n            return False\n\n        # Sort versions (newest first)\n        versions.sort(reverse=True)\n\n        # Select version to load\n        if version is not None:\n            version_path = model_path / version\n            if not version_path.exists():\n                self.log_info(f\"Version {version} not found in {model_path}\")\n                return False\n        else:\n            version_path = versions[0]  # Latest version\n\n        self.log_info(f\"Loading models from {version_path}\")\n\n        success = True\n\n        # Load RF model if available\n        rf_path = version_path / self.model_paths['artifacts']['rf_model']\n        if rf_path.exists():\n            try:\n                rf_metadata = joblib.load(rf_path)\n                self.rf_model = rf_metadata.get('model')\n                self.models['rf'] = self.rf_model\n                self.models['rf_metadata'] = rf_metadata\n                self.log_info(f\"Loaded RF model from {rf_path}\")\n            except Exception as e:\n                self.log_info(f\"Error loading RF model: {str(e)}\")\n                success = False\n\n        # Load LSTM model if available\n        lstm_path = version_path / self.model_paths['artifacts']['lstm_model']\n        if lstm_path.exists():\n            try:\n                from tensorflow.keras.models import load_model\n                self.lstm_model = load_model(lstm_path)\n                self.models['lstm'] = self.lstm_model\n                self.log_info(f\"Loaded LSTM model from {lstm_path}\")\n\n                # Try to load history\n                history_path = version_path / 'lstm_history.joblib'\n                if history_path.exists():\n                    self.history = joblib.load(history_path)\n            except Exception as e:\n                self.log_info(f\"Error loading LSTM model: {str(e)}\")\n                success = False\n\n        # Load orchestrator state\n        feature_path = version_path / self.model_paths['artifacts']['feature_engineer']\n        if feature_path.exists():\n            try:\n                orchestrator_state = joblib.load(feature_path)\n\n                # Update orchestrator with loaded state\n                if isinstance(orchestrator_state, dict):\n                    if 'preprocessing_params' in orchestrator_state:\n                        self.data_orchestrator.preprocessing_params.update(\n                            orchestrator_state['preprocessing_params']\n                        )\n                    if 'target_info' in orchestrator_state:\n                        self.data_orchestrator.target_info = orchestrator_state['target_info']\n\n                self.log_info(f\"Loaded feature engineering state from {feature_path}\")\n            except Exception as e:\n                self.log_info(f\"Error loading feature engineering state: {str(e)}\")\n\n        return success\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Train horse race prediction model')\n    parser = argparse.ArgumentParser(description='Train horse race prediction model')\n    parser.add_argument('--config', type=str, default='config.yaml', help='Path to configuration file')\n    parser.add_argument('--db', type=str, default=None, help='Database to use (overrides config)')\n    parser.add_argument('--limit', type=int, default=None, help='Limit number of races to load')\n    parser.add_argument('--race-type', type=str, default=None, help='Filter by race type (e.g., \"A\" for Attele)')\n    parser.add_argument('--date-filter', type=str, default=None, help='Date filter (e.g., \"jour > \\'2023-01-01\\'\")')\n    parser.add_argument('--no-cache', action='store_true', help='Disable caching of intermediate data')\n    parser.add_argument('--sequence-length', type=int, default=5, help='Sequence length for LSTM')\n    parser.add_argument('--embedding-dim', type=int, default=None, help='Dimension for entity embeddings')\n    parser.add_argument('--model-name', type=str, default='hybrid', help='Model architecture name')\n    parser.add_argument('--rf-only', action='store_true', help='Train only Random Forest model')\n    parser.add_argument('--lgbm-only', action='store_true', help='Train only LightGBM model')\n    parser.add_argument('--lstm-only', action='store_true', help='Train only LSTM model')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    args = parser.parse_args()\n\n    # Determine which models to train\n    train_rf = not (args.lstm_only or args.lgbm_only) and args.model_name == 'hybrid_model'\n    train_lgbm = not (args.lstm_only or args.rf_only) and args.model_name == 'hybrid_LGBM'\n    train_lstm = not (args.rf_only or args.lgbm_only)\n\n    # Create and train the model\n    trainer = HorseRaceModel(\n        config_path=args.config,\n        model_name=args.model_name,\n        sequence_length=args.sequence_length,\n        embedding_dim=args.embedding_dim,\n        verbose=args.verbose\n    )\n\n    trainer.train(\n        limit=args.limit,\n        race_filter=args.race_type,\n        date_filter=args.date_filter,\n        use_cache= not args.no_cache,\n        train_rf=train_rf,\n        train_lgbm=train_lgbm,\n        train_lstm=train_lstm\n    )\n\n\nif __name__ == \"__main__\":\n    main()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/model_training/historical/train_race_model.py b/model_training/historical/train_race_model.py
--- a/model_training/historical/train_race_model.py	(revision 59920ec236ed6d1f3998260516c48f185e450296)
+++ b/model_training/historical/train_race_model.py	(date 1744035683042)
@@ -20,14 +20,14 @@
 class HorseRaceModel:
     """Horse race prediction model that combines random forest and LSTM for predictions."""
 
-    def __init__(self, config_path: str = 'config.yaml', model_name: str = 'hybrid_model',
-                 model_type: str = None, sequence_length: int = 5, embedding_dim: int = None,
+    def __init__(self, config_path: str = 'config.yaml', model_name: str = 'hybrid',
+                 sequence_length: int = 5, embedding_dim: int = None,
                  verbose: bool = False):
         """Initialize the model with configuration."""
         self.config = AppConfig(config_path)
         self.model_name = model_name
-        self.model_type = model_type or ('hybrid_model' if model_name == 'hybrid_model' else 'incremental_models')
-        self.model_paths = self.config.get_model_paths(model_name=self.model_name, model_type=self.model_type)
+        self.model_paths = self.config.get_model_paths()
+        self.sequence_length = sequence_length
         self.verbose = verbose
 
         # Initialize model components
@@ -225,94 +225,6 @@
             'val_metrics': val_metrics if X_val is not None else None,
         }
 
-    def train_lgbm_model(self, X_train: pd.DataFrame, y_train: pd.Series,
-                         X_val: pd.DataFrame = None, y_val: pd.Series = None) -> None:
-        """
-        Train the LightGBM model using scikit-learn API as a direct replacement for RF.
-        No calibration is applied.
-        """
-        self.log_info("\n===== TRAINING LIGHTGBM MODEL =====")
-
-        # Get LightGBM parameters
-        lgbm_params = self.training_config.get('lgbm_params', {
-            'objective': 'regression',
-            'metric': 'mae',
-            'boosting_type': 'gbdt',
-            'num_leaves': 31,
-            'learning_rate': 0.05,
-            'feature_fraction': 0.9,
-            'bagging_fraction': 0.8,
-            'bagging_freq': 5,
-            'verbose': -1,
-            'n_estimators': 1000
-        })
-
-        import lightgbm as lgb
-
-        # Use LGBMRegressor (scikit-learn API) for direct RF replacement
-        self.lgbm_model = lgb.LGBMRegressor(**lgbm_params)
-
-        # Time the training
-        start_time = time.time()
-
-        # Train the model
-        if X_val is not None and y_val is not None:
-            # Use validation data for early stopping
-            self.lgbm_model.fit(
-                X_train, y_train,
-                eval_set=[(X_val, y_val)],
-                eval_metric='mae'            )
-        else:
-            self.lgbm_model.fit(X_train, y_train)
-
-        training_time = time.time() - start_time
-        self.log_info(f"LightGBM model training completed in {training_time:.2f} seconds")
-
-        # Store model in models dictionary
-        self.models['lgbm'] = self.lgbm_model
-
-        # Calculate and store metrics
-        train_pred = self.lgbm_model.predict(X_train)
-        train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
-        train_mae = mean_absolute_error(y_train, train_pred)
-
-        self.log_info(f"Training set performance:")
-        self.log_info(f"  RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}")
-
-        # Evaluate on validation data if available
-        val_rmse = None
-        val_mae = None
-        if X_val is not None and y_val is not None:
-            val_pred = self.lgbm_model.predict(X_val)
-            val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))
-            val_mae = mean_absolute_error(y_val, val_pred)
-
-            self.log_info(f"Validation set performance:")
-            self.log_info(f"  RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}")
-
-        # Store metrics
-        self.models['lgbm_metrics'] = {
-            'training_time': training_time,
-            'train_rmse': train_rmse,
-            'train_mae': train_mae,
-            'val_rmse': val_rmse,
-            'val_mae': val_mae
-        }
-
-        # Display feature importance if available
-        if hasattr(self.lgbm_model, 'feature_importances_'):
-            feature_importance = pd.DataFrame({
-                'feature': X_train.columns,
-                'importance': self.lgbm_model.feature_importances_
-            }).sort_values('importance', ascending=False)
-
-            self.log_info("\nTop feature importance for LightGBM model:")
-            for i, (feature, importance) in enumerate(
-                    zip(feature_importance['feature'][:10], feature_importance['importance'][:10])
-            ):
-                self.log_info(f"{i + 1}. {feature}: {importance:.4f}")
-
-
     def train_lstm_model(self, X_sequences, X_static, y_targets) -> None:
         """
         Train the LSTM model for sequence prediction.
@@ -532,7 +444,7 @@
         return results
 
     def train(self, limit=None, race_filter=None, date_filter=None,
-              use_cache=True, train_lgbm=False, train_rf=True, train_lstm=True):
+              use_cache=True, train_rf=True, train_lstm=True):
         """
         Train either or both models based on parameters.
 
@@ -545,7 +457,6 @@
             train_lstm: Whether to train the LSTM model
         """
         start_time = time.time()
-        print(f" value for cache is {use_cache}")
         self.log_info(f"\nStarting training process for {self.db_type} database...")
 
         # Train RF model if requested
@@ -569,17 +480,6 @@
             self.evaluate_models(X_test, y_test)
 
         # Continue with LSTM training as before...
-        # Train LGBM model if requested
-        if train_lgbm:
-            X_train, X_val, X_test, y_train, y_val, y_test = self.data_orchestrator.run_pipeline(
-                limit=limit,
-                race_filter=race_filter,
-                date_filter=date_filter,
-                use_cache=use_cache,
-                clean_embeddings=True
-            )
-            self.train_lgbm_model(X_train, y_train, X_val, y_val)
-
         if train_lstm:
             try:
                 # Get data for sequential model
@@ -625,12 +525,12 @@
         self.log_info(f"\nTotal training time: {total_time:.2f} seconds")
 
         # Save models if any were trained
-        if train_rf or train_lstm or train_lgbm:
-            self.save_models(save_rf=train_rf, save_lstm=train_lstm, save_lgbm=train_lgbm)
+        if train_rf or train_lstm:
+            self.save_models(save_rf=train_rf, save_lstm=train_lstm)
 
         self.log_info("\nTraining process completed!")
 
-    def save_models(self, save_rf: bool = True, save_lstm: bool = True, save_lgbm: bool = True) -> None:
+    def save_models(self, save_rf: bool = True, save_lstm: bool = True) -> None:
         """
         Save the trained models and metadata.
 
@@ -642,8 +542,6 @@
 
         # Create version string
         version = f"v{time.strftime('%Y%m%d')}"
-
-        # Create version directory in appropriate model type folder
         save_dir = Path(self.model_paths['model_path']) / version
         save_dir.mkdir(parents=True, exist_ok=True)
 
@@ -680,22 +578,7 @@
                 history_path = save_dir / 'lstm_history.joblib'
                 joblib.dump(self.history, history_path)
                 self.log_info(f"Saved LSTM training history to: {history_path}")
-        # Save LGBM model
-        if save_lgbm and hasattr(self, 'lgbm_model') and self.lgbm_model is not None:
-            lgbm_path = save_dir / "hybrid_lgbm_model.joblib"
 
-            if hasattr(self.lgbm_model, 'save'):
-                self.lgbm_model.save(lgbm_path)
-            else:
-                lgbm_metadata = {
-                    'model': self.lgbm_model,
-                    'version': version,
-                    'features': self.data_orchestrator.preprocessing_params.get('feature_columns', []),
-                    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
-                    'metrics': self.models.get('lgbm_metrics', {})
-                }
-                joblib.dump(lgbm_metadata, lgbm_path)
-            self.log_info(f"Saved LightGBM model to: {lgbm_path}")
         # Save orchestrator state for reproducibility
         feature_path = save_dir / self.model_paths['artifacts']['feature_engineer']
         orchestrator_state = {
@@ -712,7 +595,6 @@
         model_config = {
             'version': version,
             'model_name': self.model_name,
-            'model_type': self.model_type,
             'db_type': self.db_type,
             'sequence_length': self.sequence_length,
             'embedding_dim': self.data_orchestrator.embedding_dim,
@@ -720,9 +602,7 @@
             'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
             'models_trained': {
                 'rf': save_rf and self.rf_model is not None,
-                'lstm': save_lstm and self.lstm_model is not None,
-                'lgbm': save_lgbm and self.lgbm_model is not None
-
+                'lstm': save_lstm and self.lstm_model is not None
             },
             'evaluation_results': self.models.get('test_evaluation', {})
         }
@@ -826,7 +706,9 @@
 
 
 def main():
-    parser = argparse.ArgumentParser(description='Train horse race prediction model')
+    """
+    Main function to run the training process from command line.
+    """
     parser = argparse.ArgumentParser(description='Train horse race prediction model')
     parser.add_argument('--config', type=str, default='config.yaml', help='Path to configuration file')
     parser.add_argument('--db', type=str, default=None, help='Database to use (overrides config)')
@@ -838,15 +720,19 @@
     parser.add_argument('--embedding-dim', type=int, default=None, help='Dimension for entity embeddings')
     parser.add_argument('--model-name', type=str, default='hybrid', help='Model architecture name')
     parser.add_argument('--rf-only', action='store_true', help='Train only Random Forest model')
-    parser.add_argument('--lgbm-only', action='store_true', help='Train only LightGBM model')
     parser.add_argument('--lstm-only', action='store_true', help='Train only LSTM model')
     parser.add_argument('--verbose', action='store_true', help='Enable verbose output')
+
     args = parser.parse_args()
 
+    # Update AppConfig with specified database if provided
+    if args.db is not None:
+        config = AppConfig(args.config)
+        config._config.base.active_db = args.db
+
     # Determine which models to train
-    train_rf = not (args.lstm_only or args.lgbm_only) and args.model_name == 'hybrid_model'
-    train_lgbm = not (args.lstm_only or args.rf_only) and args.model_name == 'hybrid_LGBM'
-    train_lstm = not (args.rf_only or args.lgbm_only)
+    train_rf = not args.lstm_only  # Train RF unless LSTM-only flag is set
+    train_lstm = not args.rf_only  # Train LSTM unless RF-only flag is set
 
     # Create and train the model
     trainer = HorseRaceModel(
@@ -861,9 +747,8 @@
         limit=args.limit,
         race_filter=args.race_type,
         date_filter=args.date_filter,
-        use_cache= not args.no_cache,
+        use_cache=args.no_cache,
         train_rf=train_rf,
-        train_lgbm=train_lgbm,
         train_lstm=train_lstm
     )
 
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"AutoImportSettings\">\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\n  </component>\n  <component name=\"ChangeListManager\">\n    <list default=\"true\" id=\"43ec0894-de26-497b-a8fc-a968059a9170\" name=\"Changes\" comment=\"adding daily races sync\">\n      <change afterPath=\"$PROJECT_DIR$/core/orchestrators/prediction_orchestrator.py\" afterDir=\"false\" />\n      <change afterPath=\"$PROJECT_DIR$/race_prediction/predict_daily_races.py\" afterDir=\"false\" />\n      <change afterPath=\"$PROJECT_DIR$/race_prediction/predict_orchestrator_cli.py\" afterDir=\"false\" />\n      <change afterPath=\"$PROJECT_DIR$/race_prediction/race_predict.py\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/.idea/workspace.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/workspace.xml\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/core/connectors/api_daily_sync.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/core/connectors/api_daily_sync.py\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/race_prediction/predict_race.py\" beforeDir=\"false\" />\n    </list>\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\n  </component>\n  <component name=\"FileTemplateManagerImpl\">\n    <option name=\"RECENT_TEMPLATES\">\n      <list>\n        <option value=\"Python Script\" />\n      </list>\n    </option>\n  </component>\n  <component name=\"Git.Settings\">\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\n  </component>\n  <component name=\"GitHubPullRequestSearchHistory\">{\n  &quot;lastFilter&quot;: {\n    &quot;state&quot;: &quot;OPEN&quot;,\n    &quot;assignee&quot;: &quot;Mattg0&quot;\n  }\n}</component>\n  <component name=\"GithubPullRequestsUISettings\">{\n  &quot;selectedUrlAndAccountId&quot;: {\n    &quot;url&quot;: &quot;https://github.com/Mattg0/HorseAI.git&quot;,\n    &quot;accountId&quot;: &quot;aee67ed6-2c23-4be9-a632-2459e76288ac&quot;\n  }\n}</component>\n  <component name=\"ProjectColorInfo\">{\n  &quot;associatedIndex&quot;: 4\n}</component>\n  <component name=\"ProjectId\" id=\"2tOCkYF0cZhZtRPomssp0yLhNGU\" />\n  <component name=\"ProjectLevelVcsManager\" settingsEditedManually=\"true\" />\n  <component name=\"ProjectViewState\">\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\n    <option name=\"showLibraryContents\" value=\"true\" />\n  </component>\n  <component name=\"PropertiesComponent\"><![CDATA[{\n  \"keyToString\": {\n    \"Python.api_daily_sync.executor\": \"Run\",\n    \"Python.embedding_feature.executor\": \"Run\",\n    \"Python.env_setup.executor\": \"Run\",\n    \"Python.horse_embedding.executor\": \"Run\",\n    \"Python.musique_calculation.executor\": \"Debug\",\n    \"Python.mysql_connector.executor\": \"Run\",\n    \"Python.mysql_sqlite_sync.executor\": \"Run\",\n    \"Python.mysql_to_sqlite.executor\": \"Run\",\n    \"Python.predict_daily_races.executor\": \"Run\",\n    \"Python.prediction_orchestrator.executor\": \"Run\",\n    \"Python.test.executor\": \"Debug\",\n    \"Python.train_model.executor\": \"Debug\",\n    \"Python.train_race_model.executor\": \"Run\",\n    \"RunOnceActivity.ShowReadmeOnStart\": \"true\",\n    \"RunOnceActivity.git.unshallow\": \"true\",\n    \"git-widget-placeholder\": \"main\"\n  }\n}]]></component>\n  <component name=\"RecentsManager\">\n    <key name=\"MoveFile.RECENT_KEYS\">\n      <recent name=\"$PROJECT_DIR$/core/connectors\" />\n    </key>\n  </component>\n  <component name=\"RunManager\" selected=\"Python.predict_daily_races\">\n    <configuration name=\"api_daily_sync\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"HorseAIv2\" />\n      <option name=\"ENV_FILES\" value=\"\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/core/connectors/api_daily_sync.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"predict_daily_races\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"HorseAIv2\" />\n      <option name=\"ENV_FILES\" value=\"\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/race_prediction/predict_daily_races.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"prediction_orchestrator\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"HorseAIv2\" />\n      <option name=\"ENV_FILES\" value=\"\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/core/orchestrators\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/core/orchestrators/prediction_orchestrator.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"test\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"HorseAIv2\" />\n      <option name=\"ENV_FILES\" value=\"\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/test.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"train_race_model\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"HorseAIv2\" />\n      <option name=\"ENV_FILES\" value=\"\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/model_training/historical/train_race_model.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <list>\n      <item itemvalue=\"Python.predict_daily_races\" />\n      <item itemvalue=\"Python.prediction_orchestrator\" />\n      <item itemvalue=\"Python.api_daily_sync\" />\n      <item itemvalue=\"Python.train_race_model\" />\n      <item itemvalue=\"Python.test\" />\n    </list>\n    <recent_temporary>\n      <list>\n        <item itemvalue=\"Python.predict_daily_races\" />\n        <item itemvalue=\"Python.prediction_orchestrator\" />\n        <item itemvalue=\"Python.api_daily_sync\" />\n        <item itemvalue=\"Python.train_race_model\" />\n        <item itemvalue=\"Python.test\" />\n      </list>\n    </recent_temporary>\n  </component>\n  <component name=\"SharedIndexes\">\n    <attachedChunks>\n      <set>\n        <option value=\"bundled-python-sdk-fb887030ada0-aa17d162503b-com.jetbrains.pycharm.community.sharedIndexes.bundled-PC-243.21565.199\" />\n      </set>\n    </attachedChunks>\n  </component>\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\n  <component name=\"TaskManager\">\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\n      <changelist id=\"43ec0894-de26-497b-a8fc-a968059a9170\" name=\"Changes\" comment=\"\" />\n      <created>1740213877566</created>\n      <option name=\"number\" value=\"Default\" />\n      <option name=\"presentableId\" value=\"Default\" />\n      <updated>1740213877566</updated>\n    </task>\n    <task id=\"LOCAL-00001\" summary=\"Project Initialisiation\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740556622361</created>\n      <option name=\"number\" value=\"00001\" />\n      <option name=\"presentableId\" value=\"LOCAL-00001\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740556622361</updated>\n    </task>\n    <task id=\"LOCAL-00002\" summary=\"Working MySQL connector!\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740584410696</created>\n      <option name=\"number\" value=\"00002\" />\n      <option name=\"presentableId\" value=\"LOCAL-00002\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740584410696</updated>\n    </task>\n    <task id=\"LOCAL-00003\" summary=\"modular mysql&lt;-&gt;sqlite sync\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740650477583</created>\n      <option name=\"number\" value=\"00003\" />\n      <option name=\"presentableId\" value=\"LOCAL-00003\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740650477583</updated>\n    </task>\n    <task id=\"LOCAL-00004\" summary=\"adding embeddings\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740659886795</created>\n      <option name=\"number\" value=\"00004\" />\n      <option name=\"presentableId\" value=\"LOCAL-00004\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740659886795</updated>\n    </task>\n    <task id=\"LOCAL-00005\" summary=\"adding cache manager\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740693327554</created>\n      <option name=\"number\" value=\"00005\" />\n      <option name=\"presentableId\" value=\"LOCAL-00005\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740693327554</updated>\n    </task>\n    <task id=\"LOCAL-00006\" summary=\"updating embedding feature with config approach\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740755058414</created>\n      <option name=\"number\" value=\"00006\" />\n      <option name=\"presentableId\" value=\"LOCAL-00006\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740755058414</updated>\n    </task>\n    <task id=\"LOCAL-00007\" summary=\"updating embedding feature with config approach\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740920224165</created>\n      <option name=\"number\" value=\"00007\" />\n      <option name=\"presentableId\" value=\"LOCAL-00007\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740920224165</updated>\n    </task>\n    <task id=\"LOCAL-00008\" summary=\"updating embedding feature with config approach\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1741207626289</created>\n      <option name=\"number\" value=\"00008\" />\n      <option name=\"presentableId\" value=\"LOCAL-00008\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1741207626289</updated>\n    </task>\n    <task id=\"LOCAL-00009\" summary=\"refactored_horse_embedding\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1741207970875</created>\n      <option name=\"number\" value=\"00009\" />\n      <option name=\"presentableId\" value=\"LOCAL-00009\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1741207970875</updated>\n    </task>\n    <task id=\"LOCAL-00010\" summary=\"refactored_couple_embedding\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1741208877518</created>\n      <option name=\"number\" value=\"00010\" />\n      <option name=\"presentableId\" value=\"LOCAL-00010\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1741208877518</updated>\n    </task>\n    <task id=\"LOCAL-00011\" summary=\"refactored_cache_manager\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742135265333</created>\n      <option name=\"number\" value=\"00011\" />\n      <option name=\"presentableId\" value=\"LOCAL-00011\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742135265333</updated>\n    </task>\n    <task id=\"LOCAL-00012\" summary=\"fixed cache_manager &amp; orchestrator\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742200198713</created>\n      <option name=\"number\" value=\"00012\" />\n      <option name=\"presentableId\" value=\"LOCAL-00012\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742200198713</updated>\n    </task>\n    <task id=\"LOCAL-00013\" summary=\"fixing feature storing and loading with testing\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742201779545</created>\n      <option name=\"number\" value=\"00013\" />\n      <option name=\"presentableId\" value=\"LOCAL-00013\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742201779545</updated>\n    </task>\n    <task id=\"LOCAL-00014\" summary=\"fixed feature embedding\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742226900595</created>\n      <option name=\"number\" value=\"00014\" />\n      <option name=\"presentableId\" value=\"LOCAL-00014\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742226900595</updated>\n    </task>\n    <task id=\"LOCAL-00015\" summary=\"fixed feature embedding\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742541186315</created>\n      <option name=\"number\" value=\"00015\" />\n      <option name=\"presentableId\" value=\"LOCAL-00015\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742541186315</updated>\n    </task>\n    <task id=\"LOCAL-00016\" summary=\"fixed feature embedding\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742549204949</created>\n      <option name=\"number\" value=\"00016\" />\n      <option name=\"presentableId\" value=\"LOCAL-00016\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742549204949</updated>\n    </task>\n    <task id=\"LOCAL-00017\" summary=\"fixed feature embedding\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742825115896</created>\n      <option name=\"number\" value=\"00017\" />\n      <option name=\"presentableId\" value=\"LOCAL-00017\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742825115896</updated>\n    </task>\n    <task id=\"LOCAL-00018\" summary=\"adding daily races sync\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742903024757</created>\n      <option name=\"number\" value=\"00018\" />\n      <option name=\"presentableId\" value=\"LOCAL-00018\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742903024757</updated>\n    </task>\n    <task id=\"LOCAL-00019\" summary=\"adding daily races sync\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1743020439512</created>\n      <option name=\"number\" value=\"00019\" />\n      <option name=\"presentableId\" value=\"LOCAL-00019\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1743020439512</updated>\n    </task>\n    <task id=\"LOCAL-00020\" summary=\"adding daily races sync\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1743421163636</created>\n      <option name=\"number\" value=\"00020\" />\n      <option name=\"presentableId\" value=\"LOCAL-00020\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1743421163636</updated>\n    </task>\n    <option name=\"localTasksCounter\" value=\"21\" />\n    <servers />\n  </component>\n  <component name=\"VcsManagerConfiguration\">\n    <MESSAGE value=\"Project Initialisiation\" />\n    <MESSAGE value=\"Working MySQL connector!\" />\n    <MESSAGE value=\"modular mysql&lt;-&gt;sqlite sync\" />\n    <MESSAGE value=\"adding embeddings\" />\n    <MESSAGE value=\"adding cache manager\" />\n    <MESSAGE value=\"updating embedding feature with config approach\" />\n    <MESSAGE value=\"refactored_horse_embedding\" />\n    <MESSAGE value=\"refactored_couple_embedding\" />\n    <MESSAGE value=\"refactored_cache_manager\" />\n    <MESSAGE value=\"fixed cache_manager &amp; orchestrator\" />\n    <MESSAGE value=\"fixing feature storing and loading with testing\" />\n    <MESSAGE value=\"fixed feature embedding\" />\n    <MESSAGE value=\"adding daily races sync\" />\n    <option name=\"LAST_COMMIT_MESSAGE\" value=\"adding daily races sync\" />\n  </component>\n  <component name=\"XDebuggerManager\">\n    <breakpoint-manager>\n      <breakpoints>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/calculators/static_feature_calculator.py</url>\n          <line>55</line>\n          <option name=\"timeStamp\" value=\"27\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/calculators/static_feature_calculator.py</url>\n          <line>56</line>\n          <option name=\"timeStamp\" value=\"28\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/calculators/static_feature_calculator.py</url>\n          <line>57</line>\n          <option name=\"timeStamp\" value=\"29\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/calculators/static_feature_calculator.py</url>\n          <line>169</line>\n          <option name=\"timeStamp\" value=\"30\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/transformers/historical_race_transformer.py</url>\n          <line>84</line>\n          <option name=\"timeStamp\" value=\"55\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/orchestrators/mysql_sqlite_sync.py</url>\n          <line>25</line>\n          <option name=\"timeStamp\" value=\"67\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/orchestrators/embedding_feature.py</url>\n          <line>514</line>\n          <option name=\"timeStamp\" value=\"92\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/transformers/daily_race_transformer.py</url>\n          <line>315</line>\n          <option name=\"timeStamp\" value=\"97\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/race_prediction/predict_daily_races.py</url>\n          <line>127</line>\n          <option name=\"timeStamp\" value=\"98\" />\n        </line-breakpoint>\n      </breakpoints>\n      <default-breakpoints>\n        <breakpoint type=\"python-exception\">\n          <properties notifyOnTerminate=\"true\" exception=\"BaseException\">\n            <option name=\"notifyOnTerminate\" value=\"true\" />\n          </properties>\n        </breakpoint>\n      </default-breakpoints>\n    </breakpoint-manager>\n    <watches-manager>\n      <configuration name=\"PythonConfigurationType\">\n        <watch expression=\"race_type['course_info']\" language=\"Python\" />\n        <watch expression=\"musique_stats.race_types.__len__()\" />\n        <watch expression=\"musique_stats.__len__()\" />\n      </configuration>\n    </watches-manager>\n  </component>\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision 59920ec236ed6d1f3998260516c48f185e450296)
+++ b/.idea/workspace.xml	(date 1744035575388)
@@ -4,14 +4,10 @@
     <option name="autoReloadType" value="SELECTIVE" />
   </component>
   <component name="ChangeListManager">
-    <list default="true" id="43ec0894-de26-497b-a8fc-a968059a9170" name="Changes" comment="adding daily races sync">
-      <change afterPath="$PROJECT_DIR$/core/orchestrators/prediction_orchestrator.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/race_prediction/predict_daily_races.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/race_prediction/predict_orchestrator_cli.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/race_prediction/race_predict.py" afterDir="false" />
+    <list default="true" id="43ec0894-de26-497b-a8fc-a968059a9170" name="Changes" comment="VastAI config">
       <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/core/connectors/api_daily_sync.py" beforeDir="false" afterPath="$PROJECT_DIR$/core/connectors/api_daily_sync.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/race_prediction/predict_race.py" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/config.yaml" beforeDir="false" afterPath="$PROJECT_DIR$/config.yaml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/core/orchestrators/mysql_sqlite_sync.py" beforeDir="false" afterPath="$PROJECT_DIR$/core/orchestrators/mysql_sqlite_sync.py" afterDir="false" />
     </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
@@ -26,7 +22,13 @@
     </option>
   </component>
   <component name="Git.Settings">
+    <option name="RECENT_BRANCH_BY_REPOSITORY">
+      <map>
+        <entry key="$PROJECT_DIR$" value="LightGBM" />
+      </map>
+    </option>
     <option name="RECENT_GIT_ROOT_PATH" value="$PROJECT_DIR$" />
+    <option name="UPDATE_TYPE" value="REBASE" />
   </component>
   <component name="GitHubPullRequestSearchHistory">{
   &quot;lastFilter&quot;: {
@@ -61,12 +63,14 @@
     "Python.mysql_to_sqlite.executor": "Run",
     "Python.predict_daily_races.executor": "Run",
     "Python.prediction_orchestrator.executor": "Run",
+    "Python.regression_enhancement.executor": "Run",
     "Python.test.executor": "Debug",
     "Python.train_model.executor": "Debug",
     "Python.train_race_model.executor": "Run",
     "RunOnceActivity.ShowReadmeOnStart": "true",
     "RunOnceActivity.git.unshallow": "true",
-    "git-widget-placeholder": "main"
+    "git-widget-placeholder": "main",
+    "settings.editor.selected.configurable": "com.github.continuedev.continueintellijextension.services.ContinueExtensionConfigurable"
   }
 }]]></component>
   <component name="RecentsManager">
@@ -74,7 +78,7 @@
       <recent name="$PROJECT_DIR$/core/connectors" />
     </key>
   </component>
-  <component name="RunManager" selected="Python.predict_daily_races">
+  <component name="RunManager" selected="Python.mysql_sqlite_sync">
     <configuration name="api_daily_sync" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="HorseAIv2" />
       <option name="ENV_FILES" value="" />
@@ -97,6 +101,28 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
+    <configuration name="mysql_sqlite_sync" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+      <module name="HorseAIv2" />
+      <option name="ENV_FILES" value="" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/core/orchestrators/mysql_sqlite_sync.py" />
+      <option name="PARAMETERS" value="" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <option name="EMULATE_TERMINAL" value="false" />
+      <option name="MODULE_MODE" value="false" />
+      <option name="REDIRECT_INPUT" value="false" />
+      <option name="INPUT_FILE" value="" />
+      <method v="2" />
+    </configuration>
     <configuration name="predict_daily_races" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="HorseAIv2" />
       <option name="ENV_FILES" value="" />
@@ -141,7 +167,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="test" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="regression_enhancement" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="HorseAIv2" />
       <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -150,11 +176,11 @@
         <env name="PYTHONUNBUFFERED" value="1" />
       </envs>
       <option name="SDK_HOME" value="" />
-      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/" />
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/test.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/model_training/regressions/regression_enhancement.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -163,7 +189,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="train_race_model" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="train_race_model" type="PythonConfigurationType" factoryName="Python" nameIsGenerated="true">
       <module name="HorseAIv2" />
       <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -186,26 +212,27 @@
       <method v="2" />
     </configuration>
     <list>
+      <item itemvalue="Python.train_race_model" />
+      <item itemvalue="Python.mysql_sqlite_sync" />
+      <item itemvalue="Python.regression_enhancement" />
       <item itemvalue="Python.predict_daily_races" />
       <item itemvalue="Python.prediction_orchestrator" />
       <item itemvalue="Python.api_daily_sync" />
-      <item itemvalue="Python.train_race_model" />
-      <item itemvalue="Python.test" />
     </list>
     <recent_temporary>
       <list>
-        <item itemvalue="Python.predict_daily_races" />
+        <item itemvalue="Python.mysql_sqlite_sync" />
+        <item itemvalue="Python.api_daily_sync" />
         <item itemvalue="Python.prediction_orchestrator" />
-        <item itemvalue="Python.api_daily_sync" />
-        <item itemvalue="Python.train_race_model" />
-        <item itemvalue="Python.test" />
+        <item itemvalue="Python.predict_daily_races" />
+        <item itemvalue="Python.regression_enhancement" />
       </list>
     </recent_temporary>
   </component>
   <component name="SharedIndexes">
     <attachedChunks>
       <set>
-        <option value="bundled-python-sdk-fb887030ada0-aa17d162503b-com.jetbrains.pycharm.community.sharedIndexes.bundled-PC-243.21565.199" />
+        <option value="bundled-python-sdk-4f4e415b4190-aa17d162503b-com.jetbrains.pycharm.community.sharedIndexes.bundled-PC-243.26053.29" />
       </set>
     </attachedChunks>
   </component>
@@ -378,7 +405,39 @@
       <option name="project" value="LOCAL" />
       <updated>1743421163636</updated>
     </task>
-    <option name="localTasksCounter" value="21" />
+    <task id="LOCAL-00021" summary="Prediction and evaluation implementation is complete!">
+      <option name="closed" value="true" />
+      <created>1743598256848</created>
+      <option name="number" value="00021" />
+      <option name="presentableId" value="LOCAL-00021" />
+      <option name="project" value="LOCAL" />
+      <updated>1743598256848</updated>
+    </task>
+    <task id="LOCAL-00022" summary="Delta training done!">
+      <option name="closed" value="true" />
+      <created>1743665566400</created>
+      <option name="number" value="00022" />
+      <option name="presentableId" value="LOCAL-00022" />
+      <option name="project" value="LOCAL" />
+      <updated>1743665566400</updated>
+    </task>
+    <task id="LOCAL-00023" summary="VastAI config">
+      <option name="closed" value="true" />
+      <created>1744013431097</created>
+      <option name="number" value="00023" />
+      <option name="presentableId" value="LOCAL-00023" />
+      <option name="project" value="LOCAL" />
+      <updated>1744013431097</updated>
+    </task>
+    <task id="LOCAL-00024" summary="VastAI config">
+      <option name="closed" value="true" />
+      <created>1744014699360</created>
+      <option name="number" value="00024" />
+      <option name="presentableId" value="LOCAL-00024" />
+      <option name="project" value="LOCAL" />
+      <updated>1744014699360</updated>
+    </task>
+    <option name="localTasksCounter" value="25" />
     <servers />
   </component>
   <component name="VcsManagerConfiguration">
@@ -395,7 +454,10 @@
     <MESSAGE value="fixing feature storing and loading with testing" />
     <MESSAGE value="fixed feature embedding" />
     <MESSAGE value="adding daily races sync" />
-    <option name="LAST_COMMIT_MESSAGE" value="adding daily races sync" />
+    <MESSAGE value="Prediction and evaluation implementation is complete!" />
+    <MESSAGE value="Delta training done!" />
+    <MESSAGE value="VastAI config" />
+    <option name="LAST_COMMIT_MESSAGE" value="VastAI config" />
   </component>
   <component name="XDebuggerManager">
     <breakpoint-manager>
@@ -424,16 +486,6 @@
           <url>file://$PROJECT_DIR$/core/transformers/historical_race_transformer.py</url>
           <line>84</line>
           <option name="timeStamp" value="55" />
-        </line-breakpoint>
-        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
-          <url>file://$PROJECT_DIR$/core/orchestrators/mysql_sqlite_sync.py</url>
-          <line>25</line>
-          <option name="timeStamp" value="67" />
-        </line-breakpoint>
-        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
-          <url>file://$PROJECT_DIR$/core/orchestrators/embedding_feature.py</url>
-          <line>514</line>
-          <option name="timeStamp" value="92" />
         </line-breakpoint>
         <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
           <url>file://$PROJECT_DIR$/core/transformers/daily_race_transformer.py</url>
@@ -444,6 +496,16 @@
           <url>file://$PROJECT_DIR$/race_prediction/predict_daily_races.py</url>
           <line>127</line>
           <option name="timeStamp" value="98" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/model_training/regressions/isotonic_calibration.py</url>
+          <line>329</line>
+          <option name="timeStamp" value="102" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/model_training/historical/train_race_model.py</url>
+          <line>761</line>
+          <option name="timeStamp" value="111" />
         </line-breakpoint>
       </breakpoints>
       <default-breakpoints>
Index: core/orchestrators/mysql_sqlite_sync.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from core.connectors.mysql_historical_fetcher import fetch_race_data\nfrom core.transformers.historical_race_transformer import transform_race_data, transform_results\nfrom core.connectors.sqlite_historical_writer import write_races_to_sqlite, write_results_to_sqlite\nfrom utils.env_setup import AppConfig\n\nfrom core.connectors.mysql_connector import connect_to_mysql\n\n\ndef sync_data(mysql_dbname=None):\n    \"\"\"\n    Main function to synchronize data from MySQL to SQLite.\n\n    Args:\n        mysql_dbname: Optional name of MySQL database\n    \"\"\"\n    config = AppConfig()\n    # Connect to data sources\n    mysql_conn = connect_to_mysql(mysql_dbname)\n    sqlite_db_path = config.get_active_db_path()\n\n\n    # Fetch data\n    df_raw_data = fetch_race_data(mysql_conn)\n\n    # Transform race data\n    course_infos, participants = transform_race_data(df_raw_data)\n\n    # Transform results data\n    results = transform_results(df_raw_data)\n\n    # Write data to SQLite\n    write_races_to_sqlite(sqlite_db_path, course_infos, participants)\n    write_results_to_sqlite(sqlite_db_path, results)\n\n    # Clean up\n    mysql_conn.close()\n    print(\"Data synchronization completed successfully.\")\n\n\nif __name__ == \"__main__\":\n    sync_data('pturf2020')
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/core/orchestrators/mysql_sqlite_sync.py b/core/orchestrators/mysql_sqlite_sync.py
--- a/core/orchestrators/mysql_sqlite_sync.py	(revision 59920ec236ed6d1f3998260516c48f185e450296)
+++ b/core/orchestrators/mysql_sqlite_sync.py	(date 1744022651985)
@@ -38,4 +38,4 @@
 
 
 if __name__ == "__main__":
-    sync_data('pturf2020')
\ No newline at end of file
+    sync_data('pturf2024')
\ No newline at end of file

Index: race_prediction/race_predict.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport numpy as np\nimport pandas as pd\nimport json\nimport joblib\nfrom pathlib import Path\nfrom typing import Dict, List, Union, Any, Optional, Tuple\nimport logging\nfrom datetime import datetime\n\n# Import from existing code\nfrom utils.env_setup import AppConfig, get_sqlite_dbpath\nfrom core.orchestrators.embedding_feature import FeatureEmbeddingOrchestrator\nfrom model_training.regressions.isotonic_calibration import CalibratedRegressor\n\n\nclass RacePredictor:\n    \"\"\"\n    Race predictor that loads trained models and applies them to new race data.\n    Handles both static models (Random Forest) and sequence models (LSTM) if available.\n    \"\"\"\n\n    def __init__(self, model_path: str = None, db_name: str = None,\n                 use_latest_base: bool = False, verbose: bool = False):\n        \"\"\"\n        Initialize the race predictor.\n\n        Args:\n            model_path: Path to saved model directory (use None with use_latest_base=True to use latest from config)\n            db_name: Database configuration name from config\n            use_latest_base: Whether to use the latest base model from config\n            verbose: Whether to print verbose output\n        \"\"\"\n        # Initialize config\n        self.config = AppConfig()\n\n        # If use_latest_base is True, try to get latest model from config\n        if use_latest_base:\n            try:\n                if hasattr(self.config._config.models, 'latest_base_model'):\n                    latest_model = self.config._config.models.latest_base_model\n                    # Construct full path to model\n                    base_model_dir = os.path.join(\n                        self.config._config.models.model_dir,\n                        'hybrid'  # Default model name\n                    )\n                    self.model_path = Path(base_model_dir) / latest_model\n                    print(f\"Using latest base model from config: {self.model_path}\")\n                else:\n                    # Fall back to provided model_path\n                    self.model_path = Path(model_path) if model_path else None\n                    print(\"No latest_base_model found in config, using specified model path\")\n            except (AttributeError, TypeError) as e:\n                print(f\"Error loading latest_base_model from config: {str(e)}\")\n                self.model_path = Path(model_path) if model_path else None\n        else:\n            # Use specified model path\n            self.model_path = Path(model_path) if model_path else None\n\n        # Check if model path exists\n        if self.model_path is None:\n            raise ValueError(\"No model path provided and couldn't get latest_base_model from config\")\n\n        if not self.model_path.exists():\n            raise FileNotFoundError(f\"Model path {self.model_path} does not exist\")\n\n        self.verbose = verbose\n        # Get database path from config\n        self.db_path = get_sqlite_dbpath(db_name)\n\n        # Initialize orchestrator for feature embedding\n        self.orchestrator = FeatureEmbeddingOrchestrator(\n            sqlite_path=self.db_path,\n            verbose=verbose\n        )\n\n        # Set up logging with proper verbose control\n        self._setup_logging()\n\n        # Load models and configuration\n        self._load_models()\n\n        if self.verbose:\n            self.log_info(f\"Race predictor initialized with model at {self.model_path}\")\n            self.log_info(f\"Using database: {self.db_path}\")\n\n    def _setup_logging(self):\n        \"\"\"Set up logging with proper verbose control.\"\"\"\n        # Create logs directory if it doesn't exist\n        log_dir = self.model_path / \"logs\"\n        log_dir.mkdir(parents=True, exist_ok=True)\n\n        # Get or create logger\n        self.logger = logging.getLogger(\"RacePredictor\")\n\n        # Remove any existing handlers to avoid duplicates\n        for handler in list(self.logger.handlers):\n            self.logger.removeHandler(handler)\n\n        # Set level based on verbose flag\n        self.logger.setLevel(logging.INFO if self.verbose else logging.WARNING)\n\n        # Add file handler (always log to file)\n        log_file = log_dir / f\"race_predictor_{datetime.now().strftime('%Y%m%d')}.log\"\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n        self.logger.addHandler(file_handler)\n\n        # Add console handler only if verbose is True\n        if self.verbose:\n            console_handler = logging.StreamHandler()\n            console_handler.setFormatter(logging.Formatter('%(name)s - %(levelname)s - %(message)s'))\n            self.logger.addHandler(console_handler)\n\n    def log_info(self, message):\n        \"\"\"Log an info message.\"\"\"\n        if hasattr(self, 'logger'):\n            self.logger.info(message)\n        elif self.verbose:  # Only print if verbose when logger not initialized\n            print(message)\n\n    def log_error(self, message):\n        \"\"\"Log an error message.\"\"\"\n        if hasattr(self, 'logger'):\n            self.logger.error(message)\n        else:  # Always print errors even without logger\n            print(f\"ERROR: {message}\")\n    def _load_models(self):\n        # Get model manager\n        from utils.model_manager import get_model_manager\n        model_manager = get_model_manager()\n\n        # Load models\n        artifacts = model_manager.load_model_artifacts(\n            base_path=self.model_path,\n            load_rf=True,\n            load_lstm=True,\n            load_feature_config=True\n        )\n\n        # Set model attributes from loaded artifacts\n        if 'rf_model' in artifacts:\n            self.rf_model = artifacts['rf_model']\n            self.log_info(f\"Loaded RF model\")\n        else:\n            self.rf_model = None\n            self.log_info(\"RF model not available\")\n\n        if 'lstm_model' in artifacts:\n            self.lstm_model = artifacts['lstm_model']\n            self.log_info(f\"Loaded LSTM model\")\n        else:\n            self.lstm_model = None\n            self.log_info(\"LSTM model not available\")\n\n        if 'feature_config' in artifacts:\n            self.feature_config = artifacts['feature_config']\n            self.log_info(f\"Loaded feature configuration\")\n        else:\n            self.feature_config = {}\n\n        if 'model_config' in artifacts:\n            self.model_config = artifacts['model_config']\n            self.log_info(f\"Loaded model configuration\")\n\n    def prepare_race_data(self, race_df: pd.DataFrame) -> Tuple[\n        pd.DataFrame, Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"\n        Prepare race data for prediction by applying feature engineering and embeddings.\n        \"\"\"\n        # Prepare RF features\n        X = self._prepare_rf_features(race_df)  # Extract current RF preparation logic to this method\n\n        # Prepare LSTM features if the model is available\n        X_seq, X_static = None, None\n        if self.lstm_model is not None:\n            X_seq, X_static = self.prepare_lstm_race_data(race_df)\n\n        return X, X_seq, X_static\n    def _prepare_rf_features(self, race_df: pd.DataFrame) -> Tuple[\n        pd.DataFrame, Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"\n        Prepare race data for prediction by applying feature engineering and embeddings.\n\n        Args:\n            race_df: DataFrame with race and participant data\n\n        Returns:\n            Tuple of (processed features DataFrame, sequence features, static features)\n        \"\"\"\n        # Make a copy to avoid modifying the original\n        df = race_df.copy()\n\n        # Add missing columns that are expected by the feature engineering pipeline\n        missing_cols = []\n        expected_cols = ['idche', 'idJockey', 'cheval', 'cotedirect', 'numero']\n\n        for col in expected_cols:\n            if col not in df.columns:\n                missing_cols.append(col)\n                df[col] = None\n\n        if missing_cols:\n            self.log_info(f\"Added missing columns for feature engineering: {missing_cols}\")\n\n        # Convert numeric fields\n        numeric_fields = [\n            'age', 'cotedirect', 'coteprob', 'pourcVictChevalHippo',\n            'pourcPlaceChevalHippo', 'pourcVictJockHippo', 'pourcPlaceJockHippo',\n            'victoirescheval', 'placescheval', 'coursescheval', 'gainsCarriere',\n            'gainsAnneeEnCours', 'nbCourseCouple', 'nbVictCouple', 'nbPlaceCouple',\n            'TxVictCouple', 'recence', 'dist', 'temperature', 'forceVent',\n            'ratio_victoires', 'ratio_places', 'gains_par_course',\n            'efficacite_couple', 'regularite_couple', 'progression_couple',\n            'perf_cheval_hippo', 'perf_jockey_hippo'\n        ]\n\n        for field in numeric_fields:\n            if field in df.columns and not pd.api.types.is_numeric_dtype(df[field]):\n                df[field] = pd.to_numeric(df[field], errors='coerce')\n\n        # Prepare features using orchestrator\n        self.log_info(\"Preparing features...\")\n\n        # First apply feature calculator for static features\n        try:\n            from core.calculators.static_feature_calculator import FeatureCalculator\n            df = FeatureCalculator.calculate_all_features(df)\n            self.log_info(f\"Applied static feature calculations\")\n        except Exception as e:\n            self.log_error(f\"Error applying static feature calculations: {str(e)}\")\n\n        # Apply embeddings\n        try:\n            df = self.orchestrator.apply_embeddings(df, clean_after_embedding=False)\n            self.log_info(f\"Applied embeddings\")\n        except Exception as e:\n            self.log_error(f\"Error applying embeddings: {str(e)}\")\n\n        # Clean up features for RF model\n        X = self.orchestrator.drop_embedded_raw_features(df)\n        self.log_info(f\"Prepared {len(X)} samples with {X.shape[1]} features for RF model\")\n\n        # Prepare sequence data for LSTM model if available\n        X_seq = None\n        X_static = None\n\n        if self.lstm_model is not None and hasattr(self.orchestrator, 'prepare_sequence_data'):\n            try:\n                # Get sequence length from model config or use default\n                sequence_length = self.feature_config.get('sequence_length', 5) if self.feature_config else 5\n\n                self.log_info(f\"Preparing sequence data with length {sequence_length}...\")\n                X_seq, X_static, _ = self.orchestrator.prepare_sequence_data(\n                    df, sequence_length=sequence_length\n                )\n                self.log_info(\n                    f\"Prepared sequence data with shape {X_seq.shape} and static data with shape {X_static.shape}\")\n            except Exception as e:\n                self.log_error(f\"Error preparing sequence data: {str(e)}\")\n                X_seq = None\n                X_static = None\n        self.log_info(f\"X shape: {X.shape if X is not None else 'None'}\")\n        self.log_info(f\"X_seq shape: {X_seq.shape if X_seq is not None else 'None'}\")\n        self.log_info(f\"X_static shape: {X_static.shape if X_static is not None else 'None'}\")\n        self.log_info(f\"LSTM model available: {self.lstm_model is not None}\")\n        return X, X_seq, X_static\n\n    def fetch_horse_sequences(self, race_df, sequence_length=None):\n        \"\"\"\n        Fetch historical race sequences for all horses in a race to enable LSTM prediction.\n\n        Args:\n            race_df: DataFrame with the current race data (containing horses to predict)\n            sequence_length: Length of sequence to retrieve (uses default if None)\n\n        Returns:\n            Tuple of (X_seq, X_static, horse_ids) for sequence prediction\n        \"\"\"\n        if sequence_length is None:\n            sequence_length = self.feature_config.get('sequence_length', 5) if self.feature_config else 5\n\n        self.log_info(f\"Fetching historical sequences of length {sequence_length} for horses in race\")\n\n        # Get all horse IDs from the race\n        horse_ids = []\n        if 'idche' in race_df.columns:\n            # Filter out missing or invalid IDs\n            horse_ids = [int(h) for h in race_df['idche'] if pd.notna(h)]\n\n        if not horse_ids:\n            self.log_error(\"No valid horse IDs found in race data\")\n            return None, None, None\n\n        self.log_info(f\"Found {len(horse_ids)} horses to fetch sequences for\")\n\n        # Connect to the database\n        try:\n            conn = sqlite3.connect(self.db_path)\n            conn.row_factory = sqlite3.Row  # This enables column access by name\n            cursor = conn.cursor()\n\n            # Define sequence and static features (match training configuration)\n            # These should match what was used in training\n            sequential_features = [\n                'final_position', 'cotedirect', 'dist',\n                # Include embeddings if available\n                'horse_emb_0', 'horse_emb_1', 'horse_emb_2',\n                'jockey_emb_0', 'jockey_emb_1', 'jockey_emb_2',\n                # Add musique-derived features\n                'che_global_avg_pos', 'che_global_recent_perf', 'che_global_consistency', 'che_global_pct_top3',\n                'che_weighted_avg_pos', 'che_weighted_recent_perf', 'che_weighted_consistency', 'che_weighted_pct_top3'\n            ]\n\n            static_features = [\n                'age', 'temperature', 'natpis', 'typec', 'meteo', 'corde',\n                'couple_emb_0', 'couple_emb_1', 'couple_emb_2',\n                'course_emb_0', 'course_emb_1', 'course_emb_2'\n            ]\n\n            # Prepare containers for sequences\n            all_sequences = []\n            all_static_features = []\n            all_horse_ids = []\n\n            # For each horse, retrieve its historical races\n            for horse_id in horse_ids:\n                self.log_info(f\"Processing historical data for horse {horse_id}\")\n\n                # Fetch historical races for this horse\n                # Note: SQL query filters races before the current race date\n                # to prevent data leakage\n\n                # First get the current race date to use as a cutoff\n                current_race_date = None\n                if 'jour' in race_df.columns:\n                    # Try to get the date from the current race data\n                    current_race_date = race_df['jour'].iloc[0] if len(race_df) > 0 else None\n\n                # Determine SQL date filter based on current race date\n                date_filter = \"\"\n                if current_race_date:\n                    # Convert to proper date format if needed\n                    if isinstance(current_race_date, str):\n                        date_filter = f\" AND hr.jour < '{current_race_date}'\"\n                    else:\n                        # Try to format as a date string\n                        try:\n                            date_str = current_race_date.strftime('%Y-%m-%d')\n                            date_filter = f\" AND hr.jour < '{date_str}'\"\n                        except:\n                            # If formatting fails, don't use a date filter\n                            pass\n\n                # Create query to find races with this horse\n                query = f\"\"\"\n                SELECT hr.* \n                FROM historical_races hr\n                WHERE hr.participants LIKE ?\n                {date_filter}\n                ORDER BY hr.jour DESC\n                LIMIT 20\n                \"\"\"\n\n                # Execute query\n                cursor.execute(query, (f'%\"idche\": {horse_id}%',))\n                horse_races = cursor.fetchall()\n\n                if not horse_races:\n                    self.log_info(f\"No historical races found for horse {horse_id}\")\n                    continue\n\n                self.log_info(f\"Found {len(horse_races)} historical races for horse {horse_id}\")\n\n                # Extract and process participant data for this horse\n                horse_data = []\n                for race in horse_races:\n                    try:\n                        # Parse participant JSON\n                        participants = json.loads(race['participants'])\n\n                        # Find this horse in the participants\n                        horse_entry = next((p for p in participants if int(p.get('idche', 0)) == horse_id), None)\n\n                        if horse_entry:\n                            # Add race attributes to horse entry\n                            for key in ['jour', 'hippo', 'dist', 'typec', 'temperature', 'natpis', 'meteo', 'corde']:\n                                if key in race:\n                                    horse_entry[key] = race[key]\n\n                            # If there's a race result, try to get the final position for this horse\n                            if 'ordre_arrivee' in race and race['ordre_arrivee']:\n                                try:\n                                    results = json.loads(race['ordre_arrivee'])\n                                    # Find this horse's position\n                                    horse_result = next((r for r in results if int(r.get('cheval', 0)) == horse_id),\n                                                        None)\n                                    if horse_result:\n                                        horse_entry['final_position'] = horse_result.get('narrivee')\n                                except:\n                                    # If we can't parse results, skip\n                                    pass\n\n                            horse_data.append(horse_entry)\n                    except:\n                        # If we can't parse participants, skip this race\n                        continue\n\n                # Convert to DataFrame for easier processing\n                if not horse_data:\n                    self.log_info(f\"No historical data could be extracted for horse {horse_id}\")\n                    continue\n\n                horse_df = pd.DataFrame(horse_data)\n\n                # Sort by date\n                if 'jour' in horse_df.columns:\n                    horse_df['jour'] = pd.to_datetime(horse_df['jour'], errors='coerce')\n                    horse_df = horse_df.sort_values('jour', ascending=False)\n\n                # Apply feature engineering to get embeddings\n                try:\n                    # Use orchestrator to process features\n                    processed_df = self.orchestrator.prepare_features(horse_df)\n                    embedded_df = self.orchestrator.apply_embeddings(processed_df, clean_after_embedding=False)\n\n                    # Check if we have enough races for a sequence\n                    if len(embedded_df) >= sequence_length:\n                        # Extract sequential features (only keep those that exist in the DataFrame)\n                        seq_features = [f for f in sequential_features if f in embedded_df.columns]\n\n                        if not seq_features:\n                            self.log_info(f\"No sequential features found for horse {horse_id}\")\n                            continue\n\n                        # Get sequence data (take first sequence_length races)\n                        seq_data = embedded_df[seq_features].head(sequence_length).values.astype(np.float32)\n\n                        # Make sure we have the right sequence length\n                        if len(seq_data) < sequence_length:\n                            # Pad with zeros if needed\n                            padding = np.zeros((sequence_length - len(seq_data), len(seq_features)), dtype=np.float32)\n                            seq_data = np.vstack([seq_data, padding])\n\n                        # Get static features from current race for this horse\n                        current_horse = race_df[race_df['idche'] == horse_id]\n\n                        if len(current_horse) > 0:\n                            # Start with empty array for static features\n                            static_data = np.zeros(len(static_features), dtype=np.float32)\n\n                            # Fill in available static features from current race\n                            for i, feature in enumerate(static_features):\n                                if feature in current_horse.columns:\n                                    try:\n                                        val = current_horse[feature].iloc[0]\n                                        if pd.notna(val):\n                                            static_data[i] = float(val)\n                                    except:\n                                        pass\n\n                            # Add to output containers\n                            all_sequences.append(seq_data)\n                            all_static_features.append(static_data)\n                            all_horse_ids.append(horse_id)\n\n                            self.log_info(f\"Successfully created sequence for horse {horse_id}\")\n                        else:\n                            self.log_info(f\"Horse {horse_id} not found in current race data\")\n                    else:\n                        self.log_info(\n                            f\"Not enough historical races for horse {horse_id} (found {len(embedded_df)}, need {sequence_length})\")\n                except Exception as e:\n                    self.log_error(f\"Error processing horse {horse_id}: {str(e)}\")\n                    import traceback\n                    self.log_error(traceback.format_exc())\n\n            conn.close()\n\n            # Convert to numpy arrays\n            if not all_sequences:\n                self.log_info(f\"No valid sequences could be created for any horse\")\n                return None, None, None\n\n            X_sequences = np.array(all_sequences, dtype=np.float32)\n            X_static = np.array(all_static_features, dtype=np.float32)\n            sequence_horse_ids = np.array(all_horse_ids)\n\n            self.log_info(f\"Created {len(X_sequences)} sequences for {len(np.unique(sequence_horse_ids))} horses\")\n            self.log_info(f\"Sequence shape: {X_sequences.shape}, Static shape: {X_static.shape}\")\n\n            return X_sequences, X_static, sequence_horse_ids\n\n        except Exception as e:\n            self.log_error(f\"Error in fetch_horse_sequences: {str(e)}\")\n            import traceback\n            self.log_error(traceback.format_exc())\n            return None, None, None\n\n    def prepare_lstm_race_data(self, race_df: pd.DataFrame) -> Tuple[\n        Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"\n        Prepare race data specifically for LSTM prediction.\n        Enhanced to fetch historical sequences for horses.\n\n        Args:\n            race_df: DataFrame with race and participant data\n\n        Returns:\n            Tuple of (sequence features, static features, horse_ids)\n        \"\"\"\n        # Make a copy to avoid modifying the original\n        df = race_df.copy()\n\n        # Apply basic feature engineering but preserve 'jour' and 'idche'\n        df = self.orchestrator.apply_embeddings(df, clean_after_embedding=True, keep_identifiers=True)\n\n        # Try preparing sequence data using the traditional method first (for backward compatibility)\n        try:\n            # Get sequence length from model config or use default\n            sequence_length = self.feature_config.get('sequence_length', 5) if self.feature_config else 5\n\n            self.log_info(f\"Trying standard sequence preparation with length {sequence_length}...\")\n            # Note: Removed the '*' which might be a typo in your original code\n            X_seq, X_static, _ = self.orchestrator.prepare_sequence_data(\n                df, sequence_length=sequence_length\n            )\n            self.log_info(\n                f\"Successfully prepared LSTM sequence data with shape {X_seq.shape} and static data with shape {X_static.shape}\")\n\n            # This succeeded, so return the results without horse IDs (since the standard method doesn't provide them)\n            return X_seq, X_static, None\n\n        except Exception as e:\n            self.log_info(\n                f\"Standard sequence preparation failed: {str(e)} - Attempting to fetch historical horse sequences\")\n\n        # If the standard method failed, try our new approach that fetches historical data\n        try:\n            # Get sequence length from model config or use default\n            sequence_length = self.feature_config.get('sequence_length', 5) if self.feature_config else 5\n\n            self.log_info(f\"Fetching historical horse sequences with length {sequence_length}...\")\n            X_seq, X_static, horse_ids = self.fetch_horse_sequences(df, sequence_length=sequence_length)\n\n            if X_seq is not None and X_static is not None:\n                self.log_info(\n                    f\"Successfully fetched LSTM sequence data with shape {X_seq.shape} and static data with shape {X_static.shape}\")\n                return X_seq, X_static, horse_ids\n            else:\n                self.log_error(\"Could not create sequences from historical data\")\n                return None, None, None\n\n        except Exception as e:\n            self.log_error(f\"Error preparing LSTM sequence data: {str(e)}\")\n            import traceback\n            self.log_error(traceback.format_exc())\n            return None, None, None\n\n    def predict_with_rf(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Make predictions using the Random Forest model with feature name alignment.\n        \"\"\"\n        if self.rf_model is None:\n            self.log_error(\"Random Forest model not loaded\")\n            return np.zeros(len(X))\n\n        try:\n            # Check if we have feature names from training\n            expected_features = None\n\n            # Try to get expected feature names from different places\n            if hasattr(self.feature_config,\n                       'preprocessing_params') and 'feature_columns' in self.feature_config.preprocessing_params:\n                expected_features = self.feature_config.preprocessing_params['feature_columns']\n                self.log_info(f\"Found {len(expected_features)} expected feature columns from config\")\n            elif hasattr(self.rf_model, 'feature_names_in_'):\n                expected_features = self.rf_model.feature_names_in_\n                self.log_info(f\"Found {len(expected_features)} feature names from model\")\n            elif hasattr(self.rf_model, 'base_regressor') and hasattr(self.rf_model.base_regressor,\n                                                                      'feature_names_in_'):\n                expected_features = self.rf_model.base_regressor.feature_names_in_\n                self.log_info(f\"Found {len(expected_features)} feature names from base_regressor\")\n\n            # If we have expected features, align the input data\n            if expected_features is not None:\n                # Create a DataFrame with expected feature columns filled with zeros\n                aligned_X = pd.DataFrame(0, index=range(len(X)), columns=expected_features)\n\n                # Copy values for columns that exist in both DataFrames\n                common_features = set(X.columns) & set(expected_features)\n                self.log_info(f\"Found {len(common_features)} common features out of {len(expected_features)} expected\")\n\n                for feature in common_features:\n                    aligned_X[feature] = X[feature].values\n\n                # Use the aligned DataFrame for prediction\n                X_for_prediction = aligned_X\n            else:\n                # No expected features found, use original data (will likely fail)\n                self.log_info(\"No expected feature list found, using original features\")\n                X_for_prediction = X\n\n            # Make prediction with appropriate model\n            preds = self.rf_model.predict(X_for_prediction)\n            self.log_info(f\"Generated predictions for {len(X)} samples\")\n            return preds\n\n        except Exception as e:\n            self.log_error(f\"Error generating RF predictions: {str(e)}\")\n            import traceback\n            self.log_error(traceback.format_exc())\n            return np.zeros(len(X))\n\n    def predict_with_lstm(self, X_seq: np.ndarray, X_static: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Make predictions using the LSTM model.\n\n        Args:\n            X_seq: Sequence features\n            X_static: Static features\n\n        Returns:\n            NumPy array with predictions\n        \"\"\"\n        if self.lstm_model is None:\n            self.log_error(\"LSTM model not loaded\")\n            return np.zeros(len(X_seq))\n\n        if X_seq is None or X_static is None:\n            self.log_error(\"Sequence data not available\")\n            return np.zeros(len(X_seq) if X_seq is not None else 0)\n\n        try:\n            preds = self.lstm_model.predict([X_seq, X_static], verbose=0)\n\n            # Flatten predictions if needed\n            if len(preds.shape) > 1:\n                preds = preds.flatten()\n\n            self.log_info(f\"Generated LSTM predictions for {len(preds)} samples\")\n            return preds\n        except Exception as e:\n            self.log_error(f\"Error generating LSTM predictions: {str(e)}\")\n            return np.zeros(len(X_seq))\n\n    def predict(self, X: pd.DataFrame, X_seq: Optional[np.ndarray] = None,\n                X_static: Optional[np.ndarray] = None, blend_weight: float = 0.7) -> np.ndarray:\n        \"\"\"\n        Make predictions using available models, optionally blending results.\n\n        Args:\n            X: Feature DataFrame for RF model\n            X_seq: Sequence features for LSTM model (optional)\n            X_static: Static features for LSTM model (optional)\n            blend_weight: Weight for RF model in blended predictions (0-1)\n\n        Returns:\n            NumPy array with predictions\n        \"\"\"\n        # Get RF predictions\n        if self.lstm_model is None:\n            self.log_info(\"LSTM model is None, no blending will occur\")\n        if X_seq is None:\n            self.log_info(\"X_seq is None, no blending will occur\")\n        if X_static is None:\n            self.log_info(\"X_static is None, no blending will occur\")\n\n        rf_preds = self.predict_with_rf(X)\n\n        # Get LSTM predictions if possible\n        lstm_preds = None\n        if self.lstm_model is not None and X_seq is not None and X_static is not None:\n            lstm_preds = self.predict_with_lstm(X_seq, X_static)\n            self.log_info(f\"lstm prediction is {lstm_preds}\")\n\n            # Make sure shapes match\n            if len(lstm_preds) != len(rf_preds):\n                self.log_error(f\"Shape mismatch: RF {len(rf_preds)}, LSTM {len(lstm_preds)}\")\n                lstm_preds = None\n\n        # Blend predictions if both models are available\n        if lstm_preds is not None:\n            self.log_info(f\"Blending predictions with weight {blend_weight} for RF\")\n            final_preds = rf_preds * blend_weight + lstm_preds * (1 - blend_weight)\n            self.log_info(f\"Predict before blending: RF= {rf_preds} | LSTM={lstm_preds} and after: {final_preds} with weight {blend_weight}\")\n        else:\n            final_preds = rf_preds\n\n        return final_preds\n\n    def predict_race(self, race_df: pd.DataFrame, blend_weight: float = 0.7) -> pd.DataFrame:\n        \"\"\"\n        Predict race outcome with arrival string format.\n\n        Args:\n            race_df: DataFrame with race data\n            blend_weight: Weight for RF model in blended predictions (0-1)\n\n        Returns:\n            DataFrame with predictions\n        \"\"\"\n        # Prepare data for prediction\n        X, X_seq, X_static = self.prepare_race_data(race_df)\n\n        # Make predictions\n        predictions = self.predict(X, X_seq, X_static, blend_weight)\n\n        # Add predictions to original data\n        result_df = race_df.copy()\n        result_df['predicted_position'] = predictions\n\n        # Sort by predicted position (ascending, better positions first)\n        result_df = result_df.sort_values('predicted_position')\n\n        # Add rank column\n        result_df['predicted_rank'] = range(1, len(result_df) + 1)\n\n        # Create arrival string format (numero-numero-numero)\n        # This follows the same format as 'arriv' in actual results\n        numeros_ordered = result_df['numero'].astype(str).tolist()\n        predicted_arriv = '-'.join(numeros_ordered)\n\n        # Add the predicted_arriv as a column to each row\n        result_df['predicted_arriv'] = predicted_arriv\n\n        return result_df
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/race_prediction/race_predict.py b/race_prediction/race_predict.py
--- a/race_prediction/race_predict.py	(revision 0d88e73ffd4a3899fde119bb4ed28c1bf49fab9b)
+++ b/race_prediction/race_predict.py	(date 1746189325727)
@@ -7,7 +7,8 @@
 from typing import Dict, List, Union, Any, Optional, Tuple
 import logging
 from datetime import datetime
-
+import time
+import sqlite3
 # Import from existing code
 from utils.env_setup import AppConfig, get_sqlite_dbpath
 from core.orchestrators.embedding_feature import FeatureEmbeddingOrchestrator
@@ -83,6 +84,7 @@
         if self.verbose:
             self.log_info(f"Race predictor initialized with model at {self.model_path}")
             self.log_info(f"Using database: {self.db_path}")
+        self.prediction_cache = {}
 
     def _setup_logging(self):
         """Set up logging with proper verbose control."""
@@ -166,17 +168,29 @@
     def prepare_race_data(self, race_df: pd.DataFrame) -> Tuple[
         pd.DataFrame, Optional[np.ndarray], Optional[np.ndarray]]:
         """
-        Prepare race data for prediction by applying feature engineering and embeddings.
+        Prepare race data for prediction, returning processed dataframes for both RF and LSTM models.
+
+        Args:
+            race_df: DataFrame with race data
+
+        Returns:
+            Tuple of (rf_features, sequence_features, static_features)
         """
-        # Prepare RF features
-        X = self._prepare_rf_features(race_df)  # Extract current RF preparation logic to this method
+        # Prepare features for RF model
+        rf_features = self._prepare_rf_features(race_df)
 
-        # Prepare LSTM features if the model is available
-        X_seq, X_static = None, None
+        # Prepare data for LSTM model if available
+        lstm_seq_features = None
+        lstm_static_features = None
+
         if self.lstm_model is not None:
-            X_seq, X_static = self.prepare_lstm_race_data(race_df)
+            try:
+                lstm_seq_features, lstm_static_features = self.prepare_lstm_race_data(race_df)
+            except Exception as e:
+                self.log_error(f"Error preparing LSTM features: {str(e)}")
 
-        return X, X_seq, X_static
+        return rf_features, lstm_seq_features, lstm_static_features
+
     def _prepare_rf_features(self, race_df: pd.DataFrame) -> Tuple[
         pd.DataFrame, Optional[np.ndarray], Optional[np.ndarray]]:
         """
@@ -692,6 +706,7 @@
     def predict_race(self, race_df: pd.DataFrame, blend_weight: float = 0.7) -> pd.DataFrame:
         """
         Predict race outcome with arrival string format.
+        Enhanced with caching for efficient re-blending.
 
         Args:
             race_df: DataFrame with race data
@@ -700,15 +715,76 @@
         Returns:
             DataFrame with predictions
         """
-        # Prepare data for prediction
-        X, X_seq, X_static = self.prepare_race_data(race_df)
+        # Generate a cache key from race DataFrame
+        comp = race_df['comp'].iloc[0] if 'comp' in race_df.columns else None
+        cache_key = str(comp) if comp else race_df['cheval'].astype(str).sum()
+
+        # Check if we have cached predictions for this race
+        if cache_key in self.prediction_cache:
+            cache_entry = self.prediction_cache[cache_key]
+
+            # Check if we have the raw predictions needed for reblending
+            if 'rf_predictions' in cache_entry and 'lstm_predictions' in cache_entry:
+                self.log_info(f"Using cached predictions with blend_weight={blend_weight}")
+
+                # Get raw predictions from cache
+                rf_predictions = cache_entry['rf_predictions']
+                lstm_predictions = cache_entry['lstm_predictions']
+
+                # Apply blending with new weight
+                if lstm_predictions is not None and len(lstm_predictions) == len(rf_predictions):
+                    final_predictions = rf_predictions * blend_weight + lstm_predictions * (1 - blend_weight)
+                    self.log_info(
+                        f"Reblended predictions with weight {blend_weight} for RF, {1 - blend_weight} for LSTM")
+                else:
+                    final_predictions = rf_predictions
+                    self.log_info("Using only RF predictions (LSTM not available)")
+
+                # Use the cached DataFrame for results
+                result_df = cache_entry['result_df'].copy()
+
+                # Update with new blended predictions
+                result_df['predicted_position'] = final_predictions
+
+                # Re-sort by the new predictions
+                result_df = result_df.sort_values('predicted_position')
+
+                # Re-calculate ranks
+                result_df['predicted_rank'] = range(1, len(result_df) + 1)
+
+                # Recreate arrival string
+                numeros_ordered = result_df['numero'].astype(str).tolist()
+                predicted_arriv = '-'.join(numeros_ordered)
+                result_df['predicted_arriv'] = predicted_arriv
+
+                return result_df
 
-        # Make predictions
-        predictions = self.predict(X, X_seq, X_static, blend_weight)
+        # If no cache hit or incomplete cache, proceed with normal prediction
+        start_time = time.time()
 
-        # Add predictions to original data
+        # Step 1: Prepare data for both models
+        rf_features, lstm_seq, lstm_static = self.prepare_race_data(race_df)
+
+        # Step 2: Predict with RF model
+        rf_predictions = self.predict_with_rf(rf_features)
+
+        # Step 3: Predict with LSTM model if available
+        lstm_predictions = None
+        if self.lstm_model is not None and lstm_seq is not None and lstm_static is not None:
+            try:
+                lstm_predictions = self.predict_with_lstm(lstm_seq, lstm_static)
+            except Exception as e:
+                self.log_error(f"Error in LSTM prediction: {str(e)}")
+
+        # Step 4: Blend predictions if both models produced results
+        final_predictions = rf_predictions
+        if lstm_predictions is not None and len(lstm_predictions) == len(rf_predictions):
+            self.log_info(f"Blending predictions with weight {blend_weight} for RF, {1 - blend_weight} for LSTM")
+            final_predictions = rf_predictions * blend_weight + lstm_predictions * (1 - blend_weight)
+
+        # Step 5: Add predictions to result DataFrame and format
         result_df = race_df.copy()
-        result_df['predicted_position'] = predictions
+        result_df['predicted_position'] = final_predictions
 
         # Sort by predicted position (ascending, better positions first)
         result_df = result_df.sort_values('predicted_position')
@@ -717,11 +793,21 @@
         result_df['predicted_rank'] = range(1, len(result_df) + 1)
 
         # Create arrival string format (numero-numero-numero)
-        # This follows the same format as 'arriv' in actual results
         numeros_ordered = result_df['numero'].astype(str).tolist()
         predicted_arriv = '-'.join(numeros_ordered)
 
         # Add the predicted_arriv as a column to each row
         result_df['predicted_arriv'] = predicted_arriv
 
+        # Store in cache for future reblending
+        self.prediction_cache[cache_key] = {
+            'rf_predictions': rf_predictions,
+            'lstm_predictions': lstm_predictions,
+            'result_df': result_df,  # Store a copy of the result DataFrame
+            'time': time.time()
+        }
+
+        prediction_time = time.time() - start_time
+        self.log_info(f"Generated predictions in {prediction_time:.3f} seconds")
+
         return result_df
\ No newline at end of file
Index: core/orchestrators/prediction_orchestrator.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport pandas as pd\nimport numpy as np\nimport json\nfrom typing import Dict, List, Union, Any, Optional\nfrom datetime import datetime, timedelta\nimport logging\nfrom pathlib import Path\n\n# Import core components\nfrom utils.env_setup import AppConfig\nfrom core.connectors.api_daily_sync import RaceFetcher\nfrom race_prediction.race_predict import RacePredictor\n\n\nclass PredictionOrchestrator:\n    \"\"\"\n    Orchestrates the end-to-end race prediction process:\n    1. Loads races from API or database\n    2. Processes races with feature engineering\n    3. Applies trained models to generate predictions\n    4. Stores prediction results back to the database\n    5. Optionally evaluates predictions against actual results\n    \"\"\"\n\n    # In PredictionOrchestrator.__init__:\n\n    def __init__(self, model_path: str, db_name: str = None, model_type: str = None, verbose: bool = False):\n        \"\"\"\n        Initialize the prediction orchestrator.\n\n        Args:\n            model_path: Path to the model or model name\n            db_name: Database name from config (default: active_db from config)\n            model_type: Type of model ('hybrid_model' or 'incremental_models')\n            verbose: Whether to output verbose logs\n        \"\"\"\n        # Initialize config\n        self.config = AppConfig()\n\n        # Store the verbose flag\n        self.verbose = verbose\n\n        # Set database\n        if db_name is None:\n            self.db_name = self.config._config.base.active_db\n        else:\n            self.db_name = db_name\n\n        # Get database path\n        self.db_path = self.config.get_sqlite_dbpath(self.db_name)\n\n        # Get base model paths with active_db consideration\n        self.model_paths = self.config.get_model_paths(model_name=model_path, model_type=model_type)\n\n        # Determine if model_path is a full path or just a model name\n        model_path_obj = Path(model_path)\n        if model_path_obj.exists() and model_path_obj.is_dir():\n            # This is a full path to a specific model version\n            self.model_path = model_path_obj\n        else:\n            # This is a model type, need to find latest version\n            model_dir = Path(self.model_paths['model_path'])\n            if model_dir.exists():\n                # Get all version folders, sorted newest first\n                versions = sorted([d for d in model_dir.iterdir() if d.is_dir() and d.name.startswith('v')],\n                                  key=lambda x: x.name, reverse=True)\n                if versions:\n                    self.model_path = versions[0]  # Use latest version\n                    if verbose:\n                        print(f\"Using latest model version: {self.model_path.name}\")\n                else:\n                    # No version folders, use model_dir itself\n                    self.model_path = model_dir\n            else:\n                # Model directory doesn't exist, create it\n                os.makedirs(model_dir, exist_ok=True)\n                self.model_path = model_dir\n                # Initialize a basic logger FIRST so it's always available\n        # Set up logging with proper verbose handling\n\n        self._setup_logging()\n\n        # Initialize components\n        self.race_fetcher = RaceFetcher(db_name=self.db_name, verbose=self.verbose)\n        self.race_predictor = RacePredictor(model_path=str(self.model_path), db_name=self.db_name, verbose=self.verbose)\n\n        # Only show initialization message if verbose\n        if self.verbose:\n            self.logger.info(f\"Prediction Orchestrator initialized with model: {self.model_path}\")\n    def log_info(self, message):\n        \"\"\"Simple logging method.\"\"\"\n        if self.verbose:\n            print(message)\n\n    def _setup_logging(self):\n        \"\"\"Set up logging with proper verbose control.\"\"\"\n        # Create logs directory if it doesn't exist\n        log_dir = self.model_path / \"logs\"\n        log_dir.mkdir(parents=True, exist_ok=True)\n\n        # Configure the root logger - important to set up first\n        root_logger = logging.getLogger()\n        root_logger.setLevel(logging.WARNING)  # Default to WARNING level\n\n        # Clear any existing handlers to avoid duplicate messages\n        for handler in list(root_logger.handlers):\n            root_logger.removeHandler(handler)\n\n        # Set up logger for this class\n        self.logger = logging.getLogger(\"PredictionOrchestrator\")\n\n        # Set log level based on verbose flag\n        self.logger.setLevel(logging.INFO if self.verbose else logging.WARNING)\n\n        # Remove any existing handlers\n        for handler in list(self.logger.handlers):\n            self.logger.removeHandler(handler)\n\n        # Add file handler always\n        log_file = log_dir / f\"race_predictor_{datetime.now().strftime('%Y%m%d')}.log\"\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n        self.logger.addHandler(file_handler)\n\n        # Add console handler only if verbose is True\n        if self.verbose:\n            console_handler = logging.StreamHandler()\n            console_handler.setFormatter(logging.Formatter('%(name)s - %(levelname)s - %(message)s'))\n            self.logger.addHandler(console_handler)\n\n    def predict_race(self, comp: str, blend_weight: float = 0.7) -> Dict:\n        \"\"\"\n        Generate predictions for a specific race.\n\n        Args:\n            comp: Race identifier\n            blend_weight: Weight for RF model in blend (0-1)\n\n        Returns:\n            Dictionary with prediction results\n        \"\"\"\n        start_time = datetime.now()\n        if not hasattr(self, 'logger'):\n            self.logger = logging.getLogger(\"PredictionOrchestrator\")\n\n        self.logger.info(f\"Starting prediction for race {comp}\")\n\n\n        try:\n            # Fetch race data from database\n            race_data = self.race_fetcher.get_race_by_comp(comp)\n\n            if race_data is None:\n                return {\n                    'status': 'error',\n                    'error': f'Race {comp} not found in database',\n                    'comp': comp\n                }\n\n            # Check if race already has predictions\n            if race_data.get('prediction_results'):\n                try:\n                    if isinstance(race_data['prediction_results'], str):\n                        pred_results = json.loads(race_data['prediction_results'])\n                    else:\n                        pred_results = race_data['prediction_results']\n\n                    if pred_results:\n                        self.logger.info(f\"Race {comp} already has predictions\")\n                        return {\n                            'status': 'already_predicted',\n                            'message': 'Race already has predictions',\n                            'comp': comp,\n                            'predictions': pred_results\n                        }\n                except:\n                    # If we can't parse the predictions, proceed to generate new ones\n                    pass\n\n            # Get participants data\n            participants = race_data.get('participants')\n            if not participants or participants == '[]':\n                return {\n                    'status': 'error',\n                    'error': 'No valid participant data found',\n                    'comp': comp\n                }\n\n            # Convert to DataFrame if needed\n            if isinstance(participants, str):\n                try:\n                    participants = json.loads(participants)\n                except:\n                    return {\n                        'status': 'error',\n                        'error': 'Could not parse participant data',\n                        'comp': comp\n                    }\n\n            # Process participants into feature DataFrame\n            race_df = pd.DataFrame(participants)\n\n            # Add race information to DataFrame\n            for field in ['typec', 'dist', 'natpis', 'meteo', 'temperature','jour', 'forceVent', 'directionVent', 'corde']:\n                if field in race_data and race_data[field] is not None:\n                    race_df[field] = race_data[field]\n\n            # Add comp to DataFrame\n            race_df['comp'] = comp\n\n            # Generate predictions\n            result_df = self.race_predictor.predict_race(race_df, blend_weight=blend_weight)\n\n            # Select columns for output\n            output_columns = [\n                'numero', 'cheval', 'predicted_position', 'predicted_rank'\n            ]\n\n            # Add optional columns if available\n            for col in ['cotedirect', 'jockey', 'idJockey', 'idche']:\n                if col in result_df.columns:\n                    output_columns.append(col)\n\n            # Create final result DataFrame\n            final_result = result_df[output_columns].copy()\n\n            # Convert to records format\n            prediction_results = final_result.to_dict(orient='records')\n\n            predicted_arriv = result_df['predicted_arriv'].iloc[0] if 'predicted_arriv' in result_df.columns else None\n\n            # Add to metadata\n            metadata = {\n                'race_id': comp,\n                'prediction_time': datetime.now().isoformat(),\n                'model_path': str(self.model_path),  # Convert Path to string here\n                'blend_weight': blend_weight,\n                'hippo': race_data.get('hippo'),\n                'prix': race_data.get('prix'),\n                'jour': race_data.get('jour'),\n                'typec': race_data.get('typec'),\n                'participants_count': len(prediction_results),\n                'predicted_arriv': predicted_arriv\n            }\n            # Store prediction results\n            prediction_data = {\n                'metadata': metadata,\n                'predictions': prediction_results,\n                'predicted_arriv': predicted_arriv  # Add at top level of JSON\n            }\n\n            # Update database\n            self.race_fetcher.update_prediction_results(comp, json.dumps(prediction_data))\n\n            # Calculate elapsed time\n            elapsed_time = (datetime.now() - start_time).total_seconds()\n\n            self.logger.info(f\"Successfully predicted race {comp} in {elapsed_time:.2f} seconds\")\n\n            return {\n                'status': 'success',\n                'comp': comp,\n                'predictions': prediction_results,\n                'metadata': metadata,\n                'elapsed_time': elapsed_time\n            }\n\n        except Exception as e:\n            self.logger.error(f\"Error predicting race {comp}: {str(e)}\")\n            import traceback\n            self.logger.error(traceback.format_exc())\n\n            return {\n                'status': 'error',\n                'error': str(e),\n                'comp': comp\n            }\n\n    def predict_races_by_date(self, date: str = None, blend_weight: float = 0.7) -> Dict:\n        # Make sure logger exists\n        if not hasattr(self, 'logger'):\n            self.logger = logging.getLogger(\"PredictionOrchestrator\")\n\n        # Use today's date if none provided\n        if date is None:\n            date = datetime.now().strftime(\"%Y-%m-%d\")\n\n        self.logger.info(f\"Starting predictions for races on {date}\")\n\n        try:\n            # Get all races for the date\n            races = self.race_fetcher.get_races_by_date(date)\n\n            if not races:\n                self.logger.info(f\"No races found for {date}\")\n                return {\n                    'status': 'no_races',\n                    'message': f'No races found for {date}',\n                    'date': date\n                }\n\n            self.logger.info(f\"Found {len(races)} races for {date}\")\n\n            # Process each race\n            results = []\n            for race in races:\n                comp = race['comp']\n\n                # Skip races that already have predictions\n                if race.get('has_predictions', 0) == 1:\n                    self.logger.info(f\"Race {comp} already has predictions, skipping\")\n                    results.append({\n                        'status': 'already_predicted',\n                        'comp': comp,\n                        'message': 'Race already has predictions'\n                    })\n                    continue\n\n                # Predict the race\n                prediction_result = self.predict_race(comp, blend_weight=blend_weight)\n                results.append(prediction_result)\n\n            # Generate summary\n            success_count = sum(1 for r in results if r['status'] == 'success')\n            error_count = sum(1 for r in results if r['status'] == 'error')\n            skip_count = sum(1 for r in results if r['status'] in ['already_predicted', 'no_data'])\n\n            summary = {\n                'date': date,\n                'total_races': len(races),\n                'predicted': success_count,\n                'errors': error_count,\n                'skipped': skip_count,\n                'results': results\n            }\n\n            self.logger.info(f\"Completed predictions for {date}: \"\n                             f\"{success_count} successful, {error_count} errors, {skip_count} skipped\")\n\n            return summary\n\n        except Exception as e:\n            self.logger.error(f\"Error processing races for {date}: {str(e)}\")\n            import traceback\n            self.logger.error(traceback.format_exc())\n\n            return {\n                'status': 'error',\n                'error': str(e),\n                'date': date\n            }\n\n    def fetch_and_predict_races(self, date: str = None, blend_weight: float = 0.7) -> Dict:\n        \"\"\"\n        Fetch races from API, store them, and generate predictions.\n\n        Args:\n            date: Date string in format YYYY-MM-DD (default: today)\n            blend_weight: Weight for RF model in blend (0-1)\n\n        Returns:\n            Dictionary with fetch and prediction results\n        \"\"\"\n        # Use today's date if none provided\n        if date is None:\n            date = datetime.now().strftime(\"%Y-%m-%d\")\n\n        self.logger.info(f\"Starting fetch and predict for races on {date}\")\n\n        try:\n            # Fetch races from API\n            fetch_results = self.race_fetcher.fetch_and_store_daily_races(date)\n\n            if fetch_results.get('status') == 'error':\n                return {\n                    'status': 'fetch_error',\n                    'error': fetch_results.get('error'),\n                    'date': date\n                }\n\n            # Extract successful races\n            successful_races = [r for r in fetch_results.get('races', [])\n                                if r.get('status') == 'success']\n\n            self.logger.info(f\"Successfully fetched {len(successful_races)} races for {date}\")\n\n            # Predict races\n            prediction_results = self.predict_races_by_date(date, blend_weight=blend_weight)\n\n            # Combine results\n            combined_results = {\n                'date': date,\n                'fetch_results': fetch_results,\n                'prediction_results': prediction_results\n            }\n\n            self.logger.info(f\"Completed fetch and predict for {date}\")\n\n            return combined_results\n\n        except Exception as e:\n            self.logger.error(f\"Error in fetch and predict for {date}: {str(e)}\")\n            import traceback\n            self.logger.error(traceback.format_exc())\n\n            return {\n                'status': 'error',\n                'error': str(e),\n                'date': date\n            }\n\n    def evaluate_predictions(self, comp: str) -> Dict:\n        \"\"\"\n        Evaluate already stored predictions against actual results for a race.\n        Does NOT attempt to make new predictions.\n\n        Args:\n            comp: Race identifier\n\n        Returns:\n            Dictionary with evaluation metrics and bet results\n        \"\"\"\n        self.logger.info(f\"Evaluating stored predictions for race {comp}\")\n\n        try:\n            # Get race data directly from database without using the predictor\n            race_data = self.race_fetcher.get_race_by_comp(comp)\n\n            if race_data is None:\n                return {\n                    'status': 'error',\n                    'error': f'Race {comp} not found in database',\n                    'comp': comp\n                }\n\n            # Check if race has predictions\n            prediction_results = race_data.get('prediction_results')\n            if not prediction_results:\n                return {\n                    'status': 'error',\n                    'error': f'Race {comp} has no predictions',\n                    'comp': comp\n                }\n\n            # Parse prediction results\n            if isinstance(prediction_results, str):\n                try:\n                    prediction_results = json.loads(prediction_results)\n                except:\n                    return {\n                        'status': 'error',\n                        'error': 'Could not parse prediction results',\n                        'comp': comp\n                    }\n\n            # Check if race has actual results\n            actual_results = race_data.get('actual_results')\n            if not actual_results or actual_results == 'pending':\n                return {\n                    'status': 'pending',\n                    'message': f'Race {comp} results are still pending',\n                    'comp': comp\n                }\n\n            # Parse actual results\n            actual_arriv = None\n            if isinstance(actual_results, str):\n                try:\n                    actual_results = json.loads(actual_results)\n                except:\n                    # It might be a direct string representation of results\n                    if '-' in actual_results:\n                        # Looks like a direct arrival string\n                        actual_arriv = actual_results\n                    else:\n                        return {\n                            'status': 'error',\n                            'error': 'Could not parse actual results',\n                            'comp': comp\n                        }\n\n            # Extract predicted_arriv from prediction results\n            predicted_arriv = None\n            metadata = None\n\n            # Check different possible structures of prediction_results\n            if isinstance(prediction_results, dict):\n                if 'metadata' in prediction_results:\n                    metadata = prediction_results['metadata']\n                    predicted_arriv = metadata.get('predicted_arriv')\n                elif 'predicted_arriv' in prediction_results:\n                    predicted_arriv = prediction_results['predicted_arriv']\n\n                # If we have predictions list, extract detailed predictions\n                predictions = prediction_results.get('predictions', [])\n            else:\n                # Assume it's a list of predictions\n                predictions = prediction_results\n                predicted_arriv = None\n\n            # If we don't have predicted_arriv but have predictions, construct it\n            if not predicted_arriv and predictions:\n                # Sort predictions by predicted_rank or predicted_position\n                if isinstance(predictions[0], dict):\n                    if 'predicted_rank' in predictions[0]:\n                        sorted_preds = sorted(predictions, key=lambda x: x['predicted_rank'])\n                    elif 'predicted_position' in predictions[0]:\n                        sorted_preds = sorted(predictions, key=lambda x: x['predicted_position'])\n                    else:\n                        # No ranking information, use as is\n                        sorted_preds = predictions\n\n                    # Construct predicted_arriv\n                    predicted_arriv = '-'.join([str(p['numero']) for p in sorted_preds])\n                    self.logger.info(f\"Constructed predicted_arriv: {predicted_arriv}\")\n\n            # No valid prediction format found\n            if not predicted_arriv:\n                return {\n                    'status': 'error',\n                    'error': 'Could not extract predicted arrival order',\n                    'comp': comp\n                }\n\n            # Extract actual_arriv from actual results\n            actual_arriv = None\n\n            # Handle different formats of actual_results\n            if isinstance(actual_results, list):\n                # Sort by position and construct arrival string\n                try:\n                    sorted_results = sorted(\n                        [r for r in actual_results if 'numero' in r and 'position' in r],\n                        key=lambda x: int(x['position']) if str(x['position']).isdigit() else float('inf')\n                    )\n                    actual_arriv = '-'.join([str(r['numero']) for r in sorted_results])\n                except Exception as e:\n                    self.logger.error(f\"Error sorting actual results: {str(e)}\")\n                    # Try to recover by using unsorted results\n                    actual_arriv = '-'.join([str(r.get('numero', '')) for r in actual_results\n                                             if 'numero' in r])\n            elif isinstance(actual_results, str) and '-' in actual_results:\n                # Direct arrival string\n                actual_arriv = actual_results\n            elif isinstance(actual_results, dict):\n                if 'arrivee' in actual_results:\n                    # Format sometimes returned from API\n                    actual_arriv = actual_results['arrivee']\n                elif 'ordre_arrivee' in actual_results:\n                    # Another possible format\n                    try:\n                        ordre = json.loads(actual_results['ordre_arrivee']) if isinstance(\n                            actual_results['ordre_arrivee'], str) else actual_results['ordre_arrivee']\n                        sorted_results = sorted(ordre, key=lambda x: int(x['narrivee']) if str(\n                            x['narrivee']).isdigit() else float('inf'))\n                        actual_arriv = '-'.join([str(r.get('cheval', r.get('numero', ''))) for r in sorted_results])\n                    except Exception as e:\n                        self.logger.error(f\"Error parsing ordre_arrivee: {str(e)}\")\n\n            if not actual_arriv:\n                return {\n                    'status': 'error',\n                    'error': 'Could not extract actual arrival order',\n                    'comp': comp\n                }\n\n            self.logger.info(f\"Actual arrival: {actual_arriv}\")\n            self.logger.info(f\"Predicted arrival: {predicted_arriv}\")\n\n            # Calculate metrics using arrival strings\n            metrics = self._calculate_arriv_metrics(predicted_arriv, actual_arriv)\n\n            # Add race info\n            metrics['race_info'] = {\n                'comp': comp,\n                'hippo': race_data.get('hippo'),\n                'prix': race_data.get('prix'),\n                'jour': race_data.get('jour'),\n                'typec': race_data.get('typec'),\n                'model': metadata.get('model_path') if metadata else None\n            }\n\n            # Add arrival strings to output\n            metrics['predicted_arriv'] = predicted_arriv\n            metrics['actual_arriv'] = actual_arriv\n\n            # Log winning bets if any\n            winning_bets = metrics.get('winning_bets', [])\n            if winning_bets:\n                self.logger.info(f\"Winning bets for race {comp}: {', '.join(winning_bets)}\")\n            else:\n                self.logger.info(f\"No winning bets for race {comp}\")\n\n            return {\n                'status': 'success',\n                'comp': comp,\n                'metrics': metrics\n            }\n\n        except Exception as e:\n            self.logger.error(f\"Error evaluating predictions for race {comp}: {str(e)}\")\n            import traceback\n            self.logger.error(traceback.format_exc())\n\n            return {\n                'status': 'error',\n                'error': str(e),\n                'comp': comp\n            }\n\n    def _calculate_summary_metrics(self, results: List[Dict]) -> Dict:\n        \"\"\"\n        Calculate summary metrics from a list of evaluation results with enhanced PMU bet reporting.\n\n        Args:\n            results: List of evaluation result dictionaries\n\n        Returns:\n            Dictionary with summary metrics and detailed PMU bet statistics\n        \"\"\"\n        success_count = sum(1 for r in results if r['status'] == 'success')\n\n        if success_count == 0:\n            return {\n                'races_evaluated': 0,\n                'winner_accuracy': 0,\n                'avg_podium_accuracy': 0,\n                'avg_mean_rank_error': float('nan'),\n                'pmu_bets': {\n                    'tierce_exact': 0, 'tierce_exact_rate': 0,\n                    'tierce_desordre': 0, 'tierce_desordre_rate': 0,\n                    'quarte_exact': 0, 'quarte_exact_rate': 0,\n                    'quarte_desordre': 0, 'quarte_desordre_rate': 0,\n                    'quinte_exact': 0, 'quinte_exact_rate': 0,\n                    'quinte_desordre': 0, 'quinte_desordre_rate': 0,\n                    'bonus4': 0, 'bonus4_rate': 0,\n                    'bonus3': 0, 'bonus3_rate': 0,\n                    'deuxsur4': 0, 'deuxsur4_rate': 0,\n                    'multi4': 0, 'multi4_rate': 0\n                },\n                'bet_type_summary': {}\n            }\n\n        # Calculate standard metrics\n        winner_correct = sum(1 for r in results\n                             if r['status'] == 'success' and r['metrics']['winner_correct'])\n\n        podium_accuracy = sum(r['metrics']['podium_accuracy'] for r in results\n                              if r['status'] == 'success') / success_count\n\n        # Handle potential NaN values in mean_rank_error\n        valid_errors = [r['metrics']['mean_rank_error'] for r in results\n                        if r['status'] == 'success' and not pd.isna(r['metrics']['mean_rank_error'])]\n\n        mean_rank_error = sum(valid_errors) / len(valid_errors) if valid_errors else float('nan')\n\n        # PMU bet type success rates and detailed statistics\n        pmu_bet_types = [\n            'tierce_exact', 'tierce_desordre',\n            'quarte_exact', 'quarte_desordre',\n            'quinte_exact', 'quinte_desordre',\n            'bonus4', 'bonus3', 'deuxsur4', 'multi4'\n        ]\n\n        pmu_bet_successes = {}\n        bet_type_summary = {}\n\n        # Track successes for each bet type\n        for bet_type in pmu_bet_types:\n            # Count successful bets\n            successes = sum(1 for r in results\n                            if r['status'] == 'success' and\n                            r['metrics'].get('pmu_bets', {}).get(bet_type, False))\n\n            # Success rate as percentage\n            success_rate = successes / success_count\n\n            # Store counts and rates\n            pmu_bet_successes[bet_type] = successes\n            pmu_bet_successes[f'{bet_type}_rate'] = success_rate\n\n            # Create summary with win/loss count and percentage\n            bet_type_summary[bet_type] = {\n                'wins': successes,\n                'losses': success_count - successes,\n                'success_rate': success_rate * 100,  # as percentage\n                'total_races': success_count\n            }\n\n        # Calculate race-by-race statistics - which races won which bets\n        races_with_wins = sum(1 for r in results\n                              if r['status'] == 'success' and\n                              any(r['metrics'].get('pmu_bets', {}).values()))\n\n        races_with_no_wins = success_count - races_with_wins\n\n        # Calculate races by number of bet types won\n        races_by_bet_count = {}\n        for r in results:\n            if r['status'] == 'success':\n                wins = sum(1 for v in r['metrics'].get('pmu_bets', {}).values() if v)\n                races_by_bet_count[wins] = races_by_bet_count.get(wins, 0) + 1\n\n        bet_statistics = {\n            'races_with_wins': races_with_wins,\n            'races_with_no_wins': races_with_no_wins,\n            'win_rate': races_with_wins / success_count if success_count > 0 else 0,\n            'races_by_bet_count': races_by_bet_count\n        }\n\n        return {\n            'races_evaluated': success_count,\n            'winner_accuracy': winner_correct / success_count,\n            'avg_podium_accuracy': podium_accuracy,\n            'avg_mean_rank_error': mean_rank_error,\n            'pmu_bets': pmu_bet_successes,\n            'bet_type_summary': bet_type_summary,\n            'bet_statistics': bet_statistics\n        }\n\n    def evaluate_predictions_by_date(self, date: str = None) -> Dict:\n        \"\"\"\n        Evaluate stored predictions for all races on a given date.\n\n        Args:\n            date: Date string in format YYYY-MM-DD (default: today)\n\n        Returns:\n            Dictionary with evaluation results\n        \"\"\"\n        # Use today's date if none provided\n        if date is None:\n            date = datetime.now().strftime(\"%Y-%m-%d\")\n\n        self.logger.info(f\"Evaluating predictions for races on {date}\")\n\n        try:\n            # Get all races for the date\n            races = self.race_fetcher.get_races_by_date(date)\n\n            if not races:\n                self.logger.info(f\"No races found for {date}\")\n                return {\n                    'status': 'no_races',\n                    'message': f'No races found for {date}',\n                    'date': date\n                }\n\n            self.logger.info(f\"Found {len(races)} races for {date}\")\n\n            # Process each race\n            results = []\n            for race in races:\n                comp = race['comp']\n\n                # Skip races without predictions or results\n                if race.get('has_predictions', 0) == 0:\n                    self.logger.info(f\"Race {comp} has no predictions, skipping\")\n                    continue\n\n                if race.get('has_results', 0) == 0:\n                    self.logger.info(f\"Race {comp} has no results, skipping\")\n                    continue\n\n                # Evaluate without trying to predict again\n                evaluation_result = self.evaluate_predictions(comp)\n                results.append(evaluation_result)\n\n            # Calculate summary metrics using our helper function\n            summary_metrics = self._calculate_summary_metrics(results)\n\n            summary = {\n                'date': date,\n                'total_races': len(races),\n                'evaluated': summary_metrics['races_evaluated'],\n                'summary_metrics': summary_metrics,\n                'results': results\n            }\n\n            self.logger.info(f\"Completed evaluation for {date}: \"\n                             f\"{summary_metrics['races_evaluated']} races evaluated, \"\n                             f\"Winner accuracy: {summary_metrics['winner_accuracy']:.2f}, \"\n                             f\"Podium accuracy: {summary_metrics['avg_podium_accuracy']:.2f}\")\n\n            return summary\n\n        except Exception as e:\n            self.logger.error(f\"Error evaluating races for {date}: {str(e)}\")\n            import traceback\n            self.logger.error(traceback.format_exc())\n\n            return {\n                'status': 'error',\n                'error': str(e),\n                'date': date\n            }\n\n    def _calculate_arriv_metrics(self, predicted_arriv: str, actual_arriv: str) -> Dict:\n        \"\"\"\n        Calculate evaluation metrics based on arrival strings, including PMU bet types.\n\n        Args:\n            predicted_arriv: Predicted arrival order (e.g., \"1-5-3-2-4\")\n            actual_arriv: Actual arrival order\n\n        Returns:\n            Dictionary with evaluation metrics and PMU bet results\n        \"\"\"\n        # Parse arrival strings to lists\n        pred_order = predicted_arriv.split('-')\n        actual_order = actual_arriv.split('-')\n\n        # Make sure we have valid data\n        if not pred_order or not actual_order:\n            return {\n                'error': 'Invalid arrival strings'\n            }\n\n        # 1. Basic metrics\n        winner_correct = pred_order[0] == actual_order[0]\n\n        # 2. Podium accuracy (top 3)\n        n_podium = min(3, len(pred_order), len(actual_order))\n        pred_podium = set(pred_order[:n_podium])\n        actual_podium = set(actual_order[:n_podium])\n        podium_overlap = len(pred_podium.intersection(actual_podium))\n        podium_accuracy = podium_overlap / n_podium if n_podium > 0 else 0\n\n        # 3. Exacta correct (top 2 in exact order)\n        exacta_correct = False\n        if len(pred_order) >= 2 and len(actual_order) >= 2:\n            exacta_correct = pred_order[:2] == actual_order[:2]\n\n        # 4. Trifecta correct (top 3 in exact order)\n        trifecta_correct = False\n        if len(pred_order) >= 3 and len(actual_order) >= 3:\n            trifecta_correct = pred_order[:3] == actual_order[:3]\n\n        # 5. Mean rank error - requires mapping between predicted and actual positions\n        # Create mapping from horse number to rank\n        pred_ranks = {horse: idx + 1 for idx, horse in enumerate(pred_order)}\n        actual_ranks = {horse: idx + 1 for idx, horse in enumerate(actual_order)}\n\n        # Find common horses\n        common_horses = set(pred_ranks.keys()) & set(actual_ranks.keys())\n\n        if common_horses:\n            rank_errors = [abs(pred_ranks[horse] - actual_ranks[horse]) for horse in common_horses]\n            mean_rank_error = sum(rank_errors) / len(rank_errors)\n        else:\n            mean_rank_error = float('nan')\n\n        # 6. Calculate PMU bet type results\n        pmu_bets = {}\n\n        # 6.1 Tierc (top 3)\n        if len(pred_order) >= 3 and len(actual_order) >= 3:\n            # Tierc exact (1-2-3 in order)\n            pmu_bets['tierce_exact'] = pred_order[:3] == actual_order[:3]\n\n            # Tierc dsordre (1-2-3 in any order)\n            pmu_bets['tierce_desordre'] = set(pred_order[:3]) == set(actual_order[:3])\n        else:\n            pmu_bets['tierce_exact'] = False\n            pmu_bets['tierce_desordre'] = False\n\n        # 6.2 Quart (top 4)\n        if len(pred_order) >= 4 and len(actual_order) >= 4:\n            # Quart exact (1-2-3-4 in order)\n            pmu_bets['quarte_exact'] = pred_order[:4] == actual_order[:4]\n\n            # Quart dsordre (1-2-3-4 in any order)\n            pmu_bets['quarte_desordre'] = set(pred_order[:4]) == set(actual_order[:4])\n\n            # Bonus 4 (first horse correct, other 3 in top 4 in any order)\n            pmu_bets['bonus4'] = (pred_order[0] == actual_order[0] and\n                                  len(set(pred_order[1:4]) & set(actual_order[1:4])) >= 3)\n        else:\n            pmu_bets['quarte_exact'] = False\n            pmu_bets['quarte_desordre'] = False\n            pmu_bets['bonus4'] = False\n\n        # 6.3 Quint+ (top 5)\n        if len(pred_order) >= 5 and len(actual_order) >= 5:\n            # Quint+ exact (1-2-3-4-5 in order)\n            pmu_bets['quinte_exact'] = pred_order[:5] == actual_order[:5]\n\n            # Quint+ dsordre (1-2-3-4-5 in any order)\n            pmu_bets['quinte_desordre'] = set(pred_order[:5]) == set(actual_order[:5])\n\n            # Bonus 3 (first horse correct, other 2 in top 3 in any order)\n            pmu_bets['bonus3'] = (pred_order[0] == actual_order[0] and\n                                  len(set(pred_order[1:3]) & set(actual_order[1:3])) >= 2)\n\n            # 2 sur 4 (at least 2 of the top 4 correct in any order)\n            pmu_bets['deuxsur4'] = len(set(pred_order[:4]) & set(actual_order[:4])) >= 2\n\n            # Multi en 4 (top 4 correct in any order)\n            pmu_bets['multi4'] = set(pred_order[:4]) == set(actual_order[:4])\n        else:\n            pmu_bets['quinte_exact'] = False\n            pmu_bets['quinte_desordre'] = False\n            pmu_bets['bonus3'] = False\n            pmu_bets['deuxsur4'] = False\n            pmu_bets['multi4'] = False\n\n        # 7. Determine which bet types were won (for easier reporting)\n        winning_bets = [bet_type for bet_type, result in pmu_bets.items() if result]\n\n        return {\n            'winner_correct': winner_correct,\n            'podium_accuracy': podium_accuracy,\n            'mean_rank_error': mean_rank_error,\n            'exacta_correct': exacta_correct,\n            'trifecta_correct': trifecta_correct,\n            'pmu_bets': pmu_bets,\n            'winning_bets': winning_bets\n        }
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/core/orchestrators/prediction_orchestrator.py b/core/orchestrators/prediction_orchestrator.py
--- a/core/orchestrators/prediction_orchestrator.py	(revision 0d88e73ffd4a3899fde119bb4ed28c1bf49fab9b)
+++ b/core/orchestrators/prediction_orchestrator.py	(date 1746182204546)
@@ -4,6 +4,7 @@
 import json
 from typing import Dict, List, Union, Any, Optional
 from datetime import datetime, timedelta
+import time
 import logging
 from pathlib import Path
 
@@ -88,6 +89,7 @@
         # Only show initialization message if verbose
         if self.verbose:
             self.logger.info(f"Prediction Orchestrator initialized with model: {self.model_path}")
+        self.prediction_cache = {}
     def log_info(self, message):
         """Simple logging method."""
         if self.verbose:
@@ -129,25 +131,105 @@
             console_handler.setFormatter(logging.Formatter('%(name)s - %(levelname)s - %(message)s'))
             self.logger.addHandler(console_handler)
 
-    def predict_race(self, comp: str, blend_weight: float = 0.7) -> Dict:
+    def predict_race(self, comp: str, blend_weight: float = 0.7, force_recalculate: bool = False) -> Dict:
         """
-        Generate predictions for a specific race.
+        Generate predictions for a specific race with caching support.
 
         Args:
             comp: Race identifier
             blend_weight: Weight for RF model in blend (0-1)
+            force_recalculate: Whether to skip cache and force recalculation
 
         Returns:
             Dictionary with prediction results
         """
         start_time = datetime.now()
-        if not hasattr(self, 'logger'):
-            self.logger = logging.getLogger("PredictionOrchestrator")
-
-        self.logger.info(f"Starting prediction for race {comp}")
-
+        self.logger.info(f"Starting prediction for race {comp} with blend_weight={blend_weight}")
 
         try:
+            # Check cache first unless forced to recalculate
+            if not force_recalculate and comp in self.prediction_cache:
+                cache_entry = self.prediction_cache[comp]
+
+                # Check if we need to reblend or can use existing predictions
+                if cache_entry.get('blend_weight') == blend_weight:
+                    self.logger.info(f"Using cached prediction for race {comp}")
+                    return cache_entry.get('result')
+
+                # If we get here, we have cached data but need to reblend
+                self.logger.info(f"Reblending predictions for race {comp} using cached data")
+
+                # Get the race data from the cache (avoid database lookup)
+                race_df = cache_entry.get('race_df')
+
+                if race_df is not None:
+                    # Repredict with new blend weight
+                    reblend_start = time.time()
+                    result_df = self.race_predictor.predict_race(race_df, blend_weight=blend_weight)
+                    reblend_time = time.time() - reblend_start
+
+                    # Select columns for output
+                    output_columns = [
+                        'numero', 'cheval', 'predicted_position', 'predicted_rank'
+                    ]
+
+                    # Add optional columns if available
+                    for col in ['cotedirect', 'jockey', 'idJockey', 'idche']:
+                        if col in result_df.columns:
+                            output_columns.append(col)
+
+                    # Create final result DataFrame
+                    final_result = result_df[output_columns].copy()
+
+                    # Convert to records format
+                    prediction_results = final_result.to_dict(orient='records')
+
+                    predicted_arriv = result_df['predicted_arriv'].iloc[
+                        0] if 'predicted_arriv' in result_df.columns else None
+
+                    # Create metadata
+                    metadata = cache_entry.get('metadata', {}).copy()
+                    metadata.update({
+                        'prediction_time': datetime.now().isoformat(),
+                        'blend_weight': blend_weight,
+                        'participants_count': len(prediction_results),
+                        'predicted_arriv': predicted_arriv,
+                        'reblend_time': reblend_time
+                    })
+
+                    # Store prediction results
+                    prediction_data = {
+                        'metadata': metadata,
+                        'predictions': prediction_results,
+                        'predicted_arriv': predicted_arriv  # Add at top level of JSON
+                    }
+
+                    # Update database
+                    self.race_fetcher.update_prediction_results(comp, json.dumps(prediction_data))
+
+                    # Calculate elapsed time
+                    elapsed_time = (datetime.now() - start_time).total_seconds()
+
+                    # Create result
+                    result = {
+                        'status': 'success',
+                        'comp': comp,
+                        'predictions': prediction_results,
+                        'metadata': metadata,
+                        'elapsed_time': elapsed_time,
+                        'reblend_time': reblend_time
+                    }
+
+                    # Update cache with new result
+                    cache_entry['result'] = result
+                    cache_entry['blend_weight'] = blend_weight
+
+                    self.logger.info(
+                        f"Successfully reblended race {comp} in {elapsed_time:.2f} seconds (reblend: {reblend_time:.2f}s)")
+                    return result
+
+            # If we're here, either force_recalculate was True, or we don't have cache data
+            # Proceed with normal prediction flow
             # Fetch race data from database
             race_data = self.race_fetcher.get_race_by_comp(comp)
 
@@ -158,26 +240,6 @@
                     'comp': comp
                 }
 
-            # Check if race already has predictions
-            if race_data.get('prediction_results'):
-                try:
-                    if isinstance(race_data['prediction_results'], str):
-                        pred_results = json.loads(race_data['prediction_results'])
-                    else:
-                        pred_results = race_data['prediction_results']
-
-                    if pred_results:
-                        self.logger.info(f"Race {comp} already has predictions")
-                        return {
-                            'status': 'already_predicted',
-                            'message': 'Race already has predictions',
-                            'comp': comp,
-                            'predictions': pred_results
-                        }
-                except:
-                    # If we can't parse the predictions, proceed to generate new ones
-                    pass
-
             # Get participants data
             participants = race_data.get('participants')
             if not participants or participants == '[]':
@@ -201,15 +263,22 @@
             # Process participants into feature DataFrame
             race_df = pd.DataFrame(participants)
 
-            # Add race information to DataFrame
-            for field in ['typec', 'dist', 'natpis', 'meteo', 'temperature','jour', 'forceVent', 'directionVent', 'corde']:
+            # Add race information to each participant row
+            race_attributes = [
+                'typec', 'dist', 'natpis', 'meteo', 'temperature',
+                'forceVent', 'directionVent', 'corde', 'jour',
+                'hippo', 'quinte', 'pistegp'
+            ]
+
+            for field in race_attributes:
                 if field in race_data and race_data[field] is not None:
                     race_df[field] = race_data[field]
+                    self.logger.info(f"Added race attribute {field}={race_data[field]} to all participants")
 
             # Add comp to DataFrame
             race_df['comp'] = comp
 
-            # Generate predictions
+            # Generate predictions using RacePredictor
             result_df = self.race_predictor.predict_race(race_df, blend_weight=blend_weight)
 
             # Select columns for output
@@ -234,7 +303,7 @@
             metadata = {
                 'race_id': comp,
                 'prediction_time': datetime.now().isoformat(),
-                'model_path': str(self.model_path),  # Convert Path to string here
+                'model_path': str(self.model_path),
                 'blend_weight': blend_weight,
                 'hippo': race_data.get('hippo'),
                 'prix': race_data.get('prix'),
@@ -243,6 +312,7 @@
                 'participants_count': len(prediction_results),
                 'predicted_arriv': predicted_arriv
             }
+
             # Store prediction results
             prediction_data = {
                 'metadata': metadata,
@@ -256,9 +326,7 @@
             # Calculate elapsed time
             elapsed_time = (datetime.now() - start_time).total_seconds()
 
-            self.logger.info(f"Successfully predicted race {comp} in {elapsed_time:.2f} seconds")
-
-            return {
+            result = {
                 'status': 'success',
                 'comp': comp,
                 'predictions': prediction_results,
@@ -266,6 +334,19 @@
                 'elapsed_time': elapsed_time
             }
 
+            # Store in cache
+            self.prediction_cache[comp] = {
+                'race_df': race_df,  # Store processed DataFrame for reblending
+                'result': result,
+                'blend_weight': blend_weight,
+                'metadata': metadata,
+                'timestamp': datetime.now()
+            }
+
+            self.logger.info(f"Successfully predicted race {comp} in {elapsed_time:.2f} seconds")
+
+            return result
+
         except Exception as e:
             self.logger.error(f"Error predicting race {comp}: {str(e)}")
             import traceback
Index: race_prediction/predict_orchestrator_cli.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/usr/bin/env python\n# race_prediction/predict_orchestrator_cli.py\n\nimport argparse\nimport json\nimport sys\nimport os\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nimport logging\n\n# Add parent directory to path for imports\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n# Import the orchestrator\nfrom core.orchestrators.prediction_orchestrator import PredictionOrchestrator\n\n\ndef report_evaluation_results(results):\n    \"\"\"\n    Generate a formatted report for race evaluation results.\n\n    Args:\n        results: Evaluation results dictionary\n\n    Returns:\n        Formatted string with evaluation report\n    \"\"\"\n    if not results or 'status' not in results:\n        return \"Invalid results format\"\n\n    if results['status'] != 'success':\n        return f\"Error: {results.get('error', 'Unknown error')}\"\n\n    # Extract metrics\n    metrics = results.get('metrics', {})\n    race_info = metrics.get('race_info', {})\n\n    # Format race information\n    race_header = (\n        f\"\\nEvaluation for race {race_info.get('comp')}: \"\n        f\"{race_info.get('hippo')} - {race_info.get('prix')} \"\n        f\"({race_info.get('jour')})\"\n    )\n\n    # Format basic metrics\n    basic_metrics = (\n        f\"Basic metrics:\\n\"\n        f\"  Winner correctly predicted: {'' if metrics.get('winner_correct') else ''}\\n\"\n        f\"  Podium accuracy: {metrics.get('podium_accuracy', 0):.2f}\\n\"\n        f\"  Mean rank error: {metrics.get('mean_rank_error', 'N/A'):.2f}\"\n    )\n\n    # Format PMU bet results\n    pmu_bets = metrics.get('pmu_bets', {})\n    winning_bets = metrics.get('winning_bets', [])\n\n    if not winning_bets:\n        bet_results = \"PMU bet results: No winning bets\"\n    else:\n        bet_results = \"PMU bet results:  \" + \", \".join([\n            format_bet_name(bet_type) for bet_type in winning_bets\n        ])\n\n    # Format arrival orders\n    arrival_info = (\n        f\"Arrival orders:\\n\"\n        f\"  Predicted: {metrics.get('predicted_arriv', 'N/A')}\\n\"\n        f\"  Actual: {metrics.get('actual_arriv', 'N/A')}\"\n    )\n\n    # Combine all sections\n    report = f\"{race_header}\\n\\n{basic_metrics}\\n\\n{bet_results}\\n\\n{arrival_info}\"\n    return report\n\n\ndef format_bet_name(bet_type):\n    \"\"\"Format bet type names for display\"\"\"\n    name_mapping = {\n        'tierce_exact': 'Tierc Exact',\n        'tierce_desordre': 'Tierc Dsordre',\n        'quarte_exact': 'Quart Exact',\n        'quarte_desordre': 'Quart Dsordre',\n        'quinte_exact': 'Quint+ Exact',\n        'quinte_desordre': 'Quint+ Dsordre',\n        'bonus4': 'Bonus 4',\n        'bonus3': 'Bonus 3',\n        'deuxsur4': '2 sur 4',\n        'multi4': 'Multi en 4'\n    }\n    return name_mapping.get(bet_type, bet_type)\n\n\ndef report_summary_evaluation(summary):\n    \"\"\"\n    Generate a formatted report for evaluation summary.\n\n    Args:\n        summary: Evaluation summary dictionary\n\n    Returns:\n        Formatted string with summary report\n    \"\"\"\n    if not summary or 'summary_metrics' not in summary:\n        return \"No summary metrics available\"\n\n    metrics = summary['summary_metrics']\n    races_evaluated = metrics.get('races_evaluated', 0)\n\n    if races_evaluated == 0:\n        return \"No races evaluated\"\n\n    # Basic statistics\n    basic_stats = (\n        f\"\\nEvaluation Summary ({races_evaluated} races):\\n\"\n        f\"  Winner accuracy: {metrics.get('winner_accuracy', 0):.2f} \"\n        f\"({int(metrics.get('winner_accuracy', 0) * races_evaluated)}/{races_evaluated})\\n\"\n        f\"  Average podium accuracy: {metrics.get('avg_podium_accuracy', 0):.2f}\\n\"\n        f\"  Average mean rank error: {metrics.get('avg_mean_rank_error', 'N/A'):.2f}\"\n    )\n\n    # PMU bet statistics\n    bet_stats = metrics.get('bet_statistics', {})\n    pmu_summary = (\n        f\"\\nOverall PMU Bet Performance:\\n\"\n        f\"  Races with at least one winning bet: {bet_stats.get('races_with_wins', 0)} \"\n        f\"({bet_stats.get('win_rate', 0):.2f})\\n\"\n        f\"  Races with no winning bets: {bet_stats.get('races_with_no_wins', 0)}\"\n    )\n\n    # Distribution of races by bet count\n    bet_counts = bet_stats.get('races_by_bet_count', {})\n    if bet_counts:\n        bet_count_lines = [\n            f\"  Races winning {count} bet types: {bet_counts.get(count, 0)}\"\n            for count in sorted(bet_counts.keys())\n        ]\n        bet_count_summary = \"\\n\" + \"\\n\".join(bet_count_lines)\n    else:\n        bet_count_summary = \"\"\n\n    # Detailed bet type performance\n    bet_type_summary = metrics.get('bet_type_summary', {})\n    if bet_type_summary:\n        bet_type_lines = []\n        for bet_type, stats in bet_type_summary.items():\n            bet_type_lines.append(\n                f\"  {format_bet_name(bet_type)}: \"\n                f\"{stats.get('wins', 0)}/{stats.get('total_races', 0)} \"\n                f\"({stats.get('success_rate', 0):.1f}%)\"\n            )\n        bet_type_report = \"\\n\\nPMU Bet Type Success Rates:\\n\" + \"\\n\".join(bet_type_lines)\n    else:\n        bet_type_report = \"\"\n\n    # Complete report\n    report = f\"{basic_stats}\\n{pmu_summary}{bet_count_summary}{bet_type_report}\"\n    return report\n\n\ndef main():\n    \"\"\"Command-line interface for race prediction orchestration.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Orchestrate race predictions workflow\")\n    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to the model directory\")\n    parser.add_argument(\"--db\", type=str, help=\"Database name from config (defaults to active_db)\")\n    parser.add_argument(\"--date\", type=str, help=\"Date to process (YYYY-MM-DD format, default: today)\")\n    parser.add_argument(\"--race\", type=str, help=\"Process a specific race by ID (comp)\")\n    parser.add_argument(\"--action\", type=str, choices=['predict', 'evaluate', 'fetch', 'fetchpredict'],\n                        default='predict', help=\"Action to perform\")\n    parser.add_argument(\"--blend\", type=float, default=0.7, help=\"Blend weight for RF model (0-1)\")\n    parser.add_argument(\"--output\", type=str, help=\"Output file for results\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Enable verbose output\")\n    parser.add_argument(\"--summary\", action=\"store_true\", help=\"Show only summary info, not detailed results\")\n\n    args = parser.parse_args()\n\n    # Create the orchestrator\n    orchestrator = PredictionOrchestrator(\n        model_path=args.model,\n        db_name=args.db,\n        verbose=args.verbose\n    )\n\n    results = None\n\n    # Process specific race if provided\n    if args.race:\n        comp = args.race\n\n        if args.action == 'predict':\n            if not args.verbose:\n                print(f\"Generating predictions for race {comp}...\")\n            results = orchestrator.predict_race(comp, blend_weight=args.blend)\n\n            if results['status'] == 'success':\n                print(f\"\\nPredictions for race {comp}:\")\n                print(f\"  Race: {results['metadata']['hippo']} - {results['metadata']['prix']}\")\n                print(f\"\\nPredicted order of finish:\")\n\n                for i, horse in enumerate(results['predictions'][:5]):\n                    print(f\"  {i + 1}. {horse['numero']} - {horse['cheval']} \"\n                          f\"(predicted: {horse['predicted_position']:.2f})\")\n\n                if len(results['predictions']) > 5:\n                    print(f\"  ... and {len(results['predictions']) - 5} more horses\")\n            else:\n                print(f\"Error: {results.get('error', 'Unknown error')}\")\n\n        elif args.action == 'evaluate':\n            results = orchestrator.evaluate_predictions(comp)\n\n            if results['status'] == 'success':\n                metrics = results['metrics']\n                bet_results = metrics.get('winning_bets', [])\n\n                print(f\"####Evaluation for race {comp}:#######\")\n                print(f\"  Race: {metrics['race_info'].get('hippo')} - {metrics['race_info'].get('prix')}\")\n                print(f\"Arrivals:\")\n                print(f\"  Predicted: {metrics['predicted_arriv']}\")\n                print(f\"  Actual:    {metrics['actual_arriv']}\")\n                print(f\"  Winner prediction: {'' if metrics['winner_correct'] else ''}\")\n                print(f\"  Podium accuracy: {metrics['podium_accuracy']:.2f}\")\n                print(f\"Bets:\")\n                if bet_results:\n                    print(\"  Winning bets:\")\n                    for bet in bet_results:\n                        print(f\"   {format_bet_name(bet)}\")\n                else:\n                    print(\"  No winning bets\")\n            else:\n                print(f\"Error: {results.get('error', 'Unknown error')}\")\n\n    # Process all races for a date\n    else:\n        # Use today's date if none provided\n        date = args.date or datetime.now().strftime(\"%Y-%m-%d\")\n\n        if args.action == 'predict':\n            if not args.verbose:\n                print(f\"Generating predictions for races on {date}...\")\n            results = orchestrator.predict_races_by_date(date, blend_weight=args.blend)\n\n            print(f\"\\nPrediction summary for {date}:\")\n            print(f\"  Total races: {results['total_races']}\")\n            print(f\"  Successfully predicted: {results['predicted']}\")\n            print(f\"  Errors: {results['errors']}\")\n            print(f\"  Skipped: {results['skipped']}\")\n\n            if not args.summary and results.get('results'):\n                print(\"\\nPredicted races:\")\n                for race in results['results']:\n                    if race['status'] == 'success':\n                        print(f\"   {race['comp']}: {race.get('metadata', {}).get('hippo', 'Unknown')}\")\n                    else:\n                        print(f\"   {race['comp']}: {race.get('error', 'Failed')}\")\n\n        elif args.action == 'evaluate':\n            if not args.verbose:\n                print(f\"Evaluating predictions for races on {date}...\")\n            results = orchestrator.evaluate_predictions_by_date(date)\n\n            if results.get('summary_metrics'):\n                metrics = results['summary_metrics']\n                races_evaluated = metrics.get('races_evaluated', 0)\n                bet_stats = metrics.get('bet_statistics', {})\n\n                print(f\"\\nEvaluation Summary ({races_evaluated} races):\")\n                print(\n                    f\"  Winner accuracy: {metrics.get('winner_accuracy', 0):.2f} ({int(metrics.get('winner_accuracy', 0) * races_evaluated)}/{races_evaluated})\")\n                print(\n                    f\"  Races with winning bets: {bet_stats.get('races_with_wins', 0)}/{races_evaluated} ({bet_stats.get('win_rate', 0) * 100:.1f}%)\")\n\n                # Show top 3 most successful bet types\n                bet_summary = metrics.get('bet_type_summary', {})\n                if bet_summary:\n                    sorted_bets = sorted(bet_summary.items(), key=lambda x: x[1]['success_rate'], reverse=True)[:3]\n                    print(\"\\nTop performing bet types:\")\n                    for bet_type, stats in sorted_bets:\n                        print(\n                            f\"  {format_bet_name(bet_type)}: {stats['wins']}/{stats['total_races']} ({stats['success_rate']:.1f}%)\")\n\n                if not args.summary and results.get('results'):\n                    print(\"\\nResults by race:\")\n                    for race in [r for r in results['results'] if r['status'] == 'success']:\n                        metrics = race['metrics']\n                        winning_bets = metrics.get('winning_bets', [])\n                        bet_count = len(winning_bets)\n\n                        print(f\"  {race['comp']}: Winning Bets {'' if bet_count> 0 else '0'} \")\n            else:\n                print(\"No metrics available. Races may not have been evaluated yet.\")\n\n        elif args.action == 'fetch':\n            if not args.verbose:\n                print(f\"Fetching races for {date}...\")\n            results = orchestrator.race_fetcher.fetch_and_store_daily_races(date)\n\n            print(f\"\\nFetch summary for {date}:\")\n            print(f\"  Total races: {results.get('total_races', 0)}\")\n            print(f\"  Successfully processed: {results.get('successful', 0)}\")\n            print(f\"  Failed: {results.get('failed', 0)}\")\n\n        elif args.action == 'fetchpredict':\n            if not args.verbose:\n                print(f\"Fetching and predicting races for {date}...\")\n            results = orchestrator.fetch_and_predict_races(date, blend_weight=args.blend)\n\n            fetch_results = results.get('fetch_results', {})\n            prediction_results = results.get('prediction_results', {})\n\n            print(f\"\\nFetch and predict for {date}:\")\n            print(f\"  Fetched: {fetch_results.get('total_races', 0)} races\")\n            print(\n                f\"  Predicted: {prediction_results.get('predicted', 0)}/{prediction_results.get('total_races', 0)} races\")\n\n    # Save results to file if requested\n    if args.output and results:\n        output_path = Path(args.output)\n        output_path.parent.mkdir(exist_ok=True, parents=True)\n\n        with open(output_path, 'w') as f:\n            json.dump(results, f, indent=2)\n\n        print(f\"\\nResults saved to {output_path}\")\n\n    return 0\n\n\n# Helper function for bet type formatting\ndef format_bet_name(bet_type):\n    \"\"\"Format bet type names for display\"\"\"\n    name_mapping = {\n        'tierce_exact': 'Tierc Exact',\n        'tierce_desordre': 'Tierc Dsordre',\n        'quarte_exact': 'Quart Exact',\n        'quarte_desordre': 'Quart Dsordre',\n        'quinte_exact': 'Quint+ Exact',\n        'quinte_desordre': 'Quint+ Dsordre',\n        'bonus4': 'Bonus 4',\n        'bonus3': 'Bonus 3',\n        'deuxsur4': '2 sur 4',\n        'multi4': 'Multi en 4'\n    }\n    return name_mapping.get(bet_type, bet_type)\n\n\n#if __name__ == \"__main__\":\n#    sys.exit(main())\n\n\ndef debug_fetchpredict(race, model_path, db_name=None, blend_weight=0.7, verbose=False):\n    \"\"\"\n    Debug function to execute fetchpredict for a specific date and model.\n    This can be called directly from the IDE for setting breakpoints.\n\n    Args:\n        date: Date string in format YYYY-MM-DD\n        model_path: Path to the model directory\n        db_name: Database name from config (defaults to active_db)\n        blend_weight: Weight for RF model in blend (0-1)\n        verbose: Whether to enable verbose output\n\n    Returns:\n        Results dictionary from fetch_and_predict_races\n    \"\"\"\n    from core.orchestrators.prediction_orchestrator import PredictionOrchestrator\n\n    print(f\"Debug: Starting fetchpredict for date {race} with model {model_path}\")\n\n    # Create orchestrator instance\n    orchestrator = PredictionOrchestrator(\n        model_path=model_path,\n        db_name=db_name,\n        verbose=verbose\n    )\n\n    # Execute fetch and predict\n    results = orchestrator.predict_race(\"1585081\",\"0.7\")\n\n    # Print summary results\n    fetch_results = results.get('fetch_results', {})\n    prediction_results = results.get('prediction_results', {})\n\n    print(f\"\\nFetch summary for {race}:\")\n    print(f\"  Total races: {fetch_results.get('total_races', 0)}\")\n    print(f\"  Successfully processed: {fetch_results.get('successful', 0)}\")\n    print(f\"  Failed: {fetch_results.get('failed', 0)}\")\n\n    print(f\"\\nPrediction summary for {race}:\")\n    print(f\"  Total races: {prediction_results.get('total_races', 0)}\")\n    print(f\"  Successfully predicted: {prediction_results.get('predicted', 0)}\")\n    print(f\"  Errors: {prediction_results.get('errors', 0)}\")\n    print(f\"  Skipped: {prediction_results.get('skipped', 0)}\")\n\n    return results\n\n\n# This can be used at the end of the file for direct execution in IDE\nif __name__ == \"__main__\":\n    # For debug via IDE - uncomment the line below to use\n     debug_fetchpredict( \"1585081\", \"models/2years/hybrid/2years_full_v20250409\", verbose=True)\n\n    # For normal CLI execution\n    #sys.exit(main())
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/race_prediction/predict_orchestrator_cli.py b/race_prediction/predict_orchestrator_cli.py
--- a/race_prediction/predict_orchestrator_cli.py	(revision 0d88e73ffd4a3899fde119bb4ed28c1bf49fab9b)
+++ b/race_prediction/predict_orchestrator_cli.py	(date 1746187345436)
@@ -398,7 +398,7 @@
 # This can be used at the end of the file for direct execution in IDE
 if __name__ == "__main__":
     # For debug via IDE - uncomment the line below to use
-     debug_fetchpredict( "1585081", "models/2years/hybrid/2years_full_v20250409", verbose=True)
+     #debug_fetchpredict( "1585081", "models/2years/hybrid/2years_full_v20250409", verbose=True)
 
     # For normal CLI execution
-    #sys.exit(main())
\ No newline at end of file
+    sys.exit(main())
\ No newline at end of file
Index: test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># test.py\nimport os\nimport argparse\nfrom utils.env_setup import AppConfig\nfrom utils.cache_manager import CacheManager\nfrom core.orchestrators.embedding_feature import FeatureEmbeddingOrchestrator\n\n\ndef test_cache_manager():\n    \"\"\"Test the CacheManager functionality.\"\"\"\n    from utils.cache_manager import CacheManager\n    import pandas as pd\n\n    # Create a test DataFrame\n    test_df = pd.DataFrame({\n        'id': [1, 2, 3],\n        'value': ['a', 'b', 'c']\n    })\n\n    # Initialize cache manager\n    cache_manager = CacheManager()\n\n    # Test cache type\n    cache_type = \"test_cache\"\n\n    # Clear any existing cache\n    cache_manager.clear_cache(cache_type)\n\n    # Test saving\n    print(f\"Saving DataFrame to cache type '{cache_type}'...\")\n    saved_df = cache_manager.save_dataframe(test_df, cache_type)\n    print(f\"Saved DataFrame\")\n\n    # Test loading\n    print(f\"Loading DataFrame from cache type '{cache_type}'...\")\n    loaded_df = cache_manager.load_dataframe(cache_type)\n\n    if loaded_df is not None:\n        print(\"Successfully loaded cached data:\")\n        print(loaded_df)\n        return True\n    else:\n        print(\"Failed to load cached data\")\n        return False\n\n\ndef test_data_loading(orchestrator, args):\n    \"\"\"Test basic data loading functionality.\"\"\"\n    print(\"\\n=== Testing data loading ===\")\n    # Checking for correct method name for loading data\n    if hasattr(orchestrator, 'load_historical_races'):\n        df = orchestrator.load_historical_races(limit=args.limit, race_filter=args.race_type)\n    else:\n        print(\"Error: 'load_historical_races' method not found\")\n        return None\n\n    print(f\"Successfully loaded {len(df)} participant records from {df['comp'].nunique()} races\")\n    print(f\"Sample columns: {list(df.columns[:5])}\")\n    return df\n\n\ndef test_feature_preparation(orchestrator, df, args):\n    \"\"\"Test feature preparation and embedding.\"\"\"\n    print(\"\\n=== Testing feature preparation ===\")\n    try:\n        # Check for correct preprocessing method\n        if hasattr(orchestrator, 'prepare_features'):\n            print(\"Using prepare_features method...\")\n            processed_df = orchestrator.prepare_features(df, use_cache=not args.no_cache)\n        elif hasattr(orchestrator, 'preprocess_data'):\n            print(\"Using preprocess_data method...\")\n            processed_df = orchestrator.preprocess_data(df, use_cache=not args.no_cache)\n        else:\n            print(\"Error: No preprocessing method found\")\n            return None\n\n        print(f\"Successfully preprocessed data: {processed_df.shape}\")\n\n        # Apply embeddings\n        if hasattr(orchestrator, 'apply_embeddings'):\n            embedded_df = orchestrator.apply_embeddings(processed_df, use_cache=not args.no_cache)\n            print(f\"Successfully applied embeddings: {embedded_df.shape}\")\n\n            # Check for embedding columns\n            embedding_cols = [col for col in embedded_df.columns if '_emb_' in col]\n            print(f\"Found {len(embedding_cols)} embedding columns\")\n\n            for prefix in ['horse', 'jockey', 'couple', 'course']:\n                cols = [col for col in embedding_cols if col.startswith(f\"{prefix}_emb_\")]\n                if cols:\n                    print(f\"  - {prefix.capitalize()} embeddings: {len(cols)}\")\n\n            return embedded_df\n        else:\n            print(\"No 'apply_embeddings' method found, using processed data\")\n            return processed_df  # Return processed data if embedding not available\n\n    except Exception as e:\n        print(f\"Error preparing features: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\n\ndef test_feature_store(orchestrator, embedded_df, args):\n    \"\"\"Test feature store saving and loading.\"\"\"\n    print(\"\\n=== Testing feature store functionality ===\")\n    try:\n        # Prepare training dataset\n        print(\"Preparing training dataset...\")\n        if hasattr(orchestrator, 'prepare_training_dataset'):\n            X, y = orchestrator.prepare_training_dataset(embedded_df)\n        else:\n            print(\"Error: No 'prepare_training_dataset' method found\")\n            return False\n\n        print(f\"Successfully prepared dataset with {X.shape[1]} features and {len(y)} samples\")\n\n        # Split dataset\n        print(\"Splitting dataset...\")\n        if hasattr(orchestrator, 'split_dataset'):\n            X_train, X_val, X_test, y_train, y_val, y_test = orchestrator.split_dataset(X, y)\n        else:\n            # Use scikit-learn directly if the method doesn't exist\n            print(\"No 'split_dataset' method found, using scikit-learn directly\")\n            from sklearn.model_selection import train_test_split\n\n            # First split: train+val vs test (0.2)\n            X_trainval, X_test, y_trainval, y_test = train_test_split(\n                X, y, test_size=0.2, random_state=42\n            )\n\n            # Second split: train vs val (0.125 of original = 0.1/0.8)\n            X_train, X_val, y_train, y_val = train_test_split(\n                X_trainval, y_trainval, test_size=0.125, random_state=42\n            )\n\n        print(f\"Successfully split dataset:\")\n        print(f\"  - Training: {X_train.shape[0]} samples\")\n        print(f\"  - Validation: {X_val.shape[0]} samples\")\n        print(f\"  - Testing: {X_test.shape[0]} samples\")\n\n        # Save feature store\n        print(\"\\n=== Testing feature store saving ===\")\n        if hasattr(orchestrator, 'save_feature_store'):\n            feature_store_path = orchestrator.save_feature_store(\n                X_train, X_val, X_test, y_train, y_val, y_test,\n                prefix=f\"{args.db}_test_\"\n            )\n        else:\n            print(\"Error: No 'save_feature_store' method found\")\n            return False\n\n        print(f\"Successfully saved feature store to {feature_store_path}\")\n\n        # Load feature store\n        print(\"\\n=== Testing feature store loading ===\")\n        if hasattr(orchestrator, 'load_feature_store'):\n            loaded_data = orchestrator.load_feature_store(feature_store_path=feature_store_path)\n        else:\n            print(\"Error: No 'load_feature_store' method found\")\n            return False\n\n        if len(loaded_data) >= 6:  # Ensure we got at least 6 elements: X_train...y_test\n            loaded_X_train, loaded_X_val, loaded_X_test, loaded_y_train, loaded_y_val, loaded_y_test = loaded_data[:6]\n            metadata = loaded_data[6] if len(loaded_data) > 6 else {}\n\n            print(f\"Successfully loaded feature store from {feature_store_path}\")\n            print(f\"Loaded data shapes match original: \"\n                  f\"{loaded_X_train.shape == X_train.shape}, \"\n                  f\"{loaded_y_train.shape == y_train.shape}\")\n\n            # Print some metadata\n            if metadata:\n                print(\"\\nFeature store metadata:\")\n                if 'created_at' in metadata:\n                    print(f\"  - Created: {metadata['created_at']}\")\n                if 'dataset_info' in metadata:\n                    info = metadata['dataset_info']\n                    print(f\"  - Training samples: {info.get('train_samples')}\")\n                    print(f\"  - Feature count: {info.get('feature_count')}\")\n\n                    # Count embedding features\n                    if 'embedding_features' in info:\n                        embedding_types = {\n                            'horse': [col for col in info['embedding_features'] if col.startswith('horse_emb_')],\n                            'jockey': [col for col in info['embedding_features'] if col.startswith('jockey_emb_')],\n                            'couple': [col for col in info['embedding_features'] if col.startswith('couple_emb_')],\n                            'course': [col for col in info['embedding_features'] if col.startswith('course_emb_')]\n                        }\n\n                        print(f\"  - Embedding features:\")\n                        for entity, cols in embedding_types.items():\n                            if cols:\n                                print(f\"    - {entity.capitalize()}: {len(cols)} dimensions\")\n\n            return True\n        else:\n            print(f\"Error: Unexpected format returned from load_feature_store\")\n            return False\n    except Exception as e:\n        print(f\"Error in feature store testing: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\n\ndef test_lstm_data_preparation(orchestrator, embedded_df, args):\n    \"\"\"Test LSTM sequence data preparation.\"\"\"\n    print(\"\\n=== Testing LSTM data preparation ===\")\n    try:\n        # Prepare sequence data\n        sequence_length = 3  # Use a small value for testing\n        print(f\"Preparing sequence data with length={sequence_length}...\")\n\n        if not hasattr(orchestrator, 'prepare_sequence_data'):\n            print(\"Error: No 'prepare_sequence_data' method found\")\n            return False\n\n        try:\n            X_sequences, X_static, y = orchestrator.prepare_sequence_data(\n                embedded_df,\n                sequence_length=sequence_length,\n                step_size=1\n            )\n\n            print(f\"Successfully prepared sequence data:\")\n            print(f\"  - Sequences shape: {X_sequences.shape}\")\n            print(f\"  - Static features shape: {X_static.shape}\")\n            print(f\"  - Targets shape: {y.shape}\")\n\n            # Split the data (simple random split for testing)\n            from sklearn.model_selection import train_test_split\n\n            print(\"Splitting sequence data...\")\n            # Split data\n            X_seq_train, X_seq_test, X_static_train, X_static_test, y_train, y_test = train_test_split(\n                X_sequences, X_static, y, test_size=0.2, random_state=42\n            )\n\n            # Further split training data into train and validation\n            X_seq_train, X_seq_val, X_static_train, X_static_val, y_train, y_val = train_test_split(\n                X_seq_train, X_static_train, y_train, test_size=0.2, random_state=42\n            )\n\n            print(f\"Split sequence data:\")\n            print(f\"  - Train: {X_seq_train.shape[0]} sequences\")\n            print(f\"  - Validation: {X_seq_val.shape[0]} sequences\")\n            print(f\"  - Test: {X_seq_test.shape[0]} sequences\")\n\n            # Save LSTM feature store\n            print(\"\\n=== Testing LSTM feature store saving ===\")\n            import numpy as np\n            from datetime import datetime\n            import json\n\n            # Create output directory\n            output_dir = os.path.join(orchestrator.feature_store_dir, 'lstm')\n            os.makedirs(output_dir, exist_ok=True)\n\n            # Create timestamp for directory\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            feature_store_dir = os.path.join(output_dir, f\"{args.db}_test_lstm_feature_store_{timestamp}\")\n            os.makedirs(feature_store_dir, exist_ok=True)\n\n            print(f\"Saving LSTM data to {feature_store_dir}...\")\n\n            # Save numpy arrays\n            np.save(os.path.join(feature_store_dir, \"X_seq_train.npy\"), X_seq_train)\n            np.save(os.path.join(feature_store_dir, \"X_seq_val.npy\"), X_seq_val)\n            np.save(os.path.join(feature_store_dir, \"X_seq_test.npy\"), X_seq_test)\n\n            np.save(os.path.join(feature_store_dir, \"X_static_train.npy\"), X_static_train)\n            np.save(os.path.join(feature_store_dir, \"X_static_val.npy\"), X_static_val)\n            np.save(os.path.join(feature_store_dir, \"X_static_test.npy\"), X_static_test)\n\n            np.save(os.path.join(feature_store_dir, \"y_train.npy\"), y_train)\n            np.save(os.path.join(feature_store_dir, \"y_val.npy\"), y_val)\n            np.save(os.path.join(feature_store_dir, \"y_test.npy\"), y_test)\n\n            # Save metadata\n            metadata = {\n                'created_at': datetime.now().isoformat(),\n                'sequence_length': sequence_length,\n                'step_size': 1,\n                'embedding_dim': orchestrator.embedding_dim,\n                'dataset_info': {\n                    'train_samples': len(X_seq_train),\n                    'val_samples': len(X_seq_val),\n                    'test_samples': len(X_seq_test),\n                    'sequence_shape': list(X_seq_train.shape),\n                    'static_shape': list(X_static_train.shape),\n                },\n                'feature_columns': orchestrator.preprocessing_params.get('feature_columns', []),\n            }\n\n            with open(os.path.join(feature_store_dir, \"metadata.json\"), 'w') as f:\n                json.dump(metadata, f, indent=2, default=str)\n\n            print(f\"Successfully saved LSTM feature store to {feature_store_dir}\")\n\n            # Test loading LSTM feature store if the method exists\n            if hasattr(orchestrator, 'load_lstm_feature_store'):\n                print(\"\\n=== Testing LSTM feature store loading ===\")\n                try:\n                    result = orchestrator.load_lstm_feature_store(feature_store_path=feature_store_dir)\n\n                    if len(result) >= 9:  # We expect at least 9 elements\n                        loaded_X_seq_train = result[0]\n                        loaded_X_static_train = result[3]\n                        loaded_y_train = result[6]\n\n                        print(f\"Successfully loaded LSTM feature store\")\n                        print(f\"Loaded data shapes match original:\")\n                        print(f\"  - X_sequences: {loaded_X_seq_train.shape == X_seq_train.shape}\")\n                        print(f\"  - X_static: {loaded_X_static_train.shape == X_static_train.shape}\")\n                        print(f\"  - y: {loaded_y_train.shape == y_train.shape}\")\n                    else:\n                        print(f\"Unexpected format returned from load_lstm_feature_store\")\n\n                except Exception as e:\n                    print(f\"Error loading LSTM feature store: {str(e)}\")\n\n            print(\"\\nLSTM feature store testing completed successfully!\")\n            return True\n        except ValueError as ve:\n            print(f\"ValueError in sequence preparation: {str(ve)}\")\n            print(\"This might be normal if there aren't enough sequences for the given horses.\")\n            return False\n\n    except Exception as e:\n        print(f\"Unexpected error in LSTM data preparation: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\n\ndef test_full_pipeline(orchestrator, args):\n    \"\"\"Test the full pipeline with the orchestrator.\"\"\"\n    print(\"\\n=== Testing full pipeline ===\")\n    try:\n        # Call the existing run_pipeline method if it exists\n        if hasattr(orchestrator, 'run_pipeline'):\n            print(\"Running full pipeline...\")\n            result = orchestrator.run_pipeline(\n                limit=args.limit,\n                race_filter=args.race_type,\n                use_cache=not args.no_cache\n            )\n\n            if isinstance(result, tuple) and len(result) >= 6:  # Ensure we got at least 6 elements (X_train...y_test)\n                X_train, X_val, X_test, y_train, y_val, y_test = result[:6]\n                feature_store_path = result[6] if len(result) > 6 else None\n\n                print(f\"\\nPipeline results:\")\n                print(f\"  - Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n                print(f\"  - Validation set: {X_val.shape[0]} samples\")\n                print(f\"  - Test set: {X_test.shape[0]} samples\")\n\n                # Print some embedding statistics\n                embedding_cols = {\n                    'horse': [col for col in X_train.columns if col.startswith('horse_emb_')],\n                    'jockey': [col for col in X_train.columns if col.startswith('jockey_emb_')],\n                    'couple': [col for col in X_train.columns if col.startswith('couple_emb_')],\n                    'course': [col for col in X_train.columns if col.startswith('course_emb_')]\n                }\n\n                print(\"\\nEmbedding features:\")\n                for entity, cols in embedding_cols.items():\n                    if cols:\n                        print(f\"  - {entity.capitalize()}: {len(cols)} dimensions\")\n\n                if feature_store_path:\n                    print(f\"\\nFeature store saved to: {feature_store_path}\")\n\n                print(\"\\nFull pipeline test completed successfully!\")\n                return True\n            else:\n                print(f\"Unexpected format returned from run_pipeline\")\n                return False\n        else:\n            # If run_pipeline doesn't exist, run the steps manually\n            print(\"run_pipeline method not found. Running steps manually...\")\n\n            # 1. Load data\n            df = test_data_loading(orchestrator, args)\n            if df is None:\n                return False\n\n            # 2. Preprocess data\n            embedded_df = test_feature_preparation(orchestrator, df, args)\n            if embedded_df is None:\n                return False\n\n            # 3. Save feature store\n            feature_store_result = test_feature_store(orchestrator, embedded_df, args)\n\n            print(\"\\nFull pipeline test completed by running individual steps\")\n            return feature_store_result\n\n    except Exception as e:\n        print(f\"Error in full pipeline test: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Test the FeatureEmbeddingOrchestrator\")\n    parser.add_argument('--limit', type=int, default=10, help='Limit the number of races to process')\n    parser.add_argument('--race-type', type=str, default=None, help='Filter by race type (e.g., \"A\" for Attele)')\n    parser.add_argument('--run-full', action='store_true', help='Run the full pipeline')\n    parser.add_argument('--test-load', action='store_true', help='Test only feature store loading')\n    parser.add_argument('--test-lstm', action='store_true', help='Test LSTM data preparation')\n    parser.add_argument('--no-cache', action='store_true', help='Disable use of cache')\n    parser.add_argument('--clear-cache', action='store_true', help='Clear cache before running')\n    parser.add_argument('--db', type=str, default=\"dev\", help='Database to use from config')\n    parser.add_argument('--embedding-dim', type=int, default=8, help='Embedding dimension to use')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    args = parser.parse_args()\n\n    # Initialize orchestrator with specified database\n    config = AppConfig()\n    sqlite_path = config.get_sqlite_dbpath(args.db)\n\n    # Create orchestrator with only arguments we know exist\n    try:\n        # Try with all parameters first\n        orchestrator = FeatureEmbeddingOrchestrator(\n            sqlite_path=sqlite_path,\n            embedding_dim=args.embedding_dim,\n            verbose=args.verbose\n        )\n    except TypeError:\n        # If verbose is not supported, try without it\n        print(\"Note: 'verbose' parameter not supported, creating orchestrator without it.\")\n        orchestrator = FeatureEmbeddingOrchestrator(\n            sqlite_path=sqlite_path,\n            embedding_dim=args.embedding_dim\n        )\n\n    print(f\"Testing orchestrator with database: {args.db} ({sqlite_path})\")\n    print(f\"Using embedding dimension: {args.embedding_dim}\")\n\n    # Clear cache if requested\n    if args.clear_cache and hasattr(orchestrator, 'cache_manager') and hasattr(orchestrator.cache_manager,\n                                                                               'clear_cache'):\n        print(\"Clearing cache...\")\n        orchestrator.cache_manager.clear_cache()\n\n    # Test only loading if requested\n    if args.test_load:\n        print(\"\\n=== Testing feature store loading only ===\")\n        try:\n            # Check for list_feature_stores method\n            if hasattr(orchestrator, '_list_feature_stores'):\n                feature_stores = orchestrator._list_feature_stores()\n            else:\n                # Try direct path if method doesn't exist\n                feature_store_dir = orchestrator.feature_store_dir\n                if os.path.exists(feature_store_dir):\n                    feature_stores = [d for d in os.listdir(feature_store_dir)\n                                      if os.path.isdir(os.path.join(feature_store_dir, d))\n                                      and d.startswith('feature_store_')]\n                    feature_stores.sort(reverse=True)  # Most recent first\n                else:\n                    feature_stores = []\n\n            if not feature_stores:\n                print(\"No feature stores found. Run the test first to create a feature store.\")\n                return\n\n            # Load the most recent feature store\n            feature_store_path = os.path.join(orchestrator.feature_store_dir, feature_stores[0])\n            print(f\"Loading most recent feature store: {feature_store_path}\")\n\n            if hasattr(orchestrator, 'load_feature_store'):\n                result = orchestrator.load_feature_store(feature_store_path=feature_store_path)\n                if len(result) >= 6:\n                    X_train, X_val, X_test, y_train, y_val, y_test = result[:6]\n\n                    print(f\"Successfully loaded feature store\")\n                    print(f\"Loaded data shapes:\")\n                    print(f\"  - X_train: {X_train.shape}\")\n                    print(f\"  - y_train: {y_train.shape}\")\n\n                    print(\"\\nFeature store loading test completed successfully!\")\n                else:\n                    print(f\"Unexpected format returned from load_feature_store\")\n            else:\n                print(\"Error: No 'load_feature_store' method found\")\n            return\n        except Exception as e:\n            print(f\"Error loading feature store: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            return\n\n    # If running full pipeline test\n    if args.run_full:\n        test_full_pipeline(orchestrator, args)\n        return\n\n    # Otherwise run step by step tests\n\n    # Test data loading\n    df = test_data_loading(orchestrator, args)\n    if df is None:\n        print(\"Data loading failed, stopping tests.\")\n        return\n\n    # Test feature preparation\n    embedded_df = test_feature_preparation(orchestrator, df, args)\n    if embedded_df is None:\n        print(\"Feature preparation failed, stopping tests.\")\n        return\n\n    # Test feature store functionality\n    feature_store_result = test_feature_store(orchestrator, embedded_df, args)\n\n    # Test LSTM data preparation if requested\n    if args.test_lstm:\n        test_lstm_data_preparation(orchestrator, embedded_df, args)\n\n    print(\"\\nAll tests completed!\")\n\n\nif __name__ == '__main__':\n    main()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test.py b/test.py
--- a/test.py	(revision 0d88e73ffd4a3899fde119bb4ed28c1bf49fab9b)
+++ b/test.py	(date 1746183014228)
@@ -1,529 +1,1045 @@
-# test.py
 import os
+import sys
+import json
+import pandas as pd
+import numpy as np
+import sqlite3
+from datetime import datetime, timedelta
+from pathlib import Path
+import matplotlib.pyplot as plt
 import argparse
+import logging
+from typing import List, Dict, Any, Tuple
+import warnings
+import time
+
+# Filter specific warnings
+warnings.filterwarnings("ignore", category=FutureWarning, message=".*Downcasting behavior in `replace` is deprecated.*")
+warnings.filterwarnings("ignore", category=RuntimeWarning, message=".*Mean of empty slice.*")
+warnings.filterwarnings("ignore", category=RuntimeWarning)
+warnings.filterwarnings("ignore", category=FutureWarning)
+
+# Import core components
 from utils.env_setup import AppConfig
-from utils.cache_manager import CacheManager
-from core.orchestrators.embedding_feature import FeatureEmbeddingOrchestrator
-
-
-def test_cache_manager():
-    """Test the CacheManager functionality."""
-    from utils.cache_manager import CacheManager
-    import pandas as pd
-
-    # Create a test DataFrame
-    test_df = pd.DataFrame({
-        'id': [1, 2, 3],
-        'value': ['a', 'b', 'c']
-    })
+from core.orchestrators.prediction_orchestrator import PredictionOrchestrator
+from core.connectors.api_daily_sync import RaceFetcher
+from race_prediction.race_predict import RacePredictor
 
-    # Initialize cache manager
-    cache_manager = CacheManager()
 
-    # Test cache type
-    cache_type = "test_cache"
+class BlendingComparison:
+    """
+    Class to compare race prediction results with different blending values.
+    """
 
-    # Clear any existing cache
-    cache_manager.clear_cache(cache_type)
+    def __init__(self, model_path: str, db_name: str = None, output_dir: str = None,
+                 blending_values: List[float] = None, verbose: bool = False):
+        """
+        Initialize the blending comparison tool.
 
-    # Test saving
-    print(f"Saving DataFrame to cache type '{cache_type}'...")
-    saved_df = cache_manager.save_dataframe(test_df, cache_type)
-    print(f"Saved DataFrame")
+        Args:
+            model_path: Path to the model or model name
+            db_name: Database name from config (default: active_db from config)
+            output_dir: Directory to save result files
+            blending_values: List of blending values to test (default: [0.0, 0.25, 0.5, 0.75, 1.0])
+            verbose: Whether to output verbose logs
+        """
+        # Initialize config
+        self.config = AppConfig()
 
-    # Test loading
-    print(f"Loading DataFrame from cache type '{cache_type}'...")
-    loaded_df = cache_manager.load_dataframe(cache_type)
-
-    if loaded_df is not None:
-        print("Successfully loaded cached data:")
-        print(loaded_df)
-        return True
-    else:
-        print("Failed to load cached data")
-        return False
-
-
-def test_data_loading(orchestrator, args):
-    """Test basic data loading functionality."""
-    print("\n=== Testing data loading ===")
-    # Checking for correct method name for loading data
-    if hasattr(orchestrator, 'load_historical_races'):
-        df = orchestrator.load_historical_races(limit=args.limit, race_filter=args.race_type)
-    else:
-        print("Error: 'load_historical_races' method not found")
-        return None
-
-    print(f"Successfully loaded {len(df)} participant records from {df['comp'].nunique()} races")
-    print(f"Sample columns: {list(df.columns[:5])}")
-    return df
-
-
-def test_feature_preparation(orchestrator, df, args):
-    """Test feature preparation and embedding."""
-    print("\n=== Testing feature preparation ===")
-    try:
-        # Check for correct preprocessing method
-        if hasattr(orchestrator, 'prepare_features'):
-            print("Using prepare_features method...")
-            processed_df = orchestrator.prepare_features(df, use_cache=not args.no_cache)
-        elif hasattr(orchestrator, 'preprocess_data'):
-            print("Using preprocess_data method...")
-            processed_df = orchestrator.preprocess_data(df, use_cache=not args.no_cache)
+        # Set database
+        if db_name is None:
+            self.db_name = self.config._config.base.active_db
         else:
-            print("Error: No preprocessing method found")
-            return None
-
-        print(f"Successfully preprocessed data: {processed_df.shape}")
-
-        # Apply embeddings
-        if hasattr(orchestrator, 'apply_embeddings'):
-            embedded_df = orchestrator.apply_embeddings(processed_df, use_cache=not args.no_cache)
-            print(f"Successfully applied embeddings: {embedded_df.shape}")
+            self.db_name = db_name
 
-            # Check for embedding columns
-            embedding_cols = [col for col in embedded_df.columns if '_emb_' in col]
-            print(f"Found {len(embedding_cols)} embedding columns")
+        # Get database path
+        self.db_path = self.config.get_sqlite_dbpath(self.db_name)
 
-            for prefix in ['horse', 'jockey', 'couple', 'course']:
-                cols = [col for col in embedding_cols if col.startswith(f"{prefix}_emb_")]
-                if cols:
-                    print(f"  - {prefix.capitalize()} embeddings: {len(cols)}")
+        # Set model path
+        self.model_path = model_path
 
-            return embedded_df
-        else:
-            print("No 'apply_embeddings' method found, using processed data")
-            return processed_df  # Return processed data if embedding not available
+        # Set blending values
+        self.blending_values = blending_values or [0.0, 0.25, 0.5, 0.75, 1.0]
 
-    except Exception as e:
-        print(f"Error preparing features: {str(e)}")
-        import traceback
-        traceback.print_exc()
-        return None
+        # Set verbosity
+        self.verbose = verbose
 
-
-def test_feature_store(orchestrator, embedded_df, args):
-    """Test feature store saving and loading."""
-    print("\n=== Testing feature store functionality ===")
-    try:
-        # Prepare training dataset
-        print("Preparing training dataset...")
-        if hasattr(orchestrator, 'prepare_training_dataset'):
-            X, y = orchestrator.prepare_training_dataset(embedded_df)
+        # Set output directory
+        if output_dir is None:
+            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+            self.output_dir = Path(f"results/blend_comparison_{timestamp}")
         else:
-            print("Error: No 'prepare_training_dataset' method found")
-            return False
+            self.output_dir = Path(output_dir)
+
+        # Create output directory if it doesn't exist
+        self.output_dir.mkdir(parents=True, exist_ok=True)
+
+        # Set up logging
+        self._setup_logging()
+
+        # Initialize race fetcher for database access
+        self.race_fetcher = RaceFetcher(db_name=self.db_name)
+
+        # Initialize orchestrator with caching capability
+        self.orchestrator = PredictionOrchestrator(
+            model_path=self.model_path,
+            db_name=self.db_name,
+            verbose=self.verbose
+        )
+
+        # Make sure prediction_cache exists (add if not)
+        if not hasattr(self.orchestrator, 'prediction_cache'):
+            self.orchestrator.prediction_cache = {}
+
+        # Make sure race_predictor has prediction_cache (add if not)
+        if not hasattr(self.orchestrator.race_predictor, 'prediction_cache'):
+            self.orchestrator.race_predictor.prediction_cache = {}
+
+        # Store results
+        self.results = {}
 
-        print(f"Successfully prepared dataset with {X.shape[1]} features and {len(y)} samples")
+        print(f"Blending comparison initialized:")
+        print(f"  - Model path: {self.model_path}")
+        print(f"  - Database: {self.db_name} ({self.db_path})")
+        print(f"  - Output directory: {self.output_dir}")
+        print(f"  - Blending values: {self.blending_values}")
+        self.logger.info(f"Blending comparison initialized:")
+        self.logger.info(f"  - Model path: {self.model_path}")
+        self.logger.info(f"  - Database: {self.db_name} ({self.db_path})")
+        self.logger.info(f"  - Output directory: {self.output_dir}")
+        self.logger.info(f"  - Blending values: {self.blending_values}")
 
-        # Split dataset
-        print("Splitting dataset...")
-        if hasattr(orchestrator, 'split_dataset'):
-            X_train, X_val, X_test, y_train, y_val, y_test = orchestrator.split_dataset(X, y)
-        else:
-            # Use scikit-learn directly if the method doesn't exist
-            print("No 'split_dataset' method found, using scikit-learn directly")
-            from sklearn.model_selection import train_test_split
+    def _setup_logging(self):
+        """Set up logging."""
+        log_dir = self.output_dir / "logs"
+        log_dir.mkdir(parents=True, exist_ok=True)
+
+        log_file = log_dir / f"blend_comparison_{datetime.now().strftime('%Y%m%d')}.log"
+
+        logging.basicConfig(
+            level=logging.DEBUG if self.verbose else logging.INFO,
+            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
+            handlers=[
+                logging.FileHandler(log_file),
+                logging.StreamHandler()
+            ],
+            force=True  # Reset existing loggers
+        )
+
+        self.logger = logging.getLogger("BlendingComparison")
+        self.logger.info(f"Logging initialized to {log_file}")
+
+    def _clear_predictions(self):
+        """Clear prediction results from the database."""
+        print("Clearing existing prediction results from database...")
+        self.logger.info("Clearing existing prediction results from database...")
+
+        conn = sqlite3.connect(self.db_path)
+        cursor = conn.cursor()
+
+        # Update all daily_race entries to clear prediction_results
+        cursor.execute("UPDATE daily_race SET prediction_results = NULL")
+
+        # Commit and close
+        conn.commit()
+        count = cursor.rowcount
+        conn.close()
+
+        print(f"Cleared prediction results from {count} races")
+        self.logger.info(f"Cleared prediction results from {count} races")
+
+        # Also clear the cache
+        if hasattr(self.orchestrator, 'prediction_cache'):
+            self.orchestrator.prediction_cache.clear()
+        if hasattr(self.orchestrator.race_predictor, 'prediction_cache'):
+            self.orchestrator.race_predictor.prediction_cache.clear()
+
+    def _get_evaluation_dates(self, start_date: str = None, num_days: int = 3) -> List[str]:
+        """
+        Get dates for evaluation.
+
+        Args:
+            start_date: Starting date in YYYY-MM-DD format (default: 3 days ago)
+            num_days: Number of days to evaluate
+
+        Returns:
+            List of date strings in YYYY-MM-DD format
+        """
+        if start_date is None:
+            # Default to 3 days ago
+            start_dt = datetime.now() - timedelta(days=num_days)
+            start_date = start_dt.strftime("%Y-%m-%d")
+
+        # Convert to datetime
+        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
+
+        # Generate list of dates
+        dates = [
+            (start_dt + timedelta(days=i)).strftime("%Y-%m-%d")
+            for i in range(num_days)
+        ]
+
+        return dates
+
+    def _check_race_availability(self, dates: List[str]) -> Tuple[bool, int]:
+        """
+        Check if races are available for the specified dates.
+
+        Args:
+            dates: List of date strings
+
+        Returns:
+            Tuple of (all_have_results, total_race_count)
+        """
+        conn = sqlite3.connect(self.db_path)
+        cursor = conn.cursor()
+
+        all_have_results = True
+        total_race_count = 0
+
+        for date in dates:
+            # Check if races exist for the date
+            cursor.execute(
+                "SELECT COUNT(*) FROM daily_race WHERE jour = ?",
+                (date,)
+            )
+            race_count = cursor.fetchone()[0]
+
+            # Check if they have results
+            cursor.execute(
+                "SELECT COUNT(*) FROM daily_race WHERE jour = ? AND actual_results IS NOT NULL AND actual_results != 'pending'",
+                (date,)
+            )
+            results_count = cursor.fetchone()[0]
+
+            if race_count == 0:
+                print(f"Warning: No races found for date {date}")
+                self.logger.warning(f"No races found for date {date}")
+                all_have_results = False
+            elif results_count < race_count:
+                print(f"Warning: Only {results_count}/{race_count} races have results for date {date}")
+                self.logger.warning(f"Only {results_count}/{race_count} races have results for date {date}")
+                all_have_results = False
+            else:
+                print(f"Found {race_count} races with results for date {date}")
+                self.logger.info(f"Found {race_count} races with results for date {date}")
+
+            total_race_count += race_count
+
+        conn.close()
+
+        return all_have_results, total_race_count
+
+    def evaluate_blending_values(self, dates: List[str] = None, num_days: int = 3, limit=None) -> Dict[str, Any]:
+        """
+        Evaluate prediction performance for different blending values.
+
+        Args:
+            dates: List of dates to evaluate (default: last 3 days)
+            num_days: Number of days to evaluate if dates not provided
+            limit: Maximum number of races to evaluate per date
+
+        Returns:
+            Dictionary with evaluation results for each blending value
+        """
+        # Get dates for evaluation if not provided
+        if dates is None:
+            dates = self._get_evaluation_dates(num_days=num_days)
+
+        print(f"Evaluating blending values for dates: {dates}")
+        self.logger.info(f"Evaluating blending values for dates: {dates}")
+
+        # Check if races are available
+        all_have_results, total_race_count = self._check_race_availability(dates)
+
+        if total_race_count == 0:
+            print("Error: No races found for the specified dates. Aborting.")
+            self.logger.error("No races found for the specified dates. Aborting.")
+            return {"status": "error", "message": "No races found"}
+
+        if not all_have_results:
+            print("Warning: Some races may not have results. Evaluation may be incomplete.")
+            self.logger.warning("Some races may not have results. Evaluation may be incomplete.")
+
+        # Clear cache before starting (to ensure a clean run)
+        self._clear_predictions()
+
+        # Track performance metrics
+        performance_data = []
+
+        # Evaluate each blending value
+        for blend_idx, blend_value in enumerate(self.blending_values):
+            print(f"\n{'-' * 40}")
+            print(f"Evaluating blending value: {blend_value} [{blend_idx + 1}/{len(self.blending_values)}]")
+            self.logger.info(f"\n{'=' * 40}")
+            self.logger.info(f"Evaluating blending value: {blend_value} [{blend_idx + 1}/{len(self.blending_values)}]")
+
+            # For the first blend value, we always clear predictions to establish a baseline
+            if blend_idx == 0:
+                self._clear_predictions()
+
+            # Process each date
+            date_results = []
+            for date_idx, date in enumerate(dates):
+                print(f"Processing date: {date} [{date_idx + 1}/{len(dates)}]")
+                self.logger.info(f"Processing date: {date} [{date_idx + 1}/{len(dates)}]")
+
+                # Get races for this date if we need to limit them
+                all_races = self.orchestrator.race_fetcher.get_races_by_date(date)
+
+                # Skip if no races
+                if not all_races:
+                    print(f"No races found for date {date}, skipping")
+                    continue
+
+                # Apply limit if needed
+                races_to_process = all_races
+                if limit is not None and limit > 0 and len(all_races) > limit:
+                    races_to_process = all_races[:limit]
+                    print(f"Limiting from {len(all_races)} to {limit} races for {date}")
+
+                # Process each race
+                per_race_times = []
+                for race_idx, race in enumerate(races_to_process):
+                    comp = race['comp']
+                    print(f"  Race {race_idx + 1}/{len(races_to_process)}: {comp}")
+
+                    # Measure prediction time
+                    start_time = time.time()
+                    prediction_results = self.orchestrator.predict_race(comp, blend_weight=blend_value)
+                    prediction_time = time.time() - start_time
+
+                    # Store timing info
+                    has_cached = False
+                    reblend_time = None
+
+                    # Check if we used cache for this prediction
+                    if hasattr(self.orchestrator, 'prediction_cache') and comp in self.orchestrator.prediction_cache:
+                        has_cached = True
+                        # Extract reblend time if available
+                        reblend_time = prediction_results.get('reblend_time')
 
-            # First split: train+val vs test (0.2)
-            X_trainval, X_test, y_trainval, y_test = train_test_split(
-                X, y, test_size=0.2, random_state=42
-            )
+                    per_race_times.append({
+                        'comp': comp,
+                        'prediction_time': prediction_time,
+                        'reblend_time': reblend_time,
+                        'used_cache': has_cached,
+                        'blend_value': blend_value,
+                        'date': date
+                    })
 
-            # Second split: train vs val (0.125 of original = 0.1/0.8)
-            X_train, X_val, y_train, y_val = train_test_split(
-                X_trainval, y_trainval, test_size=0.125, random_state=42
-            )
+                # Evaluate all predictions for this date
+                start_time = time.time()
+                evaluation_results = self.orchestrator.evaluate_predictions_by_date(date)
+                evaluation_time = time.time() - start_time
 
-        print(f"Successfully split dataset:")
-        print(f"  - Training: {X_train.shape[0]} samples")
-        print(f"  - Validation: {X_val.shape[0]} samples")
-        print(f"  - Testing: {X_test.shape[0]} samples")
+                # Store timing metrics
+                avg_prediction_time = sum(r['prediction_time'] for r in per_race_times) / len(
+                    per_race_times) if per_race_times else 0
 
-        # Save feature store
-        print("\n=== Testing feature store saving ===")
-        if hasattr(orchestrator, 'save_feature_store'):
-            feature_store_path = orchestrator.save_feature_store(
-                X_train, X_val, X_test, y_train, y_val, y_test,
-                prefix=f"{args.db}_test_"
-            )
-        else:
-            print("Error: No 'save_feature_store' method found")
-            return False
+                # Store reblend metrics if available
+                reblend_times = [r['reblend_time'] for r in per_race_times if r['reblend_time'] is not None]
+                avg_reblend_time = sum(reblend_times) / len(reblend_times) if reblend_times else None
+
+                date_metrics = {
+                    'date': date,
+                    'blend_value': blend_value,
+                    'num_races': len(races_to_process),
+                    'avg_prediction_time': avg_prediction_time,
+                    'avg_reblend_time': avg_reblend_time,
+                    'evaluation_time': evaluation_time,
+                    'first_blend': blend_idx == 0
+                }
+
+                # Add to performance tracking
+                performance_data.append(date_metrics)
+
+                date_results.append({
+                    "date": date,
+                    "prediction_results": per_race_times,
+                    "evaluation_results": evaluation_results,
+                    "performance": date_metrics
+                })
+
+            # Store results for this blending value
+            self.results[str(blend_value)] = {
+                "blend_value": blend_value,
+                "date_results": date_results,
+                "summary": self._calculate_summary(date_results)
+            }
+
+            # Save intermediate results
+            self._save_results()
+
+            # Log performance improvement if not first blend value
+            if blend_idx > 0 and performance_data:
+                first_blend_metrics = [d for d in performance_data if d['first_blend']]
+                this_blend_metrics = [d for d in performance_data if d['blend_value'] == blend_value]
+
+                if first_blend_metrics and this_blend_metrics:
+                    avg_first_time = sum(d['avg_prediction_time'] for d in first_blend_metrics) / len(
+                        first_blend_metrics)
+                    avg_this_time = sum(d['avg_prediction_time'] for d in this_blend_metrics) / len(this_blend_metrics)
+
+                    if avg_this_time > 0:
+                        speedup = avg_first_time / avg_this_time
+                        print(f"Performance improvement: {speedup:.2f}x faster than first blend")
+                        self.logger.info(f"Performance improvement: {speedup:.2f}x faster than first blend")
+
+                        # Log reblend times if available
+                        avg_reblend_times = [d['avg_reblend_time'] for d in this_blend_metrics if
+                                             d['avg_reblend_time'] is not None]
+                        if avg_reblend_times:
+                            avg_reblend = sum(avg_reblend_times) / len(avg_reblend_times)
+                            reblend_speedup = avg_first_time / avg_reblend
+                            print(f"Reblend speedup: {reblend_speedup:.2f}x faster than first blend")
+                            self.logger.info(f"Reblend speedup: {reblend_speedup:.2f}x faster than first blend")
+
+        # Generate comparison report
+        comparison = self._generate_comparison()
+
+        # Add performance metrics to comparison
+        comparison['performance'] = performance_data
+
+        # Visualize results
+        self._visualize_results()
+
+        # Performance visualization
+        self._visualize_performance(performance_data)
+
+        return comparison
+
+    def _calculate_summary(self, date_results: List[Dict]) -> Dict[str, Any]:
+        """
+        Calculate summary metrics across all evaluated dates.
+
+        Args:
+            date_results: List of results for each date
+
+        Returns:
+            Dictionary with summary metrics
+        """
+        # Initialize counters and accumulators
+        total_races = 0
+        total_evaluated = 0
+        total_correct_winner = 0
+        total_podium_accuracy = 0
+        total_races_with_metrics = 0  # Count races that actually have metrics
+
+        # PMU bet types - initialize counters
+        pmu_bets = {
+            'tierce_exact': 0, 'tierce_desordre': 0,
+            'quarte_exact': 0, 'quarte_desordre': 0,
+            'quinte_exact': 0, 'quinte_desordre': 0,
+            'bonus4': 0, 'bonus3': 0,
+            'deuxsur4': 0, 'multi4': 0
+        }
+
+        # Process each date
+        for date_result in date_results:
+            eval_result = date_result.get('evaluation_results', {})
+
+            # Get summary metrics if available
+            if 'summary_metrics' in eval_result:
+                metrics = eval_result['summary_metrics']
+
+                # Add to totals
+                races_evaluated = metrics.get('races_evaluated', 0)
+                total_races_with_metrics += races_evaluated
+
+                if races_evaluated > 0:
+                    # Calculate proportional metrics
+                    winner_accuracy = metrics.get('winner_accuracy', 0)
+                    podium_accuracy = metrics.get('avg_podium_accuracy', 0)
+
+                    total_correct_winner += winner_accuracy * races_evaluated
+                    total_podium_accuracy += podium_accuracy * races_evaluated
+
+                    # Add PMU bet counts
+                    for bet_type in pmu_bets.keys():
+                        pmu_bets[bet_type] += metrics.get('pmu_bets', {}).get(bet_type, 0)
+
+            # Count total races
+            pred_result = date_result.get('prediction_results', {})
+            if isinstance(pred_result, list):
+                total_races += len(pred_result)
+            else:
+                total_races += pred_result.get('total_races', 0)
+                total_evaluated += pred_result.get('predicted', 0)
 
-        print(f"Successfully saved feature store to {feature_store_path}")
+        # Calculate averages
+        if total_races_with_metrics > 0:
+            avg_winner_accuracy = total_correct_winner / total_races_with_metrics
+            avg_podium_accuracy = total_podium_accuracy / total_races_with_metrics
 
-        # Load feature store
-        print("\n=== Testing feature store loading ===")
-        if hasattr(orchestrator, 'load_feature_store'):
-            loaded_data = orchestrator.load_feature_store(feature_store_path=feature_store_path)
+            # Calculate PMU bet rates
+            pmu_rates = {f"{k}_rate": v / total_races_with_metrics for k, v in pmu_bets.items()}
+
+            return {
+                "total_races": total_races,
+                "total_evaluated": total_evaluated,
+                "total_with_metrics": total_races_with_metrics,
+                "winner_accuracy": avg_winner_accuracy,
+                "podium_accuracy": avg_podium_accuracy,
+                "pmu_bets": {**pmu_bets, **pmu_rates}
+            }
         else:
-            print("Error: No 'load_feature_store' method found")
-            return False
+            return {
+                "total_races": total_races,
+                "total_evaluated": total_evaluated,
+                "total_with_metrics": total_races_with_metrics,
+                "winner_accuracy": 0,
+                "podium_accuracy": 0,
+                "pmu_bets": {k: 0 for k in pmu_bets.keys()}
+            }
+
+    def _generate_comparison(self) -> Dict[str, Any]:
+        """
+        Generate a comparison of results for different blending values.
+
+        Returns:
+            Dictionary with comparison results
+        """
+        comparison = {
+            "blending_values": self.blending_values,
+            "metrics": {}
+        }
+
+        # Collect key metrics for each blending value
+        winner_accuracy = []
+        podium_accuracy = []
+        tierce_exact_rate = []
+        tierce_desordre_rate = []
+        quarte_exact_rate = []
+
+        # Find best values for each metric
+        best_winner = {"value": 0, "blend": None}
+        best_podium = {"value": 0, "blend": None}
+        best_tierce_exact = {"value": 0, "blend": None}
+        best_tierce_desordre = {"value": 0, "blend": None}
+        best_quarte_exact = {"value": 0, "blend": None}
+
+        for blend_str, result in self.results.items():
+            blend_value = float(blend_str)
+            summary = result.get('summary', {})
+
+            # Extract key metrics
+            win_acc = summary.get('winner_accuracy', 0)
+            pod_acc = summary.get('podium_accuracy', 0)
+            t_exact = summary.get('pmu_bets', {}).get('tierce_exact_rate', 0)
+            t_desordre = summary.get('pmu_bets', {}).get('tierce_desordre_rate', 0)
+            q_exact = summary.get('pmu_bets', {}).get('quarte_exact_rate', 0)
+
+            # Store for comparison
+            winner_accuracy.append(win_acc)
+            podium_accuracy.append(pod_acc)
+            tierce_exact_rate.append(t_exact)
+            tierce_desordre_rate.append(t_desordre)
+            quarte_exact_rate.append(q_exact)
 
-        if len(loaded_data) >= 6:  # Ensure we got at least 6 elements: X_train...y_test
-            loaded_X_train, loaded_X_val, loaded_X_test, loaded_y_train, loaded_y_val, loaded_y_test = loaded_data[:6]
-            metadata = loaded_data[6] if len(loaded_data) > 6 else {}
+            # Update best values
+            if win_acc > best_winner["value"]:
+                best_winner = {"value": win_acc, "blend": blend_value}
+            if pod_acc > best_podium["value"]:
+                best_podium = {"value": pod_acc, "blend": blend_value}
+            if t_exact > best_tierce_exact["value"]:
+                best_tierce_exact = {"value": t_exact, "blend": blend_value}
+            if t_desordre > best_tierce_desordre["value"]:
+                best_tierce_desordre = {"value": t_desordre, "blend": blend_value}
+            if q_exact > best_quarte_exact["value"]:
+                best_quarte_exact = {"value": q_exact, "blend": blend_value}
 
-            print(f"Successfully loaded feature store from {feature_store_path}")
-            print(f"Loaded data shapes match original: "
-                  f"{loaded_X_train.shape == X_train.shape}, "
-                  f"{loaded_y_train.shape == y_train.shape}")
+        # Store metric arrays
+        comparison["metrics"]["winner_accuracy"] = winner_accuracy
+        comparison["metrics"]["podium_accuracy"] = podium_accuracy
+        comparison["metrics"]["tierce_exact_rate"] = tierce_exact_rate
+        comparison["metrics"]["tierce_desordre_rate"] = tierce_desordre_rate
+        comparison["metrics"]["quarte_exact_rate"] = quarte_exact_rate
 
-            # Print some metadata
-            if metadata:
-                print("\nFeature store metadata:")
-                if 'created_at' in metadata:
-                    print(f"  - Created: {metadata['created_at']}")
-                if 'dataset_info' in metadata:
-                    info = metadata['dataset_info']
-                    print(f"  - Training samples: {info.get('train_samples')}")
-                    print(f"  - Feature count: {info.get('feature_count')}")
+        # Store best values
+        comparison["best"] = {
+            "winner": best_winner,
+            "podium": best_podium,
+            "tierce_exact": best_tierce_exact,
+            "tierce_desordre": best_tierce_desordre,
+            "quarte_exact": best_quarte_exact
+        }
 
-                    # Count embedding features
-                    if 'embedding_features' in info:
-                        embedding_types = {
-                            'horse': [col for col in info['embedding_features'] if col.startswith('horse_emb_')],
-                            'jockey': [col for col in info['embedding_features'] if col.startswith('jockey_emb_')],
-                            'couple': [col for col in info['embedding_features'] if col.startswith('couple_emb_')],
-                            'course': [col for col in info['embedding_features'] if col.startswith('course_emb_')]
-                        }
+        # Calculate overall recommendation (weighted average)
+        weights = {
+            "winner": 0.3,
+            "podium": 0.3,
+            "tierce_exact": 0.2,
+            "tierce_desordre": 0.1,
+            "quarte_exact": 0.1
+        }
 
-                        print(f"  - Embedding features:")
-                        for entity, cols in embedding_types.items():
-                            if cols:
-                                print(f"    - {entity.capitalize()}: {len(cols)} dimensions")
+        # Create a score for each blending value
+        scores = {}
+        for blend_str, result in self.results.items():
+            blend_value = float(blend_str)
+            summary = result.get('summary', {})
+
+            # Handle zero case for best values (avoid division by zero)
+            normalized_scores = []
+
+            if best_winner["value"] > 0:
+                normalized_scores.append(weights["winner"] * summary.get('winner_accuracy', 0) / best_winner["value"])
+
+            if best_podium["value"] > 0:
+                normalized_scores.append(weights["podium"] * summary.get('podium_accuracy', 0) / best_podium["value"])
+
+            if best_tierce_exact["value"] > 0:
+                normalized_scores.append(
+                    weights["tierce_exact"] * summary.get('pmu_bets', {}).get('tierce_exact_rate', 0) /
+                    best_tierce_exact["value"])
+
+            if best_tierce_desordre["value"] > 0:
+                normalized_scores.append(
+                    weights["tierce_desordre"] * summary.get('pmu_bets', {}).get('tierce_desordre_rate', 0) /
+                    best_tierce_desordre["value"])
 
-            return True
-        else:
-            print(f"Error: Unexpected format returned from load_feature_store")
-            return False
-    except Exception as e:
-        print(f"Error in feature store testing: {str(e)}")
-        import traceback
-        traceback.print_exc()
-        return False
-
-
-def test_lstm_data_preparation(orchestrator, embedded_df, args):
-    """Test LSTM sequence data preparation."""
-    print("\n=== Testing LSTM data preparation ===")
-    try:
-        # Prepare sequence data
-        sequence_length = 3  # Use a small value for testing
-        print(f"Preparing sequence data with length={sequence_length}...")
-
-        if not hasattr(orchestrator, 'prepare_sequence_data'):
-            print("Error: No 'prepare_sequence_data' method found")
-            return False
-
-        try:
-            X_sequences, X_static, y = orchestrator.prepare_sequence_data(
-                embedded_df,
-                sequence_length=sequence_length,
-                step_size=1
-            )
+            if best_quarte_exact["value"] > 0:
+                normalized_scores.append(
+                    weights["quarte_exact"] * summary.get('pmu_bets', {}).get('quarte_exact_rate', 0) /
+                    best_quarte_exact["value"])
+
+            # Calculate score as sum of normalized values
+            if normalized_scores:
+                scores[blend_value] = sum(normalized_scores)
+            else:
+                scores[blend_value] = 0
 
-            print(f"Successfully prepared sequence data:")
-            print(f"  - Sequences shape: {X_sequences.shape}")
-            print(f"  - Static features shape: {X_static.shape}")
-            print(f"  - Targets shape: {y.shape}")
-
-            # Split the data (simple random split for testing)
-            from sklearn.model_selection import train_test_split
-
-            print("Splitting sequence data...")
-            # Split data
-            X_seq_train, X_seq_test, X_static_train, X_static_test, y_train, y_test = train_test_split(
-                X_sequences, X_static, y, test_size=0.2, random_state=42
-            )
-
-            # Further split training data into train and validation
-            X_seq_train, X_seq_val, X_static_train, X_static_val, y_train, y_val = train_test_split(
-                X_seq_train, X_static_train, y_train, test_size=0.2, random_state=42
-            )
-
-            print(f"Split sequence data:")
-            print(f"  - Train: {X_seq_train.shape[0]} sequences")
-            print(f"  - Validation: {X_seq_val.shape[0]} sequences")
-            print(f"  - Test: {X_seq_test.shape[0]} sequences")
-
-            # Save LSTM feature store
-            print("\n=== Testing LSTM feature store saving ===")
-            import numpy as np
-            from datetime import datetime
-            import json
-
-            # Create output directory
-            output_dir = os.path.join(orchestrator.feature_store_dir, 'lstm')
-            os.makedirs(output_dir, exist_ok=True)
-
-            # Create timestamp for directory
-            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
-            feature_store_dir = os.path.join(output_dir, f"{args.db}_test_lstm_feature_store_{timestamp}")
-            os.makedirs(feature_store_dir, exist_ok=True)
-
-            print(f"Saving LSTM data to {feature_store_dir}...")
-
-            # Save numpy arrays
-            np.save(os.path.join(feature_store_dir, "X_seq_train.npy"), X_seq_train)
-            np.save(os.path.join(feature_store_dir, "X_seq_val.npy"), X_seq_val)
-            np.save(os.path.join(feature_store_dir, "X_seq_test.npy"), X_seq_test)
-
-            np.save(os.path.join(feature_store_dir, "X_static_train.npy"), X_static_train)
-            np.save(os.path.join(feature_store_dir, "X_static_val.npy"), X_static_val)
-            np.save(os.path.join(feature_store_dir, "X_static_test.npy"), X_static_test)
-
-            np.save(os.path.join(feature_store_dir, "y_train.npy"), y_train)
-            np.save(os.path.join(feature_store_dir, "y_val.npy"), y_val)
-            np.save(os.path.join(feature_store_dir, "y_test.npy"), y_test)
-
-            # Save metadata
-            metadata = {
-                'created_at': datetime.now().isoformat(),
-                'sequence_length': sequence_length,
-                'step_size': 1,
-                'embedding_dim': orchestrator.embedding_dim,
-                'dataset_info': {
-                    'train_samples': len(X_seq_train),
-                    'val_samples': len(X_seq_val),
-                    'test_samples': len(X_seq_test),
-                    'sequence_shape': list(X_seq_train.shape),
-                    'static_shape': list(X_static_train.shape),
-                },
-                'feature_columns': orchestrator.preprocessing_params.get('feature_columns', []),
-            }
-
-            with open(os.path.join(feature_store_dir, "metadata.json"), 'w') as f:
-                json.dump(metadata, f, indent=2, default=str)
-
-            print(f"Successfully saved LSTM feature store to {feature_store_dir}")
-
-            # Test loading LSTM feature store if the method exists
-            if hasattr(orchestrator, 'load_lstm_feature_store'):
-                print("\n=== Testing LSTM feature store loading ===")
-                try:
-                    result = orchestrator.load_lstm_feature_store(feature_store_path=feature_store_dir)
-
-                    if len(result) >= 9:  # We expect at least 9 elements
-                        loaded_X_seq_train = result[0]
-                        loaded_X_static_train = result[3]
-                        loaded_y_train = result[6]
-
-                        print(f"Successfully loaded LSTM feature store")
-                        print(f"Loaded data shapes match original:")
-                        print(f"  - X_sequences: {loaded_X_seq_train.shape == X_seq_train.shape}")
-                        print(f"  - X_static: {loaded_X_static_train.shape == X_static_train.shape}")
-                        print(f"  - y: {loaded_y_train.shape == y_train.shape}")
-                    else:
-                        print(f"Unexpected format returned from load_lstm_feature_store")
+        # Find best overall blend value
+        if scores:
+            best_blend = max(scores.items(), key=lambda x: x[1])
+        else:
+            best_blend = (self.blending_values[0], 0)  # Default to first value if no scores
+
+        comparison["recommendation"] = {
+            "blend_value": best_blend[0],
+            "score": best_blend[1],
+            "scores": scores
+        }
+
+        # Save comparison to file
+        with open(self.output_dir / "comparison.json", 'w') as f:
+            json.dump(comparison, f, indent=2)
+
+        print(f"Comparison generated and saved to {self.output_dir / 'comparison.json'}")
+        self.logger.info(f"Comparison generated and saved to {self.output_dir / 'comparison.json'}")
+
+        # Print recommendation
+        print("\n" + "=" * 50)
+        print("BLENDING COMPARISON RESULTS:")
+        print(f"Best winner accuracy:     {best_winner['value']:.4f} (blend={best_winner['blend']})")
+        print(f"Best podium accuracy:     {best_podium['value']:.4f} (blend={best_podium['blend']})")
+        print(f"Best Tierc (exact):      {best_tierce_exact['value']:.4f} (blend={best_tierce_exact['blend']})")
+        print(f"Best Tierc (dsordre):   {best_tierce_desordre['value']:.4f} (blend={best_tierce_desordre['blend']})")
+        print(f"Best Quart (exact):      {best_quarte_exact['value']:.4f} (blend={best_quarte_exact['blend']})")
+        print("\nRECOMMENDED BLENDING VALUE:")
+        print(f"   {best_blend[0]:.2f} (weighted score: {best_blend[1]:.4f})")
+        print("=" * 50)
+
+        self.logger.info("\n" + "=" * 50)
+        self.logger.info("BLENDING COMPARISON RESULTS:")
+        self.logger.info(f"Best winner accuracy:     {best_winner['value']:.4f} (blend={best_winner['blend']})")
+        self.logger.info(f"Best podium accuracy:     {best_podium['value']:.4f} (blend={best_podium['blend']})")
+        self.logger.info(
+            f"Best Tierc (exact):      {best_tierce_exact['value']:.4f} (blend={best_tierce_exact['blend']})")
+        self.logger.info(
+            f"Best Tierc (dsordre):   {best_tierce_desordre['value']:.4f} (blend={best_tierce_desordre['blend']})")
+        self.logger.info(
+            f"Best Quart (exact):      {best_quarte_exact['value']:.4f} (blend={best_quarte_exact['blend']})")
+        self.logger.info("\nRECOMMENDED BLENDING VALUE:")
+        self.logger.info(f"   {best_blend[0]:.2f} (weighted score: {best_blend[1]:.4f})")
+        self.logger.info("=" * 50)
+
+        return comparison
+
+    def _save_results(self):
+        """Save current results to file."""
+        results_file = self.output_dir / "results.json"
+
+        with open(results_file, 'w') as f:
+            json.dump(self.results, f, indent=2, default=str)
+
+        print(f"Results saved to {results_file}")
+        self.logger.info(f"Results saved to {results_file}")
+
+    def _visualize_results(self):
+        """Create visualizations of the results."""
+        visualizations_dir = self.output_dir / "visualizations"
+        visualizations_dir.mkdir(exist_ok=True)
+
+        # Extract data for plotting
+        blend_values = self.blending_values
+        winner_accuracy = []
+        podium_accuracy = []
+        tierce_exact_rate = []
+        tierce_desordre_rate = []
+
+        for blend_value in blend_values:
+            blend_str = str(blend_value)
+            if blend_str in self.results:
+                summary = self.results[blend_str].get('summary', {})
+                winner_accuracy.append(summary.get('winner_accuracy', 0))
+                podium_accuracy.append(summary.get('podium_accuracy', 0))
+                tierce_exact_rate.append(summary.get('pmu_bets', {}).get('tierce_exact_rate', 0))
+                tierce_desordre_rate.append(summary.get('pmu_bets', {}).get('tierce_desordre_rate', 0))
+
+        # Plot 1: Winner and Podium Accuracy
+        plt.figure(figsize=(12, 6))
+        plt.plot(blend_values, winner_accuracy, 'o-', label='Winner Accuracy')
+        plt.plot(blend_values, podium_accuracy, 's-', label='Podium Accuracy')
+        plt.xlabel('RF Model Weight')
+        plt.ylabel('Accuracy')
+        plt.title('Winner and Podium Accuracy vs. RF Weight')
+        plt.grid(True, alpha=0.3)
+        plt.legend()
+        plt.xticks(blend_values)
+
+        # Annotate values
+        for i, val in enumerate(winner_accuracy):
+            plt.annotate(f"{val:.3f}", (blend_values[i], val), xytext=(0, 10),
+                         textcoords='offset points', ha='center')
+        for i, val in enumerate(podium_accuracy):
+            plt.annotate(f"{val:.3f}", (blend_values[i], val), xytext=(0, -15),
+                         textcoords='offset points', ha='center')
+
+        plt.savefig(visualizations_dir / "accuracy_plot.png", dpi=300, bbox_inches='tight')
+        plt.close()
 
-                except Exception as e:
-                    print(f"Error loading LSTM feature store: {str(e)}")
+        # Plot 2: Tierc Rates
+        plt.figure(figsize=(12, 6))
+        plt.plot(blend_values, tierce_exact_rate, 'o-', label='Tierc Exact Rate')
+        plt.plot(blend_values, tierce_desordre_rate, 's-', label='Tierc Dsordre Rate')
+        plt.xlabel('RF Model Weight')
+        plt.ylabel('Rate')
+        plt.title('Tierc Hit Rates vs. RF Weight')
+        plt.grid(True, alpha=0.3)
+        plt.legend()
+        plt.xticks(blend_values)
 
-            print("\nLSTM feature store testing completed successfully!")
-            return True
-        except ValueError as ve:
-            print(f"ValueError in sequence preparation: {str(ve)}")
-            print("This might be normal if there aren't enough sequences for the given horses.")
-            return False
+        # Annotate values
+        for i, val in enumerate(tierce_exact_rate):
+            plt.annotate(f"{val:.3f}", (blend_values[i], val), xytext=(0, 10),
+                         textcoords='offset points', ha='center')
+        for i, val in enumerate(tierce_desordre_rate):
+            plt.annotate(f"{val:.3f}", (blend_values[i], val), xytext=(0, -15),
+                         textcoords='offset points', ha='center')
 
-    except Exception as e:
-        print(f"Unexpected error in LSTM data preparation: {str(e)}")
-        import traceback
-        traceback.print_exc()
-        return False
+        plt.savefig(visualizations_dir / "tierce_rates_plot.png", dpi=300, bbox_inches='tight')
+        plt.close()
 
+        # Plot 3: Combined metrics on radar chart
+        metrics_names = ['Winner Accuracy', 'Podium Accuracy', 'Tierc Exact', 'Tierc Dsordre', 'Quart Exact']
 
-def test_full_pipeline(orchestrator, args):
-    """Test the full pipeline with the orchestrator."""
-    print("\n=== Testing full pipeline ===")
-    try:
-        # Call the existing run_pipeline method if it exists
-        if hasattr(orchestrator, 'run_pipeline'):
-            print("Running full pipeline...")
-            result = orchestrator.run_pipeline(
-                limit=args.limit,
-                race_filter=args.race_type,
-                use_cache=not args.no_cache
-            )
-
-            if isinstance(result, tuple) and len(result) >= 6:  # Ensure we got at least 6 elements (X_train...y_test)
-                X_train, X_val, X_test, y_train, y_val, y_test = result[:6]
-                feature_store_path = result[6] if len(result) > 6 else None
-
-                print(f"\nPipeline results:")
-                print(f"  - Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features")
-                print(f"  - Validation set: {X_val.shape[0]} samples")
-                print(f"  - Test set: {X_test.shape[0]} samples")
-
-                # Print some embedding statistics
-                embedding_cols = {
-                    'horse': [col for col in X_train.columns if col.startswith('horse_emb_')],
-                    'jockey': [col for col in X_train.columns if col.startswith('jockey_emb_')],
-                    'couple': [col for col in X_train.columns if col.startswith('couple_emb_')],
-                    'course': [col for col in X_train.columns if col.startswith('course_emb_')]
-                }
-
-                print("\nEmbedding features:")
-                for entity, cols in embedding_cols.items():
-                    if cols:
-                        print(f"  - {entity.capitalize()}: {len(cols)} dimensions")
-
-                if feature_store_path:
-                    print(f"\nFeature store saved to: {feature_store_path}")
-
-                print("\nFull pipeline test completed successfully!")
-                return True
-            else:
-                print(f"Unexpected format returned from run_pipeline")
-                return False
-        else:
-            # If run_pipeline doesn't exist, run the steps manually
-            print("run_pipeline method not found. Running steps manually...")
-
-            # 1. Load data
-            df = test_data_loading(orchestrator, args)
-            if df is None:
-                return False
-
-            # 2. Preprocess data
-            embedded_df = test_feature_preparation(orchestrator, df, args)
-            if embedded_df is None:
-                return False
-
-            # 3. Save feature store
-            feature_store_result = test_feature_store(orchestrator, embedded_df, args)
-
-            print("\nFull pipeline test completed by running individual steps")
-            return feature_store_result
-
-    except Exception as e:
-        print(f"Error in full pipeline test: {str(e)}")
-        import traceback
-        traceback.print_exc()
-        return False
+        # Prepare data for radar chart
+        radar_data = []
+        for blend_value in blend_values:
+            blend_str = str(blend_value)
+            if blend_str in self.results:
+                summary = self.results[blend_str].get('summary', {})
+                radar_data.append([
+                    summary.get('winner_accuracy', 0),
+                    summary.get('podium_accuracy', 0),
+                    summary.get('pmu_bets', {}).get('tierce_exact_rate', 0),
+                    summary.get('pmu_bets', {}).get('tierce_desordre_rate', 0),
+                    summary.get('pmu_bets', {}).get('quarte_exact_rate', 0)
+                ])
+
+        if radar_data:
+            try:
+                # Create radar chart
+                self._create_radar_chart(
+                    radar_data,
+                    metrics_names,
+                    [f"RF Weight {v}" for v in blend_values],
+                    visualizations_dir / "radar_chart.png"
+                )
+            except Exception as e:
+                print(f"Error creating radar chart: {str(e)}")
+                self.logger.error(f"Error creating radar chart: {str(e)}")
+
+        print(f"Visualizations saved to {visualizations_dir}")
+        self.logger.info(f"Visualizations saved to {visualizations_dir}")
+
+    def _visualize_performance(self, performance_data):
+        """Create visualizations of the performance metrics."""
+        if not performance_data:
+            return
+
+        visualizations_dir = self.output_dir / "visualizations"
+        visualizations_dir.mkdir(exist_ok=True)
+
+        # Group by blend value
+        blend_groups = {}
+        for data in performance_data:
+            blend_value = data['blend_value']
+            if blend_value not in blend_groups:
+                blend_groups[blend_value] = []
+            blend_groups[blend_value].append(data)
+
+        # Calculate average metrics per blend value
+        blend_values = []
+        avg_prediction_times = []
+        avg_reblend_times = []
+
+        for blend_value, data_list in sorted(blend_groups.items()):
+            blend_values.append(blend_value)
+
+            # Prediction times
+            pred_times = [d['avg_prediction_time'] for d in data_list]
+            avg_prediction_times.append(sum(pred_times) / len(pred_times) if pred_times else 0)
+
+            # Reblend times (if available)
+            reblend_times = [d['avg_reblend_time'] for d in data_list if d['avg_reblend_time'] is not None]
+            avg_reblend_times.append(sum(reblend_times) / len(reblend_times) if reblend_times else None)
+
+        # Plot performance comparison
+        plt.figure(figsize=(12, 6))
+
+        # Plot prediction times
+        plt.plot(blend_values, avg_prediction_times, 'o-', label='Total Prediction Time')
+
+        # Plot reblend times if available
+        if any(t is not None for t in avg_reblend_times):
+            # Filter out None values
+            valid_blends = []
+            valid_reblends = []
+            for i, time in enumerate(avg_reblend_times):
+                if time is not None:
+                    valid_blends.append(blend_values[i])
+                    valid_reblends.append(time)
+
+            plt.plot(valid_blends, valid_reblends, 's-', label='Reblend Time')
+
+        plt.xlabel('RF Model Weight')
+        plt.ylabel('Time (seconds)')
+        plt.title('Prediction Performance vs. RF Weight')
+        plt.grid(True, alpha=0.3)
+        plt.legend()
+
+        # Annotate values
+        for i, val in enumerate(avg_prediction_times):
+            plt.annotate(f"{val:.3f}s", (blend_values[i], val), xytext=(0, 10),
+                         textcoords='offset points', ha='center')
+
+        plt.savefig(visualizations_dir / "performance_comparison.png", dpi=300, bbox_inches='tight')
+        plt.close()
+
+        # Calculate speedup compared to first blend value
+        if len(blend_values) > 1:
+            first_time = avg_prediction_times[0]
+            speedup = [first_time / t if t > 0 else 0 for t in avg_prediction_times]
+
+            # Plot speedup
+            plt.figure(figsize=(10, 6))
+            bars = plt.bar(blend_values, speedup)
+
+            # Color the first bar differently
+            bars[0].set_color('gray')
+            for i in range(1, len(bars)):
+                bars[i].set_color('green')
+
+            plt.xlabel('RF Model Weight')
+            plt.ylabel('Speedup (x times)')
+            plt.title('Performance Speedup vs. First Run')
+            plt.grid(True, alpha=0.3)
+
+            # Annotate values
+            for i, val in enumerate(speedup):
+                if i > 0:  # Skip first value (no speedup)
+                    plt.annotate(f"{val:.1f}x", (blend_values[i], val), xytext=(0, 5),
+                                 textcoords='offset points', ha='center')
+
+            plt.savefig(visualizations_dir / "speedup_comparison.png", dpi=300, bbox_inches='tight')
+            plt.close()
+
+    def _create_radar_chart(self, data, labels, legend, output_file):
+        """
+        Create a radar chart for the metrics.
+
+        Args:
+            data: List of data series to plot
+            labels: Labels for each axis
+            legend: Legend entries for each data series
+            output_file: Output file path
+        """
+        # Number of metrics
+        N = len(labels)
+
+        # Create angles for each metric
+        angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
+
+        # Close the polygon
+        angles += angles[:1]
+
+        # Create figure
+        plt.figure(figsize=(10, 10))
+        ax = plt.subplot(111, polar=True)
+
+        # Plot each data series
+        for i, series in enumerate(data):
+            # Close the polygon
+            values = series + [series[0]]
+
+            # Plot values
+            ax.plot(angles, values, 'o-', linewidth=2, label=legend[i])
+            ax.fill(angles, values, alpha=0.1)
+
+        # Set labels
+        ax.set_thetagrids(np.degrees(angles[:-1]), labels)
+
+        # Add legend
+        plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
+
+        plt.title("Metric Comparison Across RF Weights", size=15)
+        plt.savefig(output_file, dpi=300, bbox_inches='tight')
+        plt.close()
+
+    @classmethod
+    def load_results(cls, results_dir: str) -> 'BlendingComparison':
+        """
+        Load results from a previous comparison run.
+
+        Args:
+            results_dir: Directory with saved results
+
+        Returns:
+            BlendingComparison instance with loaded results
+        """
+        results_path = Path(results_dir)
+        results_file = results_path / "results.json"
+
+        if not results_file.exists():
+            raise FileNotFoundError(f"Results file not found: {results_file}")
+
+        # Load results
+        with open(results_file, 'r') as f:
+            results = json.load(f)
+
+        # Create instance
+        comparison = cls(
+            model_path="dummy_path",  # Will be overridden
+            output_dir=str(results_path),
+            blending_values=[float(k) for k in results.keys()]
+        )
+
+        # Set results
+        comparison.results = results
+
+        # Generate comparison and visualizations
+        comparison._generate_comparison()
+        comparison._visualize_results()
+
+        return comparison
 
 
 def main():
-    parser = argparse.ArgumentParser(description="Test the FeatureEmbeddingOrchestrator")
-    parser.add_argument('--limit', type=int, default=10, help='Limit the number of races to process')
-    parser.add_argument('--race-type', type=str, default=None, help='Filter by race type (e.g., "A" for Attele)')
-    parser.add_argument('--run-full', action='store_true', help='Run the full pipeline')
-    parser.add_argument('--test-load', action='store_true', help='Test only feature store loading')
-    parser.add_argument('--test-lstm', action='store_true', help='Test LSTM data preparation')
-    parser.add_argument('--no-cache', action='store_true', help='Disable use of cache')
-    parser.add_argument('--clear-cache', action='store_true', help='Clear cache before running')
-    parser.add_argument('--db', type=str, default="dev", help='Database to use from config')
-    parser.add_argument('--embedding-dim', type=int, default=8, help='Embedding dimension to use')
-    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')
+    parser = argparse.ArgumentParser(description="Compare race prediction results with different blending values")
+    parser.add_argument("--model", type=str, required=True, help="Path to the model or model name")
+    parser.add_argument("--db", type=str, help="Database name from config (defaults to active_db)")
+    parser.add_argument("--output", type=str, help="Output directory for results")
+    parser.add_argument("--blend-values", type=str, help="Comma-separated list of blending values to test")
+    parser.add_argument("--start-date", type=str, help="Start date for evaluation (YYYY-MM-DD)")
+    parser.add_argument("--days", type=int, default=3, help="Number of days to evaluate")
+    parser.add_argument("--load", type=str, help="Load and analyze results from specified directory")
+    parser.add_argument("--verbose", action="store_true", help="Enable verbose output")
+    parser.add_argument("--limit", type=int, help="Limit number of races to process (for debugging)")
+    parser.add_argument("--optimize", action="store_true", help="Use optimized blending with caching (default: true)")
+
     args = parser.parse_args()
 
-    # Initialize orchestrator with specified database
-    config = AppConfig()
-    sqlite_path = config.get_sqlite_dbpath(args.db)
-
-    # Create orchestrator with only arguments we know exist
-    try:
-        # Try with all parameters first
-        orchestrator = FeatureEmbeddingOrchestrator(
-            sqlite_path=sqlite_path,
-            embedding_dim=args.embedding_dim,
-            verbose=args.verbose
-        )
-    except TypeError:
-        # If verbose is not supported, try without it
-        print("Note: 'verbose' parameter not supported, creating orchestrator without it.")
-        orchestrator = FeatureEmbeddingOrchestrator(
-            sqlite_path=sqlite_path,
-            embedding_dim=args.embedding_dim
-        )
-
-    print(f"Testing orchestrator with database: {args.db} ({sqlite_path})")
-    print(f"Using embedding dimension: {args.embedding_dim}")
+    if args.load:
+        # Load previous results
+        try:
+            comparison = BlendingComparison.load_results(args.load)
+            print(f"Loaded and analyzed results from {args.load}")
+            return 0
+        except Exception as e:
+            print(f"Error loading results: {str(e)}")
+            return 1
 
-    # Clear cache if requested
-    if args.clear_cache and hasattr(orchestrator, 'cache_manager') and hasattr(orchestrator.cache_manager,
-                                                                               'clear_cache'):
-        print("Clearing cache...")
-        orchestrator.cache_manager.clear_cache()
-
-    # Test only loading if requested
-    if args.test_load:
-        print("\n=== Testing feature store loading only ===")
-        try:
-            # Check for list_feature_stores method
-            if hasattr(orchestrator, '_list_feature_stores'):
-                feature_stores = orchestrator._list_feature_stores()
-            else:
-                # Try direct path if method doesn't exist
-                feature_store_dir = orchestrator.feature_store_dir
-                if os.path.exists(feature_store_dir):
-                    feature_stores = [d for d in os.listdir(feature_store_dir)
-                                      if os.path.isdir(os.path.join(feature_store_dir, d))
-                                      and d.startswith('feature_store_')]
-                    feature_stores.sort(reverse=True)  # Most recent first
-                else:
-                    feature_stores = []
-
-            if not feature_stores:
-                print("No feature stores found. Run the test first to create a feature store.")
-                return
+    # Parse blend values
+    if args.blend_values:
+        blend_values = [float(v) for v in args.blend_values.split(",")]
+    else:
+        blend_values = [0.0, 0.25, 0.5, 0.7, 0.85, 1.0]  # Default values
 
-            # Load the most recent feature store
-            feature_store_path = os.path.join(orchestrator.feature_store_dir, feature_stores[0])
-            print(f"Loading most recent feature store: {feature_store_path}")
+    # Create dates list
+    if args.start_date:
+        start_date = args.start_date
+        # Parse date to check format
+        try:
+            datetime.strptime(start_date, "%Y-%m-%d")
+        except ValueError:
+            print(f"Invalid date format: {start_date}. Use YYYY-MM-DD format.")
+            return 1
 
-            if hasattr(orchestrator, 'load_feature_store'):
-                result = orchestrator.load_feature_store(feature_store_path=feature_store_path)
-                if len(result) >= 6:
-                    X_train, X_val, X_test, y_train, y_val, y_test = result[:6]
-
-                    print(f"Successfully loaded feature store")
-                    print(f"Loaded data shapes:")
-                    print(f"  - X_train: {X_train.shape}")
-                    print(f"  - y_train: {y_train.shape}")
-
-                    print("\nFeature store loading test completed successfully!")
-                else:
-                    print(f"Unexpected format returned from load_feature_store")
-            else:
-                print("Error: No 'load_feature_store' method found")
-            return
-        except Exception as e:
-            print(f"Error loading feature store: {str(e)}")
-            import traceback
-            traceback.print_exc()
-            return
+        # Generate dates
+        dates = []
+        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
+        for i in range(args.days):
+            date_str = (start_dt + timedelta(days=i)).strftime("%Y-%m-%d")
+            dates.append(date_str)
+    else:
+        dates = None
+
+    # Create comparison object
+    comparison = BlendingComparison(
+        model_path=args.model,
+        db_name=args.db,
+        output_dir=args.output,
+        blending_values=blend_values,
+        verbose=args.verbose
+    )
+
+    # Run evaluation
+    if dates:
+        result = comparison.evaluate_blending_values(dates=dates, limit=args.limit)
+    else:
+        result = comparison.evaluate_blending_values(num_days=args.days, limit=args.limit)
 
-    # If running full pipeline test
-    if args.run_full:
-        test_full_pipeline(orchestrator, args)
-        return
+    print("\nBlending value comparison completed!")
+    print(f"Results saved to: {comparison.output_dir}")
+    print("\nRecommended blending value: " +
+          f"{result['recommendation']['blend_value']:.2f} (score: {result['recommendation']['score']:.4f})")
 
-    # Otherwise run step by step tests
+    # Print best values for each metric
+    best = result.get('best', {})
+    print("\nBest values by metric:")
+    metrics = [
+        ("Winner Accuracy", best.get('winner', {})),
+        ("Podium Accuracy", best.get('podium', {})),
+        ("Tierc Exact", best.get('tierce_exact', {})),
+        ("Tierc Dsordre", best.get('tierce_desordre', {})),
+        ("Quart Exact", best.get('quarte_exact', {}))
+    ]
+    for metric_name, metric_data in metrics:
+        value = metric_data.get('value', 0)
+        blend = metric_data.get('blend')
+        print(f"  - {metric_name}: {value:.4f} (blend={blend})")
 
-    # Test data loading
-    df = test_data_loading(orchestrator, args)
-    if df is None:
-        print("Data loading failed, stopping tests.")
-        return
+    # Print performance improvement if available
+    if 'performance' in result:
+        perf_data = result['performance']
+        first_blend_metrics = [d for d in perf_data if d.get('first_blend', False)]
+        other_blend_metrics = [d for d in perf_data if not d.get('first_blend', False)]
 
-    # Test feature preparation
-    embedded_df = test_feature_preparation(orchestrator, df, args)
-    if embedded_df is None:
-        print("Feature preparation failed, stopping tests.")
-        return
+        if first_blend_metrics and other_blend_metrics:
+            avg_first_time = sum(d['avg_prediction_time'] for d in first_blend_metrics) / len(first_blend_metrics)
+            avg_other_time = sum(d['avg_prediction_time'] for d in other_blend_metrics) / len(other_blend_metrics)
 
-    # Test feature store functionality
-    feature_store_result = test_feature_store(orchestrator, embedded_df, args)
+            if avg_other_time > 0:
+                overall_speedup = avg_first_time / avg_other_time
+                print(f"\nPerformance improvement: {overall_speedup:.2f}x faster with caching")
+                print(f"Average prediction time (first run): {avg_first_time:.3f} seconds")
+                print(f"Average prediction time (with cache): {avg_other_time:.3f} seconds")
 
-    # Test LSTM data preparation if requested
-    if args.test_lstm:
-        test_lstm_data_preparation(orchestrator, embedded_df, args)
+    return 0
 
-    print("\nAll tests completed!")
 
-
-if __name__ == '__main__':
-    main()
\ No newline at end of file
+if __name__ == "__main__":
+    sys.exit(main())
\ No newline at end of file
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"AutoImportSettings\">\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\n  </component>\n  <component name=\"ChangeListManager\">\n    <list default=\"true\" id=\"43ec0894-de26-497b-a8fc-a968059a9170\" name=\"Changes\" comment=\"ModelManager\">\n      <change beforePath=\"$PROJECT_DIR$/.idea/workspace.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/workspace.xml\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/config.yaml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/config.yaml\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/core/connectors/api_daily_sync.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/core/connectors/api_daily_sync.py\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/core/orchestrators/prediction_orchestrator.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/core/orchestrators/prediction_orchestrator.py\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/model_training/historical/train_race_model.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/model_training/historical/train_race_model.py\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/model_training/regressions/regression_enhancement.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/model_training/regressions/regression_enhancement.py\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/race_prediction/predict_orchestrator_cli.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/race_prediction/predict_orchestrator_cli.py\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/race_prediction/race_predict.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/race_prediction/race_predict.py\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/utils/model_manager.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/utils/model_manager.py\" afterDir=\"false\" />\n    </list>\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\n  </component>\n  <component name=\"FileTemplateManagerImpl\">\n    <option name=\"RECENT_TEMPLATES\">\n      <list>\n        <option value=\"Python Script\" />\n      </list>\n    </option>\n  </component>\n  <component name=\"Git.Settings\">\n    <option name=\"RECENT_BRANCH_BY_REPOSITORY\">\n      <map>\n        <entry key=\"$PROJECT_DIR$\" value=\"LightGBM\" />\n      </map>\n    </option>\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\n  </component>\n  <component name=\"GitHubPullRequestSearchHistory\">{\n  &quot;lastFilter&quot;: {\n    &quot;state&quot;: &quot;OPEN&quot;,\n    &quot;assignee&quot;: &quot;Mattg0&quot;\n  }\n}</component>\n  <component name=\"GithubPullRequestsUISettings\">{\n  &quot;selectedUrlAndAccountId&quot;: {\n    &quot;url&quot;: &quot;https://github.com/Mattg0/HorseAI.git&quot;,\n    &quot;accountId&quot;: &quot;aee67ed6-2c23-4be9-a632-2459e76288ac&quot;\n  }\n}</component>\n  <component name=\"ProjectColorInfo\">{\n  &quot;associatedIndex&quot;: 4\n}</component>\n  <component name=\"ProjectId\" id=\"2tOCkYF0cZhZtRPomssp0yLhNGU\" />\n  <component name=\"ProjectLevelVcsManager\" settingsEditedManually=\"true\" />\n  <component name=\"ProjectViewState\">\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\n    <option name=\"showLibraryContents\" value=\"true\" />\n  </component>\n  <component name=\"PropertiesComponent\"><![CDATA[{\n  \"keyToString\": {\n    \"Python.api_daily_sync.executor\": \"Run\",\n    \"Python.embedding_feature.executor\": \"Run\",\n    \"Python.env_setup.executor\": \"Run\",\n    \"Python.horse_embedding.executor\": \"Run\",\n    \"Python.musique_calculation.executor\": \"Debug\",\n    \"Python.mysql_connector.executor\": \"Run\",\n    \"Python.mysql_sqlite_sync.executor\": \"Run\",\n    \"Python.mysql_to_sqlite.executor\": \"Run\",\n    \"Python.predict_daily_races.executor\": \"Run\",\n    \"Python.prediction_orchestrator.executor\": \"Run\",\n    \"Python.regression_enhancement.executor\": \"Run\",\n    \"Python.test.executor\": \"Debug\",\n    \"Python.train_model.executor\": \"Debug\",\n    \"Python.train_race_model.executor\": \"Run\",\n    \"RunOnceActivity.ShowReadmeOnStart\": \"true\",\n    \"RunOnceActivity.git.unshallow\": \"true\",\n    \"git-widget-placeholder\": \"main\"\n  }\n}]]></component>\n  <component name=\"RecentsManager\">\n    <key name=\"MoveFile.RECENT_KEYS\">\n      <recent name=\"$PROJECT_DIR$/core/connectors\" />\n    </key>\n  </component>\n  <component name=\"RunManager\" selected=\"Python.train_race_model\">\n    <configuration name=\"api_daily_sync\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"HorseAIv2\" />\n      <option name=\"ENV_FILES\" value=\"\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/core/connectors/api_daily_sync.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"predict_daily_races\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"HorseAIv2\" />\n      <option name=\"ENV_FILES\" value=\"\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/race_prediction/predict_daily_races.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"prediction_orchestrator\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"HorseAIv2\" />\n      <option name=\"ENV_FILES\" value=\"\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/core/orchestrators\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/core/orchestrators/prediction_orchestrator.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"regression_enhancement\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"HorseAIv2\" />\n      <option name=\"ENV_FILES\" value=\"\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/model_training/regressions/regression_enhancement.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"train_race_model\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"HorseAIv2\" />\n      <option name=\"ENV_FILES\" value=\"\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/model_training/historical/train_race_model.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <list>\n      <item itemvalue=\"Python.train_race_model\" />\n      <item itemvalue=\"Python.regression_enhancement\" />\n      <item itemvalue=\"Python.predict_daily_races\" />\n      <item itemvalue=\"Python.prediction_orchestrator\" />\n      <item itemvalue=\"Python.api_daily_sync\" />\n    </list>\n    <recent_temporary>\n      <list>\n        <item itemvalue=\"Python.train_race_model\" />\n        <item itemvalue=\"Python.regression_enhancement\" />\n        <item itemvalue=\"Python.api_daily_sync\" />\n        <item itemvalue=\"Python.prediction_orchestrator\" />\n        <item itemvalue=\"Python.predict_daily_races\" />\n      </list>\n    </recent_temporary>\n  </component>\n  <component name=\"SharedIndexes\">\n    <attachedChunks>\n      <set>\n        <option value=\"bundled-python-sdk-fb887030ada0-aa17d162503b-com.jetbrains.pycharm.community.sharedIndexes.bundled-PC-243.21565.199\" />\n      </set>\n    </attachedChunks>\n  </component>\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\n  <component name=\"TaskManager\">\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\n      <changelist id=\"43ec0894-de26-497b-a8fc-a968059a9170\" name=\"Changes\" comment=\"\" />\n      <created>1740213877566</created>\n      <option name=\"number\" value=\"Default\" />\n      <option name=\"presentableId\" value=\"Default\" />\n      <updated>1740213877566</updated>\n    </task>\n    <task id=\"LOCAL-00001\" summary=\"Project Initialisiation\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740556622361</created>\n      <option name=\"number\" value=\"00001\" />\n      <option name=\"presentableId\" value=\"LOCAL-00001\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740556622361</updated>\n    </task>\n    <task id=\"LOCAL-00002\" summary=\"Working MySQL connector!\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740584410696</created>\n      <option name=\"number\" value=\"00002\" />\n      <option name=\"presentableId\" value=\"LOCAL-00002\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740584410696</updated>\n    </task>\n    <task id=\"LOCAL-00003\" summary=\"modular mysql&lt;-&gt;sqlite sync\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740650477583</created>\n      <option name=\"number\" value=\"00003\" />\n      <option name=\"presentableId\" value=\"LOCAL-00003\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740650477583</updated>\n    </task>\n    <task id=\"LOCAL-00004\" summary=\"adding embeddings\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740659886795</created>\n      <option name=\"number\" value=\"00004\" />\n      <option name=\"presentableId\" value=\"LOCAL-00004\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740659886795</updated>\n    </task>\n    <task id=\"LOCAL-00005\" summary=\"adding cache manager\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740693327554</created>\n      <option name=\"number\" value=\"00005\" />\n      <option name=\"presentableId\" value=\"LOCAL-00005\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740693327554</updated>\n    </task>\n    <task id=\"LOCAL-00006\" summary=\"updating embedding feature with config approach\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740755058414</created>\n      <option name=\"number\" value=\"00006\" />\n      <option name=\"presentableId\" value=\"LOCAL-00006\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740755058414</updated>\n    </task>\n    <task id=\"LOCAL-00007\" summary=\"updating embedding feature with config approach\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1740920224165</created>\n      <option name=\"number\" value=\"00007\" />\n      <option name=\"presentableId\" value=\"LOCAL-00007\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1740920224165</updated>\n    </task>\n    <task id=\"LOCAL-00008\" summary=\"updating embedding feature with config approach\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1741207626289</created>\n      <option name=\"number\" value=\"00008\" />\n      <option name=\"presentableId\" value=\"LOCAL-00008\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1741207626289</updated>\n    </task>\n    <task id=\"LOCAL-00009\" summary=\"refactored_horse_embedding\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1741207970875</created>\n      <option name=\"number\" value=\"00009\" />\n      <option name=\"presentableId\" value=\"LOCAL-00009\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1741207970875</updated>\n    </task>\n    <task id=\"LOCAL-00010\" summary=\"refactored_couple_embedding\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1741208877518</created>\n      <option name=\"number\" value=\"00010\" />\n      <option name=\"presentableId\" value=\"LOCAL-00010\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1741208877518</updated>\n    </task>\n    <task id=\"LOCAL-00011\" summary=\"refactored_cache_manager\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742135265333</created>\n      <option name=\"number\" value=\"00011\" />\n      <option name=\"presentableId\" value=\"LOCAL-00011\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742135265333</updated>\n    </task>\n    <task id=\"LOCAL-00012\" summary=\"fixed cache_manager &amp; orchestrator\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742200198713</created>\n      <option name=\"number\" value=\"00012\" />\n      <option name=\"presentableId\" value=\"LOCAL-00012\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742200198713</updated>\n    </task>\n    <task id=\"LOCAL-00013\" summary=\"fixing feature storing and loading with testing\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742201779545</created>\n      <option name=\"number\" value=\"00013\" />\n      <option name=\"presentableId\" value=\"LOCAL-00013\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742201779545</updated>\n    </task>\n    <task id=\"LOCAL-00014\" summary=\"fixed feature embedding\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742226900595</created>\n      <option name=\"number\" value=\"00014\" />\n      <option name=\"presentableId\" value=\"LOCAL-00014\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742226900595</updated>\n    </task>\n    <task id=\"LOCAL-00015\" summary=\"fixed feature embedding\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742541186315</created>\n      <option name=\"number\" value=\"00015\" />\n      <option name=\"presentableId\" value=\"LOCAL-00015\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742541186315</updated>\n    </task>\n    <task id=\"LOCAL-00016\" summary=\"fixed feature embedding\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742549204949</created>\n      <option name=\"number\" value=\"00016\" />\n      <option name=\"presentableId\" value=\"LOCAL-00016\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742549204949</updated>\n    </task>\n    <task id=\"LOCAL-00017\" summary=\"fixed feature embedding\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742825115896</created>\n      <option name=\"number\" value=\"00017\" />\n      <option name=\"presentableId\" value=\"LOCAL-00017\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742825115896</updated>\n    </task>\n    <task id=\"LOCAL-00018\" summary=\"adding daily races sync\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1742903024757</created>\n      <option name=\"number\" value=\"00018\" />\n      <option name=\"presentableId\" value=\"LOCAL-00018\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1742903024757</updated>\n    </task>\n    <task id=\"LOCAL-00019\" summary=\"adding daily races sync\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1743020439512</created>\n      <option name=\"number\" value=\"00019\" />\n      <option name=\"presentableId\" value=\"LOCAL-00019\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1743020439512</updated>\n    </task>\n    <task id=\"LOCAL-00020\" summary=\"adding daily races sync\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1743421163636</created>\n      <option name=\"number\" value=\"00020\" />\n      <option name=\"presentableId\" value=\"LOCAL-00020\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1743421163636</updated>\n    </task>\n    <task id=\"LOCAL-00021\" summary=\"ModelManager\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1744094884798</created>\n      <option name=\"number\" value=\"00021\" />\n      <option name=\"presentableId\" value=\"LOCAL-00021\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1744094884798</updated>\n    </task>\n    <task id=\"LOCAL-00022\" summary=\"ModelManager\">\n      <option name=\"closed\" value=\"true\" />\n      <created>1744176790455</created>\n      <option name=\"number\" value=\"00022\" />\n      <option name=\"presentableId\" value=\"LOCAL-00022\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1744176790455</updated>\n    </task>\n    <option name=\"localTasksCounter\" value=\"23\" />\n    <servers />\n  </component>\n  <component name=\"VcsManagerConfiguration\">\n    <MESSAGE value=\"Project Initialisiation\" />\n    <MESSAGE value=\"Working MySQL connector!\" />\n    <MESSAGE value=\"modular mysql&lt;-&gt;sqlite sync\" />\n    <MESSAGE value=\"adding embeddings\" />\n    <MESSAGE value=\"adding cache manager\" />\n    <MESSAGE value=\"updating embedding feature with config approach\" />\n    <MESSAGE value=\"refactored_horse_embedding\" />\n    <MESSAGE value=\"refactored_couple_embedding\" />\n    <MESSAGE value=\"refactored_cache_manager\" />\n    <MESSAGE value=\"fixed cache_manager &amp; orchestrator\" />\n    <MESSAGE value=\"fixing feature storing and loading with testing\" />\n    <MESSAGE value=\"fixed feature embedding\" />\n    <MESSAGE value=\"adding daily races sync\" />\n    <MESSAGE value=\"ModelManager\" />\n    <option name=\"LAST_COMMIT_MESSAGE\" value=\"ModelManager\" />\n  </component>\n  <component name=\"XDebuggerManager\">\n    <breakpoint-manager>\n      <breakpoints>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/calculators/static_feature_calculator.py</url>\n          <line>55</line>\n          <option name=\"timeStamp\" value=\"27\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/calculators/static_feature_calculator.py</url>\n          <line>56</line>\n          <option name=\"timeStamp\" value=\"28\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/calculators/static_feature_calculator.py</url>\n          <line>57</line>\n          <option name=\"timeStamp\" value=\"29\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/calculators/static_feature_calculator.py</url>\n          <line>169</line>\n          <option name=\"timeStamp\" value=\"30\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/transformers/historical_race_transformer.py</url>\n          <line>84</line>\n          <option name=\"timeStamp\" value=\"55\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/transformers/daily_race_transformer.py</url>\n          <line>315</line>\n          <option name=\"timeStamp\" value=\"97\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/race_prediction/predict_daily_races.py</url>\n          <line>127</line>\n          <option name=\"timeStamp\" value=\"98\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/core/orchestrators/embedding_feature.py</url>\n          <line>1595</line>\n          <option name=\"timeStamp\" value=\"114\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/model_training/historical/train_race_model.py</url>\n          <line>966</line>\n          <option name=\"timeStamp\" value=\"116\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/model_training/historical/train_race_model.py</url>\n          <line>450</line>\n          <option name=\"timeStamp\" value=\"126\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/model_training/historical/train_race_model.py</url>\n          <line>457</line>\n          <option name=\"timeStamp\" value=\"127\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/model_training/historical/train_race_model.py</url>\n          <line>465</line>\n          <option name=\"timeStamp\" value=\"128\" />\n        </line-breakpoint>\n      </breakpoints>\n      <default-breakpoints>\n        <breakpoint type=\"python-exception\">\n          <properties notifyOnTerminate=\"true\" exception=\"BaseException\">\n            <option name=\"notifyOnTerminate\" value=\"true\" />\n          </properties>\n        </breakpoint>\n      </default-breakpoints>\n    </breakpoint-manager>\n    <watches-manager>\n      <configuration name=\"PythonConfigurationType\">\n        <watch expression=\"race_type['course_info']\" language=\"Python\" />\n        <watch expression=\"musique_stats.race_types.__len__()\" />\n        <watch expression=\"musique_stats.__len__()\" />\n      </configuration>\n    </watches-manager>\n  </component>\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision 0d88e73ffd4a3899fde119bb4ed28c1bf49fab9b)
+++ b/.idea/workspace.xml	(date 1746187353071)
@@ -6,14 +6,10 @@
   <component name="ChangeListManager">
     <list default="true" id="43ec0894-de26-497b-a8fc-a968059a9170" name="Changes" comment="ModelManager">
       <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/config.yaml" beforeDir="false" afterPath="$PROJECT_DIR$/config.yaml" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/core/connectors/api_daily_sync.py" beforeDir="false" afterPath="$PROJECT_DIR$/core/connectors/api_daily_sync.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/core/orchestrators/prediction_orchestrator.py" beforeDir="false" afterPath="$PROJECT_DIR$/core/orchestrators/prediction_orchestrator.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/model_training/historical/train_race_model.py" beforeDir="false" afterPath="$PROJECT_DIR$/model_training/historical/train_race_model.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/model_training/regressions/regression_enhancement.py" beforeDir="false" afterPath="$PROJECT_DIR$/model_training/regressions/regression_enhancement.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/race_prediction/predict_orchestrator_cli.py" beforeDir="false" afterPath="$PROJECT_DIR$/race_prediction/predict_orchestrator_cli.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/race_prediction/race_predict.py" beforeDir="false" afterPath="$PROJECT_DIR$/race_prediction/race_predict.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/utils/model_manager.py" beforeDir="false" afterPath="$PROJECT_DIR$/utils/model_manager.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/test.py" beforeDir="false" afterPath="$PROJECT_DIR$/test.py" afterDir="false" />
     </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
@@ -56,27 +52,27 @@
     <option name="hideEmptyMiddlePackages" value="true" />
     <option name="showLibraryContents" value="true" />
   </component>
-  <component name="PropertiesComponent"><![CDATA[{
-  "keyToString": {
-    "Python.api_daily_sync.executor": "Run",
-    "Python.embedding_feature.executor": "Run",
-    "Python.env_setup.executor": "Run",
-    "Python.horse_embedding.executor": "Run",
-    "Python.musique_calculation.executor": "Debug",
-    "Python.mysql_connector.executor": "Run",
-    "Python.mysql_sqlite_sync.executor": "Run",
-    "Python.mysql_to_sqlite.executor": "Run",
-    "Python.predict_daily_races.executor": "Run",
-    "Python.prediction_orchestrator.executor": "Run",
-    "Python.regression_enhancement.executor": "Run",
-    "Python.test.executor": "Debug",
-    "Python.train_model.executor": "Debug",
-    "Python.train_race_model.executor": "Run",
-    "RunOnceActivity.ShowReadmeOnStart": "true",
-    "RunOnceActivity.git.unshallow": "true",
-    "git-widget-placeholder": "main"
+  <component name="PropertiesComponent">{
+  &quot;keyToString&quot;: {
+    &quot;Python.api_daily_sync.executor&quot;: &quot;Run&quot;,
+    &quot;Python.embedding_feature.executor&quot;: &quot;Run&quot;,
+    &quot;Python.env_setup.executor&quot;: &quot;Run&quot;,
+    &quot;Python.horse_embedding.executor&quot;: &quot;Run&quot;,
+    &quot;Python.musique_calculation.executor&quot;: &quot;Debug&quot;,
+    &quot;Python.mysql_connector.executor&quot;: &quot;Run&quot;,
+    &quot;Python.mysql_sqlite_sync.executor&quot;: &quot;Run&quot;,
+    &quot;Python.mysql_to_sqlite.executor&quot;: &quot;Run&quot;,
+    &quot;Python.predict_daily_races.executor&quot;: &quot;Run&quot;,
+    &quot;Python.prediction_orchestrator.executor&quot;: &quot;Run&quot;,
+    &quot;Python.regression_enhancement.executor&quot;: &quot;Run&quot;,
+    &quot;Python.test.executor&quot;: &quot;Debug&quot;,
+    &quot;Python.train_model.executor&quot;: &quot;Debug&quot;,
+    &quot;Python.train_race_model.executor&quot;: &quot;Run&quot;,
+    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
+    &quot;RunOnceActivity.git.unshallow&quot;: &quot;true&quot;,
+    &quot;git-widget-placeholder&quot;: &quot;main&quot;
   }
-}]]></component>
+}</component>
   <component name="RecentsManager">
     <key name="MoveFile.RECENT_KEYS">
       <recent name="$PROJECT_DIR$/core/connectors" />
@@ -459,31 +455,6 @@
           <url>file://$PROJECT_DIR$/race_prediction/predict_daily_races.py</url>
           <line>127</line>
           <option name="timeStamp" value="98" />
-        </line-breakpoint>
-        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
-          <url>file://$PROJECT_DIR$/core/orchestrators/embedding_feature.py</url>
-          <line>1595</line>
-          <option name="timeStamp" value="114" />
-        </line-breakpoint>
-        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
-          <url>file://$PROJECT_DIR$/model_training/historical/train_race_model.py</url>
-          <line>966</line>
-          <option name="timeStamp" value="116" />
-        </line-breakpoint>
-        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
-          <url>file://$PROJECT_DIR$/model_training/historical/train_race_model.py</url>
-          <line>450</line>
-          <option name="timeStamp" value="126" />
-        </line-breakpoint>
-        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
-          <url>file://$PROJECT_DIR$/model_training/historical/train_race_model.py</url>
-          <line>457</line>
-          <option name="timeStamp" value="127" />
-        </line-breakpoint>
-        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
-          <url>file://$PROJECT_DIR$/model_training/historical/train_race_model.py</url>
-          <line>465</line>
-          <option name="timeStamp" value="128" />
         </line-breakpoint>
       </breakpoints>
       <default-breakpoints>
